"""
æ¨¡å‹é¢„åŠ è½½å’Œç¼“å­˜ç®¡ç†å™¨
å®ç°æ¨¡å‹é¢„åŠ è½½ã€LRUç¼“å­˜ã€å†…å­˜ç›‘æ§ç­‰åŠŸèƒ½
"""

import os
import gc
import logging
import threading
import time
import asyncio
from collections import OrderedDict
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Any
import psutil
import torch
# å»¶è¿Ÿå¯¼å…¥ faster_whisperï¼Œé¿å…å¯åŠ¨æ—¶åŠ è½½ ctranslate2 å¯¼è‡´é¦–æ¬¡å¯åŠ¨å¡æ­»
# from faster_whisper import WhisperModel  # å·²ç§»è‡³ get_model() å†…éƒ¨å»¶è¿Ÿå¯¼å…¥

# ä½¿ç”¨å®Œæ•´è·¯å¾„å¯¼å…¥
from app.models.job_models import JobSettings


@dataclass
class ModelCacheInfo:
    """æ¨¡å‹ç¼“å­˜ä¿¡æ¯"""
    model: Any
    key: Tuple[str, str, str]
    load_time: float
    last_used: float
    memory_size: int  # ä¼°ç®—çš„å†…å­˜å ç”¨(MB)


@dataclass
class PreloadConfig:
    """é¢„åŠ è½½é…ç½®"""
    enabled: bool = True
    default_models: List[str] = None
    max_cache_size: int = 3  # æœ€å¤§ç¼“å­˜æ¨¡å‹æ•°é‡
    memory_threshold: float = 0.8  # å†…å­˜ä½¿ç”¨é˜ˆå€¼(80%)
    preload_timeout: int = 300  # é¢„åŠ è½½è¶…æ—¶æ—¶é—´(ç§’)
    warmup_enabled: bool = True  # æ˜¯å¦å¯ç”¨é¢„çƒ­

    def __post_init__(self):
        if self.default_models is None:
            self.default_models = ["medium"]


class ModelPreloadManager:
    """ç®€åŒ–ç‰ˆæ¨¡å‹é¢„åŠ è½½å’Œç¼“å­˜ç®¡ç†å™¨ - æ–¹æ¡ˆäºŒå®ç°
    
    æ ¸å¿ƒæ”¹è¿›:
    1. ç»Ÿä¸€é”æœºåˆ¶é¿å…æ­»é”
    2. å¹‚ç­‰æ€§é¢„åŠ è½½é¿å…é‡å¤æ‰§è¡Œ
    3. ç¼“å­˜ç‰ˆæœ¬å·ç¡®ä¿çŠ¶æ€åŒæ­¥
    4. æ ‡å‡†åŒ–æ—¥å¿—ä¾¿äºè°ƒè¯•
    """
    
    def __init__(self, config: PreloadConfig = None):
        self.config = config or PreloadConfig()
        self.logger = self._setup_logger()

        # æ¨¡å‹ç¼“å­˜ (LRU)
        self._whisper_cache: OrderedDict[Tuple[str, str, str], ModelCacheInfo] = OrderedDict()

        # SenseVoice æ¨¡å‹ç¼“å­˜ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
        self._sensevoice_service = None

        # ç»Ÿä¸€é” - ç®€åŒ–å¹¶å‘æ§åˆ¶ï¼Œé¿å…å¤šé”æ­»é”
        self._global_lock = threading.RLock()

        # ç®€åŒ–çš„é¢„åŠ è½½çŠ¶æ€ - å•ä¸€æ•°æ®æº
        self._preload_status = {
            "is_preloading": False,
            "progress": 0.0,
            "current_model": "",
            "total_models": 0,
            "loaded_models": 0,
            "errors": [],
            "failed_attempts": 0,
            "last_attempt_time": 0,
            "max_retry_attempts": 3,
            "retry_cooldown": 30,
            "cache_version": int(time.time())  # ç¼“å­˜ç‰ˆæœ¬å·ï¼Œç”¨äºçŠ¶æ€åŒæ­¥
        }

        # é¢„åŠ è½½ä»»åŠ¡ç®¡ç† - å®ç°å¹‚ç­‰æ€§
        self._preload_promise: Optional[asyncio.Task] = None

        # å†…å­˜ç›‘æ§
        self._memory_monitor = MemoryMonitor()

        self.logger.info("ModelPreloadManageråˆå§‹åŒ–å®Œæˆ - ç®€åŒ–æ¶æ„")
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ ‡å‡†åŒ–çš„æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger(f"{__name__}.ModelPreloadManager")
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - [æ¨¡å‹ç®¡ç†] - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
        
    def get_preload_status(self) -> Dict[str, Any]:
        """è·å–é¢„åŠ è½½çŠ¶æ€ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        with self._global_lock:
            status = self._preload_status.copy()
            self.logger.debug(f"çŠ¶æ€æŸ¥è¯¢: é¢„åŠ è½½={status['is_preloading']}, è¿›åº¦={status['progress']:.1f}%, å·²åŠ è½½={status['loaded_models']}")
            return status
    
    def get_cache_status(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜çŠ¶æ€ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        with self._global_lock:
            whisper_models = [
                {
                    "key": info.key,
                    "memory_mb": info.memory_size,
                    "last_used": info.last_used,
                    "load_time": info.load_time
                }
                for info in self._whisper_cache.values()
            ]

            total_memory = sum(info.memory_size for info in self._whisper_cache.values())

            cache_status = {
                "whisper_models": whisper_models,
                "total_memory_mb": total_memory,
                "max_cache_size": self.config.max_cache_size,
                "memory_info": self._memory_monitor.get_memory_info(),
                "cache_version": self._preload_status["cache_version"]
            }

            self.logger.debug(f"ç¼“å­˜æŸ¥è¯¢: Whisperæ¨¡å‹={len(whisper_models)}ä¸ª, å†…å­˜={total_memory}MB")
            return cache_status
    
    async def preload_models(self, progress_callback=None) -> Dict[str, Any]:
        """é¢„åŠ è½½é»˜è®¤æ¨¡å‹ - é‡æ„ç‰ˆï¼šDemucs + VADï¼Œæ—  Whisper

        é¢„åŠ è½½é€»è¾‘ï¼š
        1. é¢„åŠ è½½ Silero VAD æ¨¡å‹ï¼ˆå†…ç½®ï¼Œå¿«é€Ÿï¼‰
        2. é¢„åŠ è½½é»˜è®¤ Demucs æ¨¡å‹ï¼ˆhtdemucsï¼‰
        3. ã€ç§»é™¤ã€‘ä¸å†é¢„åŠ è½½ Whisper æ¨¡å‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰
        """
        with self._global_lock:
            if self._preload_status["is_preloading"]:
                self.logger.info("é¢„åŠ è½½å·²åœ¨è¿›è¡Œä¸­ï¼Œè¿”å›å·²æœ‰ä»»åŠ¡")
                return {"success": True, "message": "é¢„åŠ è½½å·²åœ¨è¿›è¡Œä¸­"}

            if not self.config.enabled:
                self.logger.warning("æ¨¡å‹é¢„åŠ è½½åŠŸèƒ½å·²ç¦ç”¨")
                return {"success": False, "message": "é¢„åŠ è½½åŠŸèƒ½å·²ç¦ç”¨"}

            self._preload_status.update({
                "is_preloading": True,
                "progress": 0.0,
                "current_model": "",
                "total_models": 2,  # VAD + Demucs
                "loaded_models": 0,
                "errors": [],
                "last_attempt_time": time.time()
            })

        try:
            success_count = 0

            # ===== 1. é¢„åŠ è½½ Silero VAD æ¨¡å‹ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰=====
            try:
                self.logger.info("[é¢„åŠ è½½] åŠ è½½ Silero VAD æ¨¡å‹...")
                with self._global_lock:
                    self._preload_status.update({
                        "current_model": "Silero VAD",
                        "progress": 0.0
                    })

                from pathlib import Path as PathlibPath
                from silero_vad.utils_vad import OnnxWrapper

                builtin_model_path = PathlibPath(__file__).parent.parent / "assets" / "silero" / "silero_vad.onnx"

                if builtin_model_path.exists():
                    _ = OnnxWrapper(str(builtin_model_path), force_onnx_cpu=False)
                    self.logger.info("Silero VAD æ¨¡å‹é¢„åŠ è½½æˆåŠŸ")
                    success_count += 1
                    with self._global_lock:
                        self._preload_status["loaded_models"] += 1
                else:
                    self.logger.warning(f"Silero VAD æ¨¡å‹ç¼ºå¤±: {builtin_model_path}")

            except Exception as e:
                self.logger.warning(f"Silero VAD é¢„åŠ è½½å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")

            # ===== 2. é¢„åŠ è½½ Demucs æ¨¡å‹ï¼ˆæ–°å¢ï¼‰=====
            try:
                self.logger.info("[é¢„åŠ è½½] åŠ è½½ Demucs æ¨¡å‹...")
                with self._global_lock:
                    self._preload_status.update({
                        "current_model": "Demucs (htdemucs)",
                        "progress": 50.0
                    })

                from app.services.demucs_service import get_demucs_service
                demucs_service = get_demucs_service()

                # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡ŒåŒæ­¥çš„æ¨¡å‹åŠ è½½ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
                loop = asyncio.get_event_loop()
                preload_success = await loop.run_in_executor(
                    None,
                    demucs_service.preload_model,
                    "htdemucs"
                )

                if preload_success:
                    self.logger.info("Demucs æ¨¡å‹é¢„åŠ è½½æˆåŠŸ")
                    success_count += 1
                    with self._global_lock:
                        self._preload_status["loaded_models"] += 1
                else:
                    self.logger.warning("Demucs æ¨¡å‹é¢„åŠ è½½è¿”å›å¤±è´¥")
                    with self._global_lock:
                        self._preload_status["errors"].append("Demucs é¢„åŠ è½½å¤±è´¥")

            except Exception as e:
                self.logger.warning(f"Demucs é¢„åŠ è½½å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")
                with self._global_lock:
                    self._preload_status["errors"].append(f"Demucs é¢„åŠ è½½å¤±è´¥: {e}")

            # ===== ã€ç§»é™¤ã€‘ä¸å†é¢„åŠ è½½ Whisper æ¨¡å‹ =====
            # åŸæ¥çš„ Whisper é¢„åŠ è½½ä»£ç å·²åˆ é™¤
            # Whisper ä»…åœ¨åå¤„ç†è¡¥åˆ€é˜¶æ®µæŒ‰éœ€åŠ è½½

            # å®Œæˆé¢„åŠ è½½
            success = success_count > 0

            with self._global_lock:
                self._preload_status.update({
                    "is_preloading": False,
                    "progress": 100.0,
                    "current_model": "",
                    "cache_version": int(time.time())
                })

            result = {
                "success": success,
                "loaded_models": success_count,
                "total_models": 2,
                "errors": self._preload_status["errors"].copy()
            }

            if success:
                self.logger.info(f"é¢„åŠ è½½ä»»åŠ¡æˆåŠŸå®Œæˆ: {success_count}/2 ä¸ªæ¨¡å‹")
            else:
                self.logger.warning("é¢„åŠ è½½ä»»åŠ¡å®Œæˆä½†æ— æˆåŠŸåŠ è½½çš„æ¨¡å‹")

            if progress_callback:
                progress_callback(self._preload_status.copy())

            return result

        except Exception as e:
            self.logger.error(f"é¢„åŠ è½½å¼‚å¸¸: {e}", exc_info=True)
            with self._global_lock:
                self._preload_status.update({
                    "is_preloading": False,
                    "progress": 0.0,
                    "errors": [str(e)]
                })
            return {"success": False, "message": str(e)}

    def reset_preload_attempts(self):
        """é‡ç½®é¢„åŠ è½½å¤±è´¥è®¡æ•° - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        with self._global_lock:
            old_attempts = self._preload_status["failed_attempts"]
            self._preload_status["failed_attempts"] = 0
            self._preload_status["last_attempt_time"] = 0
            self._preload_status["cache_version"] = int(time.time())
            
        self.logger.info(f"é¢„åŠ è½½å¤±è´¥è®¡æ•°å·²é‡ç½®: {old_attempts} -> 0")
    
    def get_model(self, settings: JobSettings):
        """è·å–Whisperæ¨¡å‹ (å¸¦LRUç¼“å­˜) - ç®€åŒ–ç‰ˆæœ¬"""
        key = (settings.model, settings.compute_type, settings.device)
        
        with self._global_lock:
            # å‘½ä¸­ç¼“å­˜
            if key in self._whisper_cache:
                info = self._whisper_cache[key]
                info.last_used = time.time()
                # ç§»åˆ°æœ€å (æœ€è¿‘ä½¿ç”¨)
                self._whisper_cache.move_to_end(key)
                self.logger.debug(f"å‘½ä¸­æ¨¡å‹ç¼“å­˜: {key}")
                return info.model
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼ŒåŠ è½½æ–°æ¨¡å‹
            self.logger.info(f"éœ€è¦åŠ è½½æ–°æ¨¡å‹: {key}")
            return self._load_whisper_model(settings)
    
    def _load_whisper_model(self, settings: JobSettings):
        """åŠ è½½Whisperæ¨¡å‹ - ç®€åŒ–ç‰ˆæœ¬å¸¦å¹¶å‘ä¿æŠ¤"""
        key = (settings.model, settings.compute_type, settings.device)

        # å†æ¬¡æ£€æŸ¥ç¼“å­˜ï¼ˆé¿å…å¹¶å‘åŠ è½½åŒä¸€æ¨¡å‹ï¼‰
        with self._global_lock:
            if key in self._whisper_cache:
                info = self._whisper_cache[key]
                info.last_used = time.time()
                self._whisper_cache.move_to_end(key)
                self.logger.debug(f"å¹¶å‘æ£€æŸ¥å‘½ä¸­ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½: {key}")
                return info.model

        self.logger.info(f"å¼€å§‹åŠ è½½æ–°Whisperæ¨¡å‹: {key}")

        # æ£€æŸ¥å†…å­˜
        if not self._memory_monitor.check_memory_available():
            self.logger.warning("å†…å­˜ä¸è¶³ï¼Œå°è¯•æ¸…ç†ç¼“å­˜")
            self._cleanup_old_models()

        # æ£€æŸ¥ç¼“å­˜å¤§å°
        with self._global_lock:
            if len(self._whisper_cache) >= self.config.max_cache_size:
                self._evict_lru_model()

        try:
            start_time = time.time()

            # å¤„ç† auto æ¨¡å¼ï¼šè§£æä¸ºå…·ä½“çš„è®¡ç®—ç±»å‹
            compute_type_resolved = settings.compute_type
            if compute_type_resolved == "auto":
                from app.services.whisper_service import get_auto_compute_type
                compute_type_resolved = get_auto_compute_type(settings.device)
                self.logger.info(f"autoæ¨¡å¼å·²è§£æä¸º: {compute_type_resolved}")

            self.logger.info(f"æ­£åœ¨ä»ç£ç›˜åŠ è½½æ¨¡å‹ {settings.model} (device={settings.device}, compute_type={compute_type_resolved})")

            # å¯¼å…¥é…ç½®ä»¥è·å–ç¼“å­˜è·¯å¾„
            from app.core.config import config

            # å»¶è¿Ÿå¯¼å…¥ faster_whisperï¼Œé¿å…å¯åŠ¨æ—¶åŠ è½½ ctranslate2 å¯¼è‡´é¦–æ¬¡å¯åŠ¨å¡æ­»
            from faster_whisper import WhisperModel

            # ä½¿ç”¨ faster_whisper çš„ WhisperModel åŠ è½½æ¨¡å‹
            model = WhisperModel(
                settings.model,
                device=settings.device,
                compute_type=compute_type_resolved,  # ä½¿ç”¨è§£æåçš„è®¡ç®—ç±»å‹
                download_root=str(config.HF_CACHE_DIR),
                local_files_only=True
            )
            load_time = time.time() - start_time

            # ä¼°ç®—å†…å­˜ä½¿ç”¨
            memory_size = self._estimate_model_memory(model)

            # æ·»åŠ åˆ°ç¼“å­˜
            info = ModelCacheInfo(
                model=model,
                key=key,
                load_time=load_time,
                last_used=time.time(),
                memory_size=memory_size
            )

            with self._global_lock:
                self._whisper_cache[key] = info
                # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
                self._preload_status["cache_version"] = int(time.time())

            self.logger.info(f"æˆåŠŸåŠ è½½å¹¶ç¼“å­˜Whisperæ¨¡å‹ {key} (å†…å­˜: {memory_size}MB, è€—æ—¶: {load_time:.2f}s)")
            return model

        except Exception as e:
            self.logger.error(f"åŠ è½½Whisperæ¨¡å‹å¤±è´¥ {key}: {str(e)}", exc_info=True)
            raise

    def _warmup_model(self, model):
        """é¢„çƒ­æ¨¡å‹ - ç©ºè·‘ä¸€æ¬¡ç¡®ä¿å®Œå…¨åŠ è½½"""
        try:
            self.logger.debug("å¼€å§‹æ¨¡å‹é¢„çƒ­")

            # åˆ›å»ºè™šæ‹ŸéŸ³é¢‘æ•°æ® (1ç§’é™éŸ³)
            import numpy as np
            dummy_audio = np.zeros(16000, dtype=np.float32)  # 16kHz 1ç§’

            # ä½¿ç”¨ transcribe æ–¹æ³•é¢„çƒ­æ¨¡å‹
            segments, _ = model.transcribe(dummy_audio)
            _ = list(segments)  # è§¦å‘ç”Ÿæˆå™¨

            self.logger.debug("æ¨¡å‹é¢„çƒ­å®Œæˆ")

        except Exception as e:
            self.logger.warning(f"æ¨¡å‹é¢„çƒ­å¤±è´¥: {str(e)}")
    
    def _evict_lru_model(self):
        """é©±é€æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹ - éœ€è¦åœ¨é”å†…è°ƒç”¨"""
        if not self._whisper_cache:
            return
        
        # æœ€ä¹…æœªä½¿ç”¨çš„åœ¨å¼€å¤´
        oldest_key = next(iter(self._whisper_cache))
        info = self._whisper_cache.pop(oldest_key)
        
        self.logger.info(f"é©±é€LRUæ¨¡å‹: {oldest_key}, é‡Šæ”¾å†…å­˜: {info.memory_size}MB")
        
        # é‡Šæ”¾å†…å­˜
        del info.model
        del info
        gc.collect()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def _cleanup_old_models(self):
        """æ¸…ç†æ—§æ¨¡å‹é‡Šæ”¾å†…å­˜ - éœ€è¦åœ¨é”å¤–è°ƒç”¨"""
        current_time = time.time()
        to_remove = []
        
        with self._global_lock:
            for key, info in self._whisper_cache.items():
                # è¶…è¿‡10åˆ†é’Ÿæœªä½¿ç”¨çš„æ¨¡å‹
                if current_time - info.last_used > 600:
                    to_remove.append(key)
            
            for key in to_remove:
                info = self._whisper_cache.pop(key)
                self.logger.info(f"æ¸…ç†æ—§æ¨¡å‹: {key}")
                del info.model
                del info
        
        if to_remove:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            self.logger.info(f"ğŸ’« æ¸…ç†äº† {len(to_remove)} ä¸ªæ—§æ¨¡å‹")
    
    def _estimate_model_memory(self, model) -> int:
        """ä¼°ç®—æ¨¡å‹å†…å­˜ä½¿ç”¨ (MB)"""
        try:
            # ç®€å•ä¼°ç®—ï¼ŒåŸºäºæ¨¡å‹å‚æ•°
            if hasattr(model, 'model') and hasattr(model.model, 'parameters'):
                total_params = sum(p.numel() for p in model.model.parameters())
                # å‡è®¾æ¯ä¸ªå‚æ•°4å­—èŠ‚ (float32) æˆ– 2å­—èŠ‚ (float16)
                bytes_per_param = 2  # float16
                total_bytes = total_params * bytes_per_param
                return int(total_bytes / (1024 * 1024))  # è½¬æ¢ä¸ºMB
        except:
            pass
        
        # é»˜è®¤ä¼°ç®—å€¼
        return 500  # é»˜è®¤500MB
    
    def clear_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ - ç®€åŒ–ç‰ˆæœ¬ï¼Œç«‹å³åŒæ­¥çŠ¶æ€"""
        with self._global_lock:
            # è®°å½•æ¸…ç†å‰çš„ç¼“å­˜çŠ¶æ€
            whisper_count = len(self._whisper_cache)
            total_memory = sum(info.memory_size for info in self._whisper_cache.values())

            # æ¸…ç†Whisperæ¨¡å‹ç¼“å­˜
            for info in self._whisper_cache.values():
                del info.model
            self._whisper_cache.clear()

            # æ¸…ç† SenseVoice æ¨¡å‹ç¼“å­˜
            self.unload_sensevoice()

            # ç«‹å³æ›´æ–°é¢„åŠ è½½çŠ¶æ€ - è§£å†³çŠ¶æ€åŒæ­¥é—®é¢˜
            self._preload_status.update({
                "loaded_models": 0,
                "is_preloading": False,
                "progress": 0.0,
                "current_model": "",
                "errors": [],
                "cache_version": int(time.time())  # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
            })

        # åƒåœ¾å›æ”¶å’ŒGPUå†…å­˜æ¸…ç†
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        self.logger.info(f"å·²æ¸…ç©ºæ‰€æœ‰æ¨¡å‹ç¼“å­˜: Whisper={whisper_count}ä¸ª, é‡Šæ”¾å†…å­˜={total_memory}MB")

    def evict_model(self, model_id: str, device: str = "cuda", compute_type: str = "float16"):
        """
        æ¸…ç†æŒ‡å®šWhisperæ¨¡å‹çš„ç¼“å­˜

        Args:
            model_id: æ¨¡å‹ID
            device: è®¾å¤‡ç±»å‹
            compute_type: è®¡ç®—ç±»å‹
        """
        key = (model_id, compute_type, device)

        with self._global_lock:
            if key in self._whisper_cache:
                info = self._whisper_cache.pop(key)
                self.logger.info(f"æ¸…ç†æ¨¡å‹ç¼“å­˜: {key}, é‡Šæ”¾å†…å­˜: {info.memory_size}MB")

                # é‡Šæ”¾å†…å­˜
                del info.model
                del info

                # æ›´æ–°é¢„åŠ è½½çŠ¶æ€ä¸­çš„loaded_modelsè®¡æ•°
                self._preload_status["loaded_models"] = len(self._whisper_cache)
                self._preload_status["cache_version"] = int(time.time())

        # åƒåœ¾å›æ”¶å’ŒGPUå†…å­˜æ¸…ç†
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # ========== SenseVoice æ¨¡å‹ç®¡ç† ==========

    def get_sensevoice_model(self):
        """è·å– SenseVoice æ¨¡å‹ï¼ˆå•ä¾‹ï¼‰"""
        with self._global_lock:
            if self._sensevoice_service is None:
                try:
                    from app.services.sensevoice_onnx_service import get_sensevoice_service
                    self._sensevoice_service = get_sensevoice_service()

                    if not self._sensevoice_service.is_loaded:
                        self._sensevoice_service.load_model()

                    # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬
                    self._preload_status["cache_version"] = int(time.time())
                    self.logger.info("SenseVoice æ¨¡å‹å·²åŠ è½½åˆ°ç¼“å­˜")

                except Exception as e:
                    self.logger.error(f"åŠ è½½ SenseVoice æ¨¡å‹å¤±è´¥: {e}")
                    self._sensevoice_service = None
                    raise

            return self._sensevoice_service

    def unload_sensevoice(self):
        """å¸è½½ SenseVoice æ¨¡å‹"""
        with self._global_lock:
            if self._sensevoice_service is not None:
                try:
                    self._sensevoice_service.unload_model()
                    self._sensevoice_service = None

                    # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬
                    self._preload_status["cache_version"] = int(time.time())

                    self.logger.info("SenseVoice æ¨¡å‹å·²å¸è½½")
                except Exception as e:
                    self.logger.error(f"å¸è½½ SenseVoice æ¨¡å‹å¤±è´¥: {e}")

        # åƒåœ¾å›æ”¶å’ŒGPUå†…å­˜æ¸…ç†
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def unload_demucs(self):
        """
        å¸è½½ Demucs æ¨¡å‹ï¼ˆæ˜¾å¼é‡Šæ”¾æ˜¾å­˜ï¼‰

        æ³¨æ„ï¼šDemucs æ¨¡å‹é€šå¸¸åœ¨ transcription_service ä¸­ç®¡ç†
        æ­¤æ–¹æ³•æä¾›ç»Ÿä¸€çš„æ˜¾å­˜é‡Šæ”¾æ¥å£
        """
        with self._global_lock:
            self.logger.info("è§¦å‘ Demucs æ˜¾å­˜é‡Šæ”¾")

        # åƒåœ¾å›æ”¶å’ŒGPUå†…å­˜æ¸…ç†
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            self.logger.info("PyTorch æ˜¾å­˜å·²é‡Šæ”¾")

    # ========== å•æ¨¡å‹ç®¡ç†æ¥å£ - å§”æ‰˜ç»™æ¨¡å‹ç®¡ç†æœåŠ¡ ==========

    def download_whisper_model(self, model_id: str) -> bool:
        """
        ä¸‹è½½å•ä¸ªWhisperæ¨¡å‹ï¼ˆå§”æ‰˜ç»™æ¨¡å‹ç®¡ç†æœåŠ¡ï¼‰

        Args:
            model_id: æ¨¡å‹ID (tiny, base, small, medium, large-v2, large-v3)

        Returns:
            bool: æ˜¯å¦æˆåŠŸå¯åŠ¨ä¸‹è½½
        """
        try:
            from app.services.model_manager_service import get_model_manager
            model_mgr = get_model_manager()
            success = model_mgr.download_whisper_model(model_id)

            if success:
                self.logger.info(f"å·²å§”æ‰˜æ¨¡å‹ç®¡ç†æœåŠ¡ä¸‹è½½Whisperæ¨¡å‹: {model_id}")
            return success

        except Exception as e:
            self.logger.error(f"ä¸‹è½½Whisperæ¨¡å‹å¤±è´¥: {model_id} - {e}")
            return False

    def delete_whisper_model(self, model_id: str) -> bool:
        """
        åˆ é™¤Whisperæ¨¡å‹ï¼ˆå§”æ‰˜ç»™æ¨¡å‹ç®¡ç†æœåŠ¡ï¼Œå¹¶æ¸…ç†ç¼“å­˜ï¼‰

        Args:
            model_id: æ¨¡å‹ID

        Returns:
            bool: æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            from app.services.model_manager_service import get_model_manager
            model_mgr = get_model_manager()

            # å…ˆä»ç¼“å­˜ä¸­ç§»é™¤
            with self._global_lock:
                keys_to_remove = [k for k in self._whisper_cache.keys() if k[0] == model_id]
                for key in keys_to_remove:
                    info = self._whisper_cache.pop(key)
                    del info.model
                    self.logger.debug(f"ä»ç¼“å­˜ä¸­ç§»é™¤æ¨¡å‹: {key}")

                # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
                self._preload_status["cache_version"] = int(time.time())

            # æ¸…ç†GPUå†…å­˜
            if keys_to_remove:
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            # å§”æ‰˜ç»™æ¨¡å‹ç®¡ç†æœåŠ¡åˆ é™¤ç£ç›˜æ–‡ä»¶
            success = model_mgr.delete_whisper_model(model_id)

            if success:
                self.logger.info(f"å·²åˆ é™¤Whisperæ¨¡å‹: {model_id}")
            return success

        except Exception as e:
            self.logger.error(f"åˆ é™¤Whisperæ¨¡å‹å¤±è´¥: {model_id} - {e}")
            return False

    def list_all_models(self) -> Dict[str, Any]:
        """
        åˆ—å‡ºæ‰€æœ‰æ¨¡å‹çš„çŠ¶æ€ï¼ˆæ•´åˆç£ç›˜çŠ¶æ€å’Œç¼“å­˜çŠ¶æ€ï¼‰

        Returns:
            Dict: åŒ…å«whisperæ¨¡å‹çš„çŠ¶æ€ä¿¡æ¯
        """
        try:
            from app.services.model_manager_service import get_model_manager
            model_mgr = get_model_manager()

            # è·å–ç£ç›˜ä¸Šçš„æ¨¡å‹çŠ¶æ€
            whisper_models = [
                {
                    "model_id": m.model_id,
                    "size_mb": m.size_mb,
                    "status": m.status,
                    "download_progress": m.download_progress,
                    "local_path": m.local_path,
                    "description": m.description,
                    "cached": any(k[0] == m.model_id for k in self._whisper_cache.keys())
                }
                for m in model_mgr.list_whisper_models()
            ]

            return {
                "whisper_models": whisper_models,
                "cache_info": {
                    "whisper_cached": len(self._whisper_cache),
                    "total_memory_mb": sum(info.memory_size for info in self._whisper_cache.values())
                }
            }

        except Exception as e:
            self.logger.error(f"åˆ—å‡ºæ¨¡å‹å¤±è´¥: {e}")
            return {"error": str(e)}


class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def get_memory_info(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä¿¡æ¯"""
        try:
            # ç³»ç»Ÿå†…å­˜
            memory = psutil.virtual_memory()

            # GPUå†…å­˜ (å¦‚æœå¯ç”¨)
            gpu_info = {}
            if torch.cuda.is_available():
                gpu_info = {
                    "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory / (1024**3),  # GB
                    "gpu_memory_allocated": torch.cuda.memory_allocated() / (1024**3),  # GB
                    "gpu_memory_cached": torch.cuda.memory_reserved() / (1024**3),  # GB
                }

            return {
                "system_memory_total": memory.total / (1024**3),  # GB
                "system_memory_used": memory.used / (1024**3),  # GB
                "system_memory_percent": memory.percent,
                **gpu_info
            }
        except Exception as e:
            self.logger.error(f"è·å–å†…å­˜ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}

    def check_memory_available(self, threshold: float = 0.85) -> bool:
        """æ£€æŸ¥å†…å­˜æ˜¯å¦å……è¶³"""
        try:
            memory = psutil.virtual_memory()
            return memory.percent < (threshold * 100)
        except:
            return True  # é»˜è®¤è®¤ä¸ºå†…å­˜å……è¶³


# ========== å…¨å±€å•ä¾‹æ¨¡å¼ - æä¾›ç»Ÿä¸€çš„æ¨¡å‹ç®¡ç†å™¨æ¥å£ ==========

_model_manager: Optional[ModelPreloadManager] = None


def initialize_model_manager(config: PreloadConfig = None) -> ModelPreloadManager:
    """
    åˆå§‹åŒ–å…¨å±€æ¨¡å‹ç®¡ç†å™¨

    Args:
        config: é¢„åŠ è½½é…ç½®

    Returns:
        ModelPreloadManager: æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
    """
    global _model_manager
    if _model_manager is None:
        _model_manager = ModelPreloadManager(config)
        logging.getLogger(__name__).info(" å…¨å±€æ¨¡å‹é¢„åŠ è½½ç®¡ç†å™¨å·²åˆå§‹åŒ–")
    return _model_manager


def get_model_manager() -> Optional[ModelPreloadManager]:
    """
    è·å–å…¨å±€æ¨¡å‹ç®¡ç†å™¨

    Returns:
        Optional[ModelPreloadManager]: æ¨¡å‹ç®¡ç†å™¨å®ä¾‹ï¼Œæœªåˆå§‹åŒ–åˆ™è¿”å›None
    """
    return _model_manager


async def preload_default_models(progress_callback=None) -> Dict[str, Any]:
    """
    é¢„åŠ è½½é»˜è®¤æ¨¡å‹

    Args:
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        Dict: é¢„åŠ è½½ç»“æœ
    """
    if _model_manager is None:
        return {"success": False, "message": "æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–"}

    return await _model_manager.preload_models(progress_callback)


def get_preload_status() -> Dict[str, Any]:
    """
    è·å–é¢„åŠ è½½çŠ¶æ€

    Returns:
        Dict: é¢„åŠ è½½çŠ¶æ€ä¿¡æ¯
    """
    if _model_manager is None:
        return {"is_preloading": False, "message": "æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–"}

    return _model_manager.get_preload_status()


def get_cache_status() -> Dict[str, Any]:
    """
    è·å–ç¼“å­˜çŠ¶æ€

    Returns:
        Dict: ç¼“å­˜çŠ¶æ€ä¿¡æ¯
    """
    if _model_manager is None:
        return {"message": "æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–"}

    return _model_manager.get_cache_status()
