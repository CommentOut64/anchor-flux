这是一个非常理性且切中痛点的判断。完全通用的“动态图架构”确实存在**过度设计（Over-engineering）**的风险，特别是在需要处理复杂的业务状态（SSE、Prompt缓存、降级）时，迁移成本极高。

我为你构思了一个**“高内聚、低耦合”的折中方案**：**三段式异步流水线 (Three-Stage Async Pipeline)**。

这个方案不需要你写一个通用的图引擎，而是针对“双流”这一特定业务场景，构建固定的**生产-消费链条**。它既能实现 CPU/GPU 并行（Pipelining），又能彻底解决代码混乱的问题。

------

### 核心理念：流水线并行 (Pipeline Parallelism)

与其追求让 SenseVoice 和 Whisper **同时处理同一个 Chunk**（这其实没必要，因为 SenseVoice 比 Whisper 快得多），不如让它们**错位并行**：

- 当 Whisper (GPU) 正在啃 **Chunk 1** 的硬骨头时；
- SenseVoice (CPU) 已经跑到了 **Chunk 2** 甚至 **Chunk 3**，并把草稿推送给了前端。

**收益：**

1. **用户体验极佳**：草稿会像机关枪一样“秒级”连续上屏，完全不用等慢流。
2. **资源跑满**：CPU 不用等 GPU，GPU 也不用等 CPU。
3. **防 OOM**：SenseVoice 强制 CPU，Whisper 强制 GPU，物理隔离。

------

### 1. 架构设计：三级固定流水线

我们将 `DualAlignmentPipeline` 拆解为三个独立的、通过队列连接的**Stage（阶段工作者）**。

#### 数据流向

`AudioChunk` -> **[Stage 1: 快流]** -> `中间队列` -> **[Stage 2: 慢流]** -> `结果队列` -> **[Stage 3: 对齐与收尾]**

#### 组件定义

1. **Stage 1: FastStage (CPU Worker)**
   - **职责**：VAD 切分（如果没做预处理）、SenseVoice 推理、**立即推送草稿 (SSE)**、Prompt 提取。
   - **硬件**：CPU (ONNX Runtime, Intra-op threads)。
   - **输出**：打包好的 `StagePayload` (包含 chunk, sv_text, sv_tokens)。
2. **Stage 2: SlowStage (GPU Worker)**
   - **职责**：接收 `StagePayload`，根据 SV 结果构建 Prompt，运行 Whisper。
   - **硬件**：GPU (CUDA)。
   - **输出**：更新后的 `StagePayload` (追加了 whisper_result)。
3. **Stage 3: AlignmentStage (Finalizer)**
   - **职责**：双流对齐算法、降级决策、**推送定稿 (SSE)**、写入 SRT 文件。
   - **硬件**：CPU (轻量级计算)。

------

### 2. 代码实现蓝图 (The "Pragmatic" Code)

不需要复杂的 `Graph` 类，只需要标准的 `asyncio.Queue` 和三个 Worker 函数。

#### 2.1 定义统一的数据包 (Context)

这是解决“混乱”的关键：所有数据都在这个对象里，不再通过函数参数满天飞。

Python

```
@dataclass
class ProcessingContext:
    job_id: str
    chunk_index: int
    audio_chunk: AudioChunk
    
    # 状态数据
    sv_result: Optional[dict] = None     # SenseVoice 结果
    whisper_result: Optional[dict] = None # Whisper 结果
    final_sentences: List[Any] = None    # 对齐后结果
    
    # 控制标志
    is_end: bool = False                 # 结束信号
```

#### 2.2 核心控制器 (The Refactored Pipeline)

重构后的 `DualAlignmentPipeline` 变成了一个**启动器**，它不再亲自跑循环，而是启动三个 Task。

Python

```
class PragmaticDualPipeline:
    def __init__(self, config, sse_manager, resource_manager):
        # 初始化队列 (设置 maxsize 实现背压，防止快流跑太快撑爆内存)
        self.queue_inter = asyncio.Queue(maxsize=5)  # 快流 -> 慢流
        self.queue_final = asyncio.Queue(maxsize=5)  # 慢流 -> 对齐
        
        # 依赖注入
        self.fast_worker = FastWorker(config, sse_manager)
        self.slow_worker = SlowWorker(config) # Whisper
        self.align_worker = AlignmentWorker(config, sse_manager)

    async def run(self, audio_chunks):
        # 1. 启动三个并行的消费者
        t1 = asyncio.create_task(self._fast_loop(audio_chunks))
        t2 = asyncio.create_task(self._slow_loop())
        t3 = asyncio.create_task(self._align_loop())
        
        # 2. 等待所有任务完成
        await asyncio.gather(t1, t2, t3)

    # --- Stage 1: 快流循环 ---
    async def _fast_loop(self, chunks):
        for i, chunk in enumerate(chunks):
            # 构造上下文
            ctx = ProcessingContext(job_id=..., chunk_index=i, audio_chunk=chunk)
            
            # 执行业务逻辑 (强制 CPU)
            # 内部包含: SenseVoice推理 -> SSE推送草稿
            await self.fast_worker.process(ctx) 
            
            # 传给下一棒
            await self.queue_inter.put(ctx)
        
        # 发送结束信号
        await self.queue_inter.put(ProcessingContext(..., is_end=True))

    # --- Stage 2: 慢流循环 ---
    async def _slow_loop(self):
        while True:
            ctx = await self.queue_inter.get()
            if ctx.is_end:
                await self.queue_final.put(ctx) # 传递结束信号
                break
            
            # 执行业务逻辑 (GPU)
            # 内部包含: 构造Prompt -> Whisper推理
            await self.slow_worker.process(ctx)
            
            await self.queue_final.put(ctx)

    # --- Stage 3: 对齐循环 ---
    async def _align_loop(self):
        while True:
            ctx = await self.queue_final.get()
            if ctx.is_end:
                break
            
            # 执行业务逻辑
            # 内部包含: 对齐 -> 降级 -> SSE推送定稿 -> 写文件
            await self.align_worker.process(ctx)
```

------

### 3. 如何解决你的具体痛点？

#### 痛点 A: "搞不清谁在调用谁"

- **解决**：调用链被切断了。
  - `FastWorker` 只管把草稿发出去，把结果塞进队列。它根本不知道 Whisper 存不存在。
  - `SlowWorker` 只管从队列拿 SV 的结果做 Prompt，跑 Whisper。它根本不知道草稿已经发了。
  - 所有复杂的业务逻辑（如Prompt构建、SSE推送）都被封装在各自的 `process` 方法内部。

#### 痛点 B: "并行双流"

- **解决**：利用 `asyncio.Queue` 的缓冲能力。
  - SenseVoice 跑 Chunk 5 时，Whisper 正在跑 Chunk 2。两者互不阻塞。
  - **关键配置**：你需要确保 SenseVoice 运行在 CPU 上（`providers=['CPUExecutionProvider']`），而 Whisper 运行在 GPU 上。这样才能真正物理并行。

#### 痛点 C: "OOM 风险"

- **解决**：
  - **背压 (Backpressure)**：`asyncio.Queue(maxsize=5)` 限制了正在处理的 Chunk 数量。即使 CPU 跑得飞快，如果 GPU 卡住了，队列满了，CPU 就会自动暂停等待，不会把内存撑爆。
  - **资源分离**：强制 SV 用 CPU 内存，Whisper 用显存。

#### 痛点 D: "新旧代码混杂"

- **解决**：这套方案强制你把逻辑拆分到 `FastWorker`、`SlowWorker`、`AlignmentWorker` 这三个类中。
  - 你可以创建一个新的文件夹 `backend/app/pipelines/stages/`。
  - 把 `DualAlignmentPipeline` 里几百行的逻辑一点点剪切、粘贴、清理到这三个类里。
  - 旧的 pipeline 文件最终可以被安全删除。

------

### 4. 实施路线图 (低风险迁移)

1. **Step 1: 定义 Context**
   - 在 `backend/app/schemas/pipeline_context.py` 里定义 `ProcessingContext`。
2. **Step 2: 拆分 Worker 类 (最重要的一步)**
   - 创建 `FastWorker`：把 `_run_sensevoice`, `_split_sentences(draft)`, `subtitle_manager.add_draft` 移进去。
   - 创建 `SlowWorker`：把 `_build_whisper_prompt`, `_run_whisper` 移进去。
   - 创建 `AlignmentWorker`：把 `_align_and_fallback`, `subtitle_manager.replace_chunk` 移进去。
   - *此时不要改异步逻辑，只是搬运代码，确保单元测试通过。*
3. **Step 3: 改造 Pipeline**
   - 创建一个新的 `AsyncDualPipeline` 类，使用上面的 `queue` 和 `loop` 逻辑把三个 Worker 串起来。
4. **Step 4: 替换入口**
   - 在 `job_queue_service.py` 里，把调用旧 Pipeline 的地方换成调用 `AsyncDualPipeline.run()`。

### 总结

这个方案**不引入新的框架**，只利用 Python 原生的 `asyncio` 和 `Queue`。它保留了你现有代码的大部分逻辑（只是换了个位置），但从根本上解耦了**快流**和**慢流**的执行，实现了你想要的“CPU/GPU 并行”和“代码整洁”，且通过**队列最大长度**控制了 OOM 风险。这是性价比最高的重构路径。

这是一个非常专业且切中要害的问题。你担心的点（**CPU抢占导致GPU调度阻塞**、**数据交换开销**）在高性能计算中确实存在，但在我们这个具体的音视频场景下，通过合理的配置是可以完全避免的。

------

### 2. 为什么“三段式异步流水线”是安全的？

我设计的方案中，有三个“保险丝”来防止你担心的情况发生：

#### 保险丝 1：队列背压 (Queue Backpressure) —— 防止内存爆炸

我们在代码中设置了 `maxsize=5`：

Python

```
self.queue_inter = asyncio.Queue(maxsize=5)
```

**机制**：

- 假设 SenseVoice（CPU）跑得飞快，一秒钟处理了 10 个 Chunk。
- 而 Whisper（GPU）比较慢，只处理了 1 个。
- 当队列里积压了 5 个 Chunk 后，SenseVoice 试图放入第 6 个时，**会被强制暂停（await put 阻塞）**。
- **结果**：CPU 自动进入“休息”状态，等待 GPU 消化掉一些库存。这就天然实现了**“快慢同步”**，绝不会把内存撑爆，也不会让 CPU 无意义空转。

#### 保险丝 2：GIL 释放 (GIL Release) —— 防止线程锁死

- SenseVoice 使用的是 **ONNX Runtime**。
- ONNX Runtime 的底层是 C++ 实现的。在进行推理运算的那一瞬间，它会**释放 Python 的全局解释器锁 (GIL)**。
- 这意味着：当 SenseVoice 在全力计算时，Python 的主线程（负责界面响应、Whisper 调度）是可以并发运行的！

------

### 3. 具体的防卡顿配置方案 (Actionable Advice)

为了确保万无一失，你需要在代码中实施以下微调：

#### A. 限制 SenseVoice 的 CPU 核心数 (关键!)

不要让 ONNX Runtime 默认占满所有核。

修改位置：backend/app/services/sensevoice_onnx_service.py (假设位置)

修改逻辑：

Python

```
import multiprocessing
import onnxruntime as ort

class SenseVoiceONNXService:
    def load_model(self):
        # 获取物理核心数
        total_cores = multiprocessing.cpu_count()
        
        # 【关键策略】保留 2 个核心给系统和 Whisper，剩下的给 SenseVoice
        # 如果是 4核CPU，就给 2核；如果是 16核，给 14核。
        intra_op_threads = max(1, total_cores - 2)

        sess_options = ort.SessionOptions()
        # 限制单次运算的并行线程数
        sess_options.intra_op_num_threads = intra_op_threads 
        # 限制图优化的并行级别
        sess_options.inter_op_num_threads = 1 
        
        self.model = ort.InferenceSession(
            model_path, 
            sess_options, 
            providers=['CPUExecutionProvider']
        )
```

#### B. 确保 Whisper 真的在用 GPU

**修改位置**：`backend/app/services/whisper_service.py`

务必确认 `device="cuda"`，否则如果 Whisper 也跑在 CPU 上，那真的会卡死。

#### C. 任务优先级 (可选)

如果是 Windows 系统，可以在启动 Worker 时降低进程优先级（但这在 Python 脚本内部比较难精细控制，通常限制 CPU 线程数就足够了）。

------

### 4. 极端情况推演

**场景：你的电脑配置较低（例如 4核 CPU + 入门级 GPU）**

1. **SenseVoice (CPU)** 启动，占用 2 个核。
2. **Whisper (GPU)** 启动，CPU 调度线程占用 0.5 个核。
3. **系统 + UI**：占用剩下的 1.5 个核。
4. **运行状态**：
   - SenseVoice 快速产出 5 个草稿，前端秒级上屏。
   - 队列满了，SenseVoice 暂停（CPU 占用率瞬间下降）。
   - Whisper 慢慢啃这 5 个数据。
   - Whisper 啃完一个，队列空出一个位置，SenseVoice 瞬间唤醒，补一个，又暂停。
5. **用户感受**：
   - 风扇可能会转（因为 GPU 在跑）。
   - 鼠标不会卡（因为保留了核心）。
   - **内存**：平稳，只占用几百 MB。

### 总结

你的担心在**“无限制的并发”**下是成立的，但在**“受控的生产者-消费者队列”**模式下是不成立的。

推荐做法：

采用我之前提出的三段式异步流水线 + 限制 SenseVoice 线程数 (Total - 2)。这是工业界处理 CPU/GPU 混合负载的标准最佳实践。它比串行更高效，比无脑并行更稳定。