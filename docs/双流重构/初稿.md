这是一个非常棒的架构演进。你现在的目标是从“硬编码的函数调用”进化到\*\*“可配置的异步计算图（Async Computational Graph）”\*\*。

这套架构的核心在于：**节点（Node）是无状态的工人，队列（Queue）是输送带，而图（Graph）决定了工厂的流水线形状。**

以下是最终的**节点定义**、**管道设计**以及\*\*统一管理（调配器）\*\*的完整实现蓝图。

-----

### 1\. 核心抽象：基础积木

首先，我们需要定义“在管道里流动的是什么”以及“处理它的工人长什么样”。

#### 1.1 数据载体 (Payload)

所有节点之间传递的不再是简单的参数，而是一个携带了完整信息的“箱子”。

```python
from dataclasses import dataclass, field
from typing import Optional, List, Any, Dict
import numpy as np

@dataclass
class StreamPayload:
    """在管道中流动的标准数据包"""
    # 基础信息
    chunk_index: int
    audio_chunk: Any  # AudioChunk 对象
    
    # 过程数据 (由不同节点填充)
    sv_result: Optional[Dict] = None       # SenseVoice 结果
    whisper_result: Optional[Dict] = None  # Whisper 结果
    
    # 最终数据
    aligned_sentences: List[Any] = None    # 对齐后的句子
    
    # 控制信号
    is_end_of_stream: bool = False         # 结束标记 (Poison Pill)
    error: Optional[Exception] = None      # 错误携带
```

#### 1.2 节点基类 (Base Node)

节点不再直接调用下一个函数，而是**从输入队列读，向输出队列写**。

```python
import asyncio
from abc import ABC, abstractmethod

class BaseNode(ABC):
    def __init__(self, name: str, config: dict):
        self.name = name
        self.config = config
        self.input_queues: List[asyncio.Queue] = []
        self.output_queues: List[asyncio.Queue] = []
        self._task: Optional[asyncio.Task] = None

    def add_input(self, q: asyncio.Queue):
        self.input_queues.append(q)

    def add_output(self, q: asyncio.Queue):
        self.output_queues.append(q)

    async def start(self):
        """启动节点的主循环"""
        self._task = asyncio.create_task(self._run_loop())

    async def _run_loop(self):
        print(f"[{self.name}] Started.")
        try:
            while True:
                # 1. 获取输入 (对于多输入的 Barrier 节点，逻辑会不同)
                payload = await self._get_input()
                
                # 2. 检查结束信号
                if payload.is_end_of_stream:
                    await self._broadcast_output(payload)
                    break
                
                # 3. 核心处理
                try:
                    result_payload = await self.process(payload)
                except Exception as e:
                    print(f"[{self.name}] Error: {e}")
                    payload.error = e
                    result_payload = payload

                # 4. 发送输出
                if result_payload:
                    await self._broadcast_output(result_payload)
        finally:
            print(f"[{self.name}] Stopped.")

    async def _get_input(self) -> StreamPayload:
        # 默认实现：只从第一个队列读
        return await self.input_queues[0].get()

    async def _broadcast_output(self, payload: StreamPayload):
        # 将结果复制发送给所有下游 (Fan-out)
        for q in self.output_queues:
            await q.put(payload)

    @abstractmethod
    async def process(self, payload: StreamPayload) -> StreamPayload:
        pass
```

-----

### 2\. 具体节点实现

我们需要实现几种关键类型的节点来支持串行和并行。

#### 2.1 业务逻辑节点 (Workers)

**SenseVoiceNode**:

```python
class SenseVoiceNode(BaseNode):
    async def process(self, payload: StreamPayload) -> StreamPayload:
        # 模拟调用底层 Service
        # 注意：这里用 run_in_executor 避免阻塞事件循环
        loop = asyncio.get_running_loop()
        # 假设 svc 是预先初始化好的服务实例
        result = await loop.run_in_executor(
            None, 
            self.config['sv_service'].transcribe, 
            payload.audio_chunk
        )
        payload.sv_result = result
        return payload
```

**WhisperNode**:

```python
class WhisperNode(BaseNode):
    async def process(self, payload: StreamPayload) -> StreamPayload:
        # 同样，在 Executor 中运行以释放 GIL
        loop = asyncio.get_running_loop()
        result = await loop.run_in_executor(
            None, 
            self.config['whisper_service'].transcribe, 
            payload.audio_chunk
        )
        payload.whisper_result = result
        return payload
```

#### 2.2 结构控制节点 (Structural Nodes)

这是实现并行架构的关键。

**DistributorNode (分流器 - 1进N出)**:

  * 逻辑：它不需要做任何处理，基类的 `_broadcast_output` 已经实现了将同一个 Payload 发送给所有下游队列的功能。这就实现了“分叉”。

**BarrierNode (栅栏/拉链 - N进1出)**:

  * 逻辑：它需要等待同一个 `chunk_index` 的包从所有输入队列到达后，合并为一个包。

<!-- end list -->

```python
class BarrierNode(BaseNode):
    def __init__(self, name, config):
        super().__init__(name, config)
        self.buffers = {} # {queue_id: {chunk_index: payload}}
        self.next_index = 0

    async def _run_loop(self):
        # 覆盖主循环，实现多路复用等待
        while True:
            # 轮询所有输入队列 (实际可以用 asyncio.wait)
            for i, q in enumerate(self.input_queues):
                if not q.empty():
                    payload = await q.get()
                    if payload.is_end_of_stream:
                        # 处理结束逻辑...
                        pass
                    
                    # 存入缓冲区
                    if i not in self.buffers: self.buffers[i] = {}
                    self.buffers[i][payload.chunk_index] = payload

            # 检查是否凑齐了 next_index 的所有分片
            if all(self.next_index in self.buffers.get(i, {}) for i in range(len(self.input_queues))):
                # 凑齐了！合并！
                merged_payload = self.buffers[0][self.next_index]
                # 把其他队列里的结果合并进来
                for i in range(1, len(self.input_queues)):
                    other = self.buffers[i][self.next_index]
                    if other.whisper_result:
                        merged_payload.whisper_result = other.whisper_result
                    if other.sv_result:
                        merged_payload.sv_result = other.sv_result
                
                # 清理并发送
                for i in range(len(self.input_queues)):
                    del self.buffers[i][self.next_index]
                
                self.next_index += 1
                await self._broadcast_output(merged_payload)
            
            await asyncio.sleep(0.01) # 避免空转死循环
```

-----

### 3\. 统一调配器 (Graph Orchestrator)

这是“上帝类”，根据配置文件动态连线。

```python
class PipelineGraph:
    def __init__(self):
        self.nodes: List[BaseNode] = []
        self.queues: List[asyncio.Queue] = []

    def add_node(self, node: BaseNode):
        self.nodes.append(node)
        return node

    def link(self, source: BaseNode, target: BaseNode):
        """连接两个节点：Source -> Queue -> Target"""
        q = asyncio.Queue(maxsize=10) # 设置背压，防止内存爆掉
        self.queues.append(q)
        source.add_output(q)
        target.add_input(q)

    async def run(self, source_generator):
        # 1. 启动所有节点
        tasks = [node.start() for node in self.nodes]
        
        # 2. 喂入数据 (Source Node 的逻辑)
        # 这里假设图的第一个节点是 SourceNode
        source_node = self.nodes[0]
        # ... 注入逻辑 ...

        # 3. 等待结束
        await asyncio.gather(*tasks)

class PipelineFactory:
    @staticmethod
    def build(config: dict, services: dict) -> PipelineGraph:
        graph = PipelineGraph()
        
        # 通用节点
        source = graph.add_node(AudioSourceNode("Source", config))
        align = graph.add_node(AlignmentNode("Align", config))
        output = graph.add_node(OutputNode("Output", config))

        mode = config.get("pipeline_mode", "parallel_dual")

        if mode == "parallel_dual":
            # === 并行双流拓扑 ===
            #       /-> SV -> \
            # Source           Barrier -> Align -> Out
            #       \-> WH -> /
            
            distributor = graph.add_node(DistributorNode("Distributor", config))
            sv_node = graph.add_node(SenseVoiceNode("SenseVoice", config)) # CPU
            wh_node = graph.add_node(WhisperNode("Whisper", config))       # GPU
            barrier = graph.add_node(BarrierNode("Barrier", config))

            # 连线
            graph.link(source, distributor)
            
            # 分叉
            graph.link(distributor, sv_node)
            graph.link(distributor, wh_node)
            
            # 汇合
            graph.link(sv_node, barrier)
            graph.link(wh_node, barrier)
            
            # 后处理
            graph.link(barrier, align)
            graph.link(align, output)

        elif mode == "serial_dual":
            # === 串行双流拓扑 ===
            # Source -> SV -> WH -> Align -> Out
            
            sv_node = graph.add_node(SenseVoiceNode("SenseVoice", config))
            wh_node = graph.add_node(WhisperNode("Whisper", config))
            
            graph.link(source, sv_node)
            graph.link(sv_node, wh_node) # SV 的输出直接流向 WH
            graph.link(wh_node, align)   # 此时 Payload 里既有 SV 也有 WH 结果
            graph.link(align, output)

        return graph
```

-----

### 4\. 关键优势总结

1.  **真正的“搭积木”**：

      * 想并行？加个 `Distributor` 和 `Barrier`。
      * 想串行？直接 `Link(A, B)`。
      * 想加个 LLM 翻译？在 `OutputNode` 前面插一个 `LLMNode` 即可。

2.  **资源隔离与背压 (Backpressure)**：

      * 我们在 `link` 时设置了 `Queue(maxsize=10)`。
      * 如果 Whisper (GPU) 跑得慢，积压了 10 个包，SenseVoice (CPU) 的输出队列就会满，SenseVoice 就会自动暂停（`await put` 被阻塞）。这天然防止了内存爆炸，不需要写复杂的流控逻辑。

3.  **异构计算支持**：

      * SenseVoice Node 可以配置为运行在 `ProcessPoolExecutor` (多核CPU)。
      * Whisper Node 可以配置为运行在 `ThreadPoolExecutor` (GPU IO阻塞)。
      * 它们互不干扰，操作系统负责调度。

4.  **容错与监控**：

      * 每个 Node 都是一个独立的 `While True` 循环。你可以在 `BaseNode` 里加 `try-catch`，如果某个节点挂了，可以立即上报给 Orchestrator，甚至自动重启该节点，或者动态切换路由（例如 Whisper 挂了，直接把 Distributor 连到 Align，降级为纯 SenseVoice）。

### 5\. 开发路线图建议

1.  **Step 1: 基础设施**：实现 `StreamPayload`, `BaseNode`, `PipelineGraph`。
2.  **Step 2: 包装原子能力**：把现有的 `SenseVoiceONNXService` 和 `WhisperService` 包装进 `process()` 方法。
3.  **Step 3: 实现串行图**：先跑通 `Source -> SV -> WH -> Align`，验证数据流转正确。
4.  **Step 4: 实现并行图**：实现 `BarrierNode`，跑通并行模式。

这套架构一旦完成，你的 `video-to-srt` 项目将拥有工业级的鲁棒性和扩展性。