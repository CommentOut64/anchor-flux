**现状评估**

- 音频前处理早已换成 Silero VAD+Demucs 的流水线，ChunkEngine.process_audio() 在 backend/app/services/audio/chunk_engine.py (line 141) 就强制走 VAD 切分，并把每段封装成 AudioChunk；因此用户建议里的“短音频过滤+拼接太粗糙，需要引入 VAD”在现实现有流程中已经实现，问题不在这一环。
- Whisper 在标准慢流里是按单个 AudioChunk 推理，且在 backend/app/services/inference/whisper_executor.py (lines 75-83) 已默认 condition_on_previous_text=False 并关闭二次 VAD；SlowWorker._run_whisper_with_overlap() 也只在开头拼 0.5 秒上下文并带有四道幻觉检测防线（backend/app/pipelines/workers/slow_worker.py (lines 146-244)）。因此对“上下文污染”与“短片段幻觉”的担心在主流程里已经通过 prompt 构造和 gating 处理。
- 现有的重复/幻觉防线主要有三层：TextNormalizer.REPEATED_PATTERN 只能抓连续的完全重复子串（backend/app/services/text_normalizer.py (lines 36-107)）、SlowWorker 的下划线/avg_logprob/no_speech 判定（backend/app/pipelines/workers/slow_worker.py (lines 187-244)），以及 AlignmentWorker 在文本长度暴涨且低置信度时直接回退 SenseVoice（backend/app/pipelines/workers/alignment_worker.py (lines 176-208)）。但这些都没覆盖“在一句话内部多次复读但没有完全一致 pattern”的场景，所以用户提到的 goods and services 样例仍可能漏网。
- 代码里确实存在短句缓冲池模式（backend/app/services/whisper_buffer_pool.py）。这个模式会把多个 VAD chunk 拼成最长 30 秒的音频并在 process_buffer() 里重新启用 condition_on_previous_text=True（backend/app/services/whisper_buffer_pool.py (line 839)）。如果缓冲池恰好跨越两道题或长停顿，确实可能重新引入用户所说的“幽灵句子”。
- Faster‑Whisper 调用目前只用到了 beam_size、temperature、suppress_tokens 等常规参数（backend/app/services/whisper_service.py (lines 452-488)），并没有暴露 repetition_penalty、no_repeat_ngram_size 一类的解码器约束，因此模型端抑制重复的空间还没被利用。

**对建议的采纳判断**

- “必须引入 VAD 切分”这一条与当前实现重复，没必要额外投入。
- “禁用 condition_on_previous_text”在主慢流里已经落地，所以再去做一次不会带来收益；但缓冲池分支确实重新开启了该开关，如果缓冲池在你的视频上才是罪魁祸首，就应该把这条建议局部套用到 WhisperBufferService 上，而不是全局禁用。
- “调 repetition_penalty / no_repeat_ngram_size”目前尚未实现，且成本低，可以视作值得尝试的 quick win；不过需要确认 faster-whisper 的该参数是否生效（ct2 版是支持的）。
- “做 N-gram 级别的块内去重”在代码中缺位（只有长度比对和简单正则），而你手上的 SRT 已经证明现有规则无法覆盖，所以非常值得补一层滑动窗口/压缩比检测，无论是在 TextNormalizer 里还是在 AlignmentWorker 的结果阶段。
- “不要拼接短音频”在标准流水线中已经做到，但缓冲池模式确实会拼，这个策略本身就是为了降低短片段幻觉：如果你用的是该模式且复读依旧，那说明它没有达到目的，需要更精细的触发条件或串行推理，而不是简单地“一律不要拼”。
- “正则修题号”“滑动窗口去重”等后处理建议都还没实现，且投入小，可以一起排进 TODO。

**可行的改进方案**

1. **块内重复检测**：在 TextNormalizer.clean_whisper_output() 或 AlignmentWorker 进入 SRT 阶段前加一层 N‑gram/压缩比检测；例如计算句子里 3‑5 词短语的出现次数或检测 len(set(chunks))/len(chunks)，异常时截断或回退 (backend/app/services/text_normalizer.py (lines 36-118))。这能精准打击用户指出的“在同一条字幕里循环”的问题。
2. **模型端抑制**：扩展 WhisperService.transcribe() 的参数，把 repetition_penalty、no_repeat_ngram_size 暴露给配置，然后在 WhisperExecutor 或缓冲池模式中提供>1 的 penalty；这是最直接的方式，成本也只是在调用处加几个关键字参数（backend/app/services/whisper_service.py (lines 452-474)）。
3. **缓冲池策略微调**：针对 WhisperBufferService，在触发条件 A（遇到长静音）或 B（跨题边界）时强制 condition_on_previous_text=False，并在 flush 后重置 prompt，避免上一池的输出继续影响下一池；必要的话把最大拼接长度从 30s 降到 15s（backend/app/services/whisper_buffer_pool.py (lines 31-104) + 839）。这样既保留了长上下文优势，又避免跨题幽灵。
4. **语言/题号正则修复**：在 TextNormalizer 里补一条 re.sub(r'Question(\d+)', r'Question \\1', text) 类似的规则，以及在 SemanticGrouper 后加一次 short_sentence 过滤，把 <3 个词的残段与上下句合并或剔除。

综上，用户的诊断里只有“需要 VAD”这一点与现实不符；其他关于块内复读、缓冲池污染以及解码参数的建议都有对应的缺口或改进空间。建议优先落地模型端 penalty + 块内重复检测，再针对你实际启用的缓冲池模式微调 condition_on_previous_text 和触发条件，基本就能把 92%→98% 的空间吃掉。


这是一个非常深刻的痛点。你指出的矛盾在于：

1. **切片太碎** -> Whisper 失去上下文 -> 严重幻觉。
2. **强制拼接** -> 需要缓冲等待 -> 速度变慢，且简单的拼接依然可能在接缝处产生“伪边界”导致幻觉。
3. **SenseVoice 兜底** -> 缺乏标点和大小写 -> 字幕观感差。

你已经尝试了工程层面的“修补”（Prompt 优化、Overlap、简单缓冲），但效果不佳。这说明问题出在**输入数据的形态**上。

我为你提出一个新的思路：**“基于 SenseVoice 预判的语义级动态重切片（Semantic-Aware Dynamic Re-chunking）”**。

这个思路的核心在于：**彻底放弃“VAD 切在哪里，Whisper 就跑在哪里”的执念。**

---

###新思路：语义驱动的音频重组 (Semantic-Driven Audio Reconstruction)####核心逻辑SenseVoice 跑得非常快，而且它是 CTC 架构，对边界极其敏感。我们可以把 SenseVoice 升级为 Whisper 的**“超级探针”**和**“调度员”**，而不是简单的备胎。

**不再是：**
`VAD Chunk` -> `Whisper`

**而是：**
`VAD Chunk` -> `SenseVoice` -> **[语义完整性判断]** -> `动态合并音频` -> `Whisper`

---

####具体实施方案我们需要在 `FastWorker` 和 `SlowWorker` 之间引入一个智能的 **`SemanticBuffer`（语义缓冲区）**。

####1. 阶段一：SenseVoice 的“探针”作用当 `FastWorker` (CPU) 拿到 VAD 切出来的 Chunk（比如 2秒）时，先跑 SenseVoice。
SenseVoice 虽然标点不准，但它能非常准确地识别出**“静音”**和**“词的结束”**。

此时，引入一个轻量级的逻辑判断：

* **探针判断**：这段文字看起来像是一句话的结尾吗？
* 判断依据 1：SenseVoice 输出的最后 0.2秒 是静音吗？
* 判断依据 2：SenseVoice 识别出的最后一个词是虚词（the, a, and）还是实词？
* 判断依据 3：当前累计的音频时长是否超过了“Whisper 舒适区”（例如 8秒）？



####2. 阶段二：动态音频重组 (The Key Innovation)这是解决幻觉的关键。

* **情况 A：句子没说完**
* SenseVoice 输出："hello everyone welcome"
* 动作：**不发送给 Whisper！**
* 操作：把这段音频 `append` 到 `SemanticBuffer` 里。
* 前端：SSE 推送草稿 "hello everyone welcome"，用户看到字在跳，但没变定稿。


* **情况 B：句子说完了（或 buffer 满了）**
* 下一个 Chunk SenseVoice 输出："to my channel"
* 探针发现：Buffer 里的内容 + 当前内容 = "hello everyone welcome to my channel"（时长 5秒，且尾部有静音）。
* 动作：**触发 Whisper 推理**。
* **关键操作（防幻觉核武器）—— 声学重叠（Acoustic Overlap）：**
* 不要只把这 5秒 发给 Whisper。
* **取出上一次推理的最后 0.5s ~ 1s 音频，拼在本次音频的最前面。**
* **Audio Payload** = `[Prev_Tail_1s] + [Buffer_5s]`。
* 告诉 Whisper：`prompt="..."` (上文文本)。
* **推理后处理**：直接丢弃 Whisper 输出结果的前面对应时间段的文本。





**为什么这能解决幻觉？**

1. **消灭短切片**：Whisper 永远不会看到“破碎的 2秒音频”，它看到的永远是“语义完整的 5~10秒音频”。
2. **消除边界效应**：通过**物理音频的前置拼接（Pre-padding）**，Whisper 的 Transformer 内部状态（Encoder States）在处理真正内容前已经“预热”好了。这比单纯用 Prompt 文本更有效，因为 Prompt 影响的是 Decoder，而音频 Context 影响的是 Encoder。

####3. 阶段三：对齐与回写 (The Mapping)现在 Whisper 输出了一句完美的、带标点的："Hello everyone, welcome to my channel."

* **对齐难点**：Whisper 的音频是拼接过的，时间戳是相对于这段拼凑音频的。
* **映射逻辑**：
* Whisper 起始时间 0s = Buffer 起始时间 T。
* 利用 SenseVoice 之前生成的字级时间戳作为“骨架”。
* 把 Whisper 的**标点符号**和**大小写**“移植”到 SenseVoice 的时间轴上。



---

###针对“SenseVoice 兜底无标点”的补充方案如果置信度依然很低，必须回退到 SenseVoice，但你又无法接受无标点：

**方案：引入微型标点模型 (Punctuator)**

* 不要指望 Whisper 修复 SenseVoice 的标点（如果 Whisper 已经挂了）。
* 在 CPU 上跑一个**极小的 BERT-based 标点恢复模型**（ONNX 格式，几十 MB）。
* **流程**：
* Whisper 失败 -> 回退 SenseVoice 文本。
* SenseVoice 文本 -> `Punctuator` (CPU 10ms) -> 带标点文本。


* **资源消耗**：极低，完全可以在 `FastWorker` 或 `AlignmentWorker` 里跑。
* **开源选型**：DeepCyber/punctuation-restoration (即使是小模型效果也比裸奔好太多)。

---

###总结：你的新开发路径不要再在“VAD 切片”上做文章了，那个粒度对 Whisper 来说是无解的。

1. **FastWorker (CPU)**：
* 继续跑 VAD 切片（保证快流速度）。
* 增加一个 `PendingBuffer`。
* 逻辑：`PendingBuffer.append(chunk)`。只有当 `len(PendingBuffer) > 5s` OR `SenseVoice_Detect_Silence > 0.5s` 时，才打包发送给 `SlowWorker`。


2. **SlowWorker (GPU)**：
* 接收打包好的“长音频”（Long Audio）。
* **重要**：在 Long Audio 前面拼上 `Last_Audio_Tail` (1s)。
* 跑 Whisper Large。
* 切掉开头，输出结果。


3. **AlignmentWorker**：
* 将 Whisper 的“长文本”拆解，分配回原始的那些 VAD Chunk 的时间段里。



**这种“VAD 负责切，SenseVoice 负责粘，Whisper 负责润”的策略，能最大程度保证 Whisper 永远工作在它最擅长的“长句模式”下，从而从根源上消灭因边界缺失导致的幻觉。**