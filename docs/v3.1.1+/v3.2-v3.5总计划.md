### Phase 1: 语义重构 (Semantic Refactoring) —— v3.2.x

**核心目标**：彻底消灭因为切断句子导致的 Whisper 幻觉。这是地基。

这个阶段不需要引入 NISQA，也不要搞乱序执行，专注把“数据流”改对。

* **Step 1.1: 标点模型接入 (Punctuation Service)**
* **动作**: 引入 `CT-Transformer` 或 `FunASR` 的标点模型。
* **产出**: `FastWorker` 能输出带标点的文本，不仅是文本，还要输出 `split_points`（分割点时间戳）。


* **Step 1.2: 语义缓冲机制 (Semantic Buffering)**
* **动作**: 改造 `FastWorker`。不再来一个 Chunk 推一个。建立一个内部 Buffer，直到遇到标点或 buffer 满（15s-20s）才生成一个 `SemanticChunk`。
* **关键**: 这改变了 `ProcessingContext` 的粒度。以前 `Chunk` 是物理切片，现在是语义切片。


* **Step 1.3: 对齐适配 (Alignment Adaptation)**
* **动作**: 修改 `AlignmentService`。
* **挑战**: Whisper 现在处理的是长句子（可能包含多个原来的物理 Chunk）。你需要把 SenseVoice 的时间戳（属于多个物理小 Chunk）拼接起来，跟 Whisper 的长文本对齐。
* **方案**: 也就是之前讨论的“基于句子的精准对齐”。



**里程碑**: 字幕不再出现半句话被切断的情况，Whisper 幻觉率下降 80%。

---

### Phase 2: 神经中枢 (The Neural Bridge) —— v3.3.x

**核心目标**：解耦决策逻辑，建立“仲裁法庭”。

有了 v3.2 的长句子流，SlowWorker 会变得很重。这时需要把“决策逻辑”抽离出来。

* **Step 2.1: 架构解耦 (Bridge Controller)**
* **动作**: 创建 `backend/app/services/bridge/`。
* **实现**: 将原本散落在 `SlowWorker` 里的 Prompt 构建逻辑、幻觉检测逻辑，全部移入 `BridgeController`。
* **逻辑**: `SlowWorker` 变成一个纯粹的执行者：“Bridge 叫我跑我就跑，Bridge 叫我跳过我就跳过”。


* **Step 2.2: 上下文融合 (Context Fusion)**
* **动作**: 在 Bridge 中实现高级 Prompt 构建器。
* **功能**: 综合 `Prev_Final_Text` (上文定稿) + `Current_SV_Draft` (当前 SenseVoice 草稿) 生成 Prompt。如果 SV 草稿质量低（暂用简单的置信度判断），则不采纳。


* **Step 2.3: 仲裁回退机制 (Arbitration)**
* **动作**: 实现“双流校验”。如果 Whisper 结果和 SenseVoice 差异过大，Bridge 决定信谁。初步策略可以是“信 SenseVoice 时间戳，信 Whisper 内容，除非 Whisper 也是乱码”。



**里程碑**: 代码结构清晰，SlowWorker 不再臃肿，具备了初步的“自我纠错”能力。

---

### Phase 3: 感知升级与效率优化 (Perception & Speed) —— v3.4.x

**核心目标**：让系统“看”得更准（质量评估），“跑”得更快（乱序执行）。

这时候你的 Bridge 已经建好了，可以给它装上更高级的传感器（NISQA）了。

* **Step 3.1: 质量评估模型替换 (Quality Assessment)**
* **动作**: 引入 **WavLM** 或 **NISQA** (Non-Intrusive Speech Quality Assessment)。
* **替换 YAMNet**: YAMNet 只能分辩“是人声/是音乐”。NISQA 能打分（MOS Score）。
* **集成到 Bridge**: 在 `Quality Guard` 模块中，当 SenseVoice 返回空时，用 NISQA 检查音频。
* 如果是“高质量人声” -> **判定 SV 漏字** -> 强制 Whisper 补刀。
* 如果是“强噪声音频” -> **判定环境恶劣** -> 放弃或用特殊模型。




* **Step 3.2: 快流乱序执行 (Fast Stream Out-of-Order)**
* **背景**: 标点模型和 Semantic Buffer 会导致 CPU 处理时间不均匀。
* **动作**: 将 `FastWorker` 改为并行处理池。
* **关键组件**: `SequencedQueue` (重排序队列)。虽然 Worker 是乱序跑的（短句先跑完），但推送到前端和 SlowWorker 必须按 `ChunkID` 排序。
* **实现**: 这是一个纯工程优化，不涉及 AI 逻辑，但能大幅提升 UI 响应速度。



**里程碑**: 系统能精准识别“漏字”还是“静音”，UI 丝般顺滑。

---

### Phase 4: 智慧赋能 (LLM Intelligence) —— v3.5.x

**核心目标**：人类级水平的校对与翻译。

这是最后的“精装修”。

* **Step 4.1: LLM 仲裁官 (LLM Arbiter)**
* **动作**: 将 LLM 接入 Bridge。
* **场景**: 当 SV 和 Whisper 打架，且置信度都差不多时，把两者的文本 + 上下文发给 LLM：“这句话到底是在说什么？”


* **Step 4.2: LLM 校对与翻译 (Proofreading & Translation)**
* **动作**: 这是一个独立于 Pipeline 之外的**后处理步骤**。
* **逻辑**: 字幕全部生成完毕后，打包发给 LLM 进行全文润色或翻译。

---

### Neural Bridge 内部模块划分设计

建议在 `backend/app/services/bridge/` 目录下构建以下模块结构：

```text
backend/app/services/bridge/
├── __init__.py           # 暴露 BridgeController
├── controller.py         # [总司令] 门面模式，对外提供统一接口
├── quality_guard.py      # [看门人] 负责音频质量评分 (YAMNet/NISQA)
├── context_engine.py     # [军师]   负责构建 Prompt (Context Fusion)
├── arbiter.py            # [法官]   负责幻觉判定与冲突仲裁
├── llm_adapter.py        # [外援]   负责与 LLM 通信 (可选模块)
└── memory_bank.py        # [书记官] 负责短期记忆 (Cross-Chunk Context)

```

#### A. BridgeController (总控制器)

* **职责**：它是 `SlowWorker` 唯一可见的对象。它不处理具体逻辑，只负责**调度**。
* **逻辑**：
1. 调用 `QualityGuard` 检查音频。
2. 调用 `ContextEngine` 拿 Prompt。
3. 指挥 `SlowWorker` 跑推理。
4. 拿到结果后，调用 `Arbiter` 进行清洗和仲裁。

#### B. QualityGuard (质量卫士)

* **职责**：只关心**音频本身**和 **SenseVoice 原始输出**。
* **核心方法**：
```python
def assess(self, audio_chunk, sv_result) -> QualityReport:
    # 1. 运行 YAMNet 检查是否有人声
    # 2. 运行 NISQA 检查音频是否含噪
    # 3. 检查 SenseVoice 是否丢字 (有人声但无字)
    pass
```


* **独立性理由**：这一块未来会集成 NISQA 或 WavLM，需要独立的模型加载和显存管理，必须隔离。

#### C. ContextEngine (上下文引擎)

* **职责**：接管现有的 `PromptBuilder` 并升级。
* **核心方法**：
```python
def fuse_context(self, history_text, current_sv_draft, quality_report) -> str:
    # 智能决策：
    # 如果 QualityReport 说 SV 质量差，则 Prompt 不包含 SV 草稿
    # 如果历史文本太长，进行摘要 (v3.5+)
    pass
```


* **独立性理由**：Prompt 工程是一个极其精细的活，未来可能涉及 Token 计算、关键词加权等复杂逻辑。

#### D. Arbiter (仲裁法官)

* **职责**：接管 `SlowWorker` 中的 `_is_hallucination` 逻辑。
* **核心方法**：
```python
def judge(self, whisper_result, sv_result) -> FinalResult:
    # 1. 幻觉检测 (重复、下划线、置信度)
    # 2. 双流一致性校验 (Levenshtein 距离)
    # 3. 决定是否回退到 SV 或调用 LLM
    pass
```

### 数据流向图

新架构下的处理流程将变得极其清晰：

1. **Input**: `FastWorker` 产出的 `SemanticChunk` 进入 `BridgeController`。
2. **Phase 1 (Pre-Inference)**:
* `Controller` -> `QualityGuard`: "这音频能用吗？SV 结果靠谱吗？"
* `Controller` -> `ContextEngine`: "给我一个最好的 Prompt。"


3. **Action**:
* `Controller` -> `SlowWorker`: "带着这个 Prompt，去跑 Whisper。"


4. **Phase 2 (Post-Inference)**:
* `Controller` -> `Arbiter`: "Whisper 跑完了，结果是 X，SV 结果是 Y。你觉得信谁？"
* (`Arbiter` 可能会偷偷调用 `LLMAdapter` 如果它拿不准)


5. **Output**: `BridgeController` 返回最终定稿给 `Pipeline`。

### 实施路线建议

不需要一次性写完所有模块。按以下步骤逐步“细胞分裂”：

1. **v3.3 (骨架搭建)**:
* 创建 `backend/app/services/bridge/` 文件夹。
* 将 `SlowWorker` 中的 `_is_hallucination` 移动到 `arbiter.py`。
* 将 `SlowWorker` 中的 `_build_safe_prompt` 移动到 `context_engine.py`。
* 让 `SlowWorker` 调用 `BridgeController`。
* *此时逻辑没变，只是代码搬家了（Refactoring）。*


2. **v3.4 (器官升级)**:
* 升级 `QualityGuard`：引入 YAMNet/NISQA 逻辑。
* 升级 `ContextEngine`：引入“SV 草稿融合”逻辑。


3. **v3.5 (大脑接入)**:
* 实现 `llm_adapter.py`。
* 升级 `Arbiter`：在拿不准时调用 LLM。
