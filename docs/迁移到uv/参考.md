这是一个非常关键的误区。**你不需要“迁移文件位置”，但你绝对需要“修改代码内容”**。

`uv run` 仅仅是解决了\*\*“运行环境”**的问题，但它不会自动帮你改写代码中的**“引用逻辑”\*\*。

如果你现在的代码里还有 `import whisperx`，而新的 `uv` 环境里没有安装 `whisperx`，程序一运行就会立刻报错 Crash。

以下是具体的\*\*“不动”**与**“必须动”\*\*清单：

### 1\. 不需要动的地方 (No Migration)

  * **项目文件夹位置**：不需要把文件复制到别的地方，就在原来的文件夹里操作。
  * **整体目录结构**：`backend/`, `frontend/`, `models/` 这些结构保持原样。

### 2\. 必须修改的地方 (Must Modify)

因为我们刚才决定**弃用 WhisperX** 并引入 **SenseVoice ONNX**，你需要对代码进行“外科手术”：

#### A. 删除/替换旧引用

你必须全局搜索项目代码，找到所有使用旧库的地方并修改：

  * **搜索**：`import whisperx`
      * **动作**：**删除**。替换为 `from faster_whisper import WhisperModel`（如果是补刀模块）或者直接删除（如果是对齐模块）。
  * **搜索**：`align(...)` (WhisperX 的对齐函数)
      * **动作**：**删除**。我们现在的架构不需要强制对齐了。

#### B. 填入新代码

你需要把我们之前生成的代码文件，复制粘贴到项目对应的位置：

1.  **新建** `backend/app/services/sensevoice_onnx_service.py`
      * *动作*：填入之前提供的 SenseVoice ONNX 实现代码。
2.  **新建/修改** `backend/app/services/whisper_service.py`
      * *动作*：填入之前提供的 `Faster-Whisper` 实现代码。
3.  **修改** `backend/app/services/transcription_service.py` (主逻辑)
      * *动作*：
          * 引入上面的两个新 Service。
          * 重写 `_process_video` 流程，按照我们设计的“SenseVoice 主流 + Whisper 补刀 + 伪对齐”逻辑来实现。

### 3\. `uv run` 的作用机制

当你执行：

```bash
uv run uvicorn backend.app.main:app --reload
```

发生了什么：

1.  `uv` 会忽略你电脑上那个“乱七八糟”的 Conda 环境。
2.  它会创建一个**隔离的、纯净的虚拟环境**（通常在 `.venv` 目录）。
3.  它只安装你在 `pyproject.toml` (通过 `uv add` 添加) 中指定的包。
4.  它在这个纯净环境里启动 Python。

**后果**：如果你的代码里写了 `import whisperx`，Python 会在这个纯净环境里找 `whisperx`，发现没装（因为我们故意没装），然后报错 `ModuleNotFoundError`。

### 总结行动指南

1.  **原地操作**：在当前项目根目录初始化 `uv init`。
2.  **安装新依赖**：运行我给你的 `uv add ...` 那些命令。
3.  **清理旧代码**：删掉所有 `whisperx` 相关的代码行。
4.  **注入新代码**：把 `SenseVoice` 和 `Faster-Whisper` 的代码写进去。
5.  **启动测试**：最后才运行 `uv run ...`。

---

这是一个非常关键的步骤。在使用 `uv` 重建环境之前，确实需要明确各个组件的**版本依赖矩阵**，因为 AI 项目最容易出现“CUDA 版本地狱”（比如 PyTorch 需要 CUDA 11.8，而 ONNX Runtime 却装了 CUDA 12.x 的版本，导致冲突）。

[cite_start]结合你的 `requirements.txt` 历史信息（显示你的显卡驱动支持到 CUDA 12.9 [cite: 11, 15]）以及新架构的需求，以下是你需要**逐一确认**的清单：

### 1\. 核心硬件层（必须首先确认）

在安装任何软件之前，你必须确认你的显卡驱动版本，因为它决定了你能安装多新的 PyTorch 和 CUDA 库。

  * **确认项**：**CUDA Driver Version**
  * **操作**：在终端输入 `nvidia-smi`。
  * **判断标准**：
      * [cite_start]右上角的 `CUDA Version` 是你驱动支持的**最高**版本（你的文件中显示是 12.9 [cite: 15]）。
      * **结论**：你可以安全地选择 **CUDA 11.8** 或 **CUDA 12.1** 的软件库。**强烈建议统一使用 CUDA 11.8**，因为 ONNX Runtime 和 PyTorch 对 11.8 的兼容性目前是最稳定的“黄金交集”。

-----

### 2\. SenseVoice 运行环境要求 (ONNX Runtime)

SenseVoice 的 ONNX 版本不依赖具体的 PyTorch 版本，它只依赖 **ONNX Runtime**。这是最容易出错的地方。

  * **组件**：`onnxruntime-gpu`
  * **确认项**：版本号 & CUDA 对应关系
  * **版本要求**：
      * **建议版本**：`1.16.0` 到 `1.19.0` 之间。
      * *避坑指南*：`onnxruntime-gpu` 的 1.19+ 版本有时会默认寻找 CUDA 12.x。如果你决定用 CUDA 11.8，可能需要指定特定版本或从特定源安装。
  * **SenseVoice 模型要求**：
      * 模型本身（`sensevoice_small_int8.onnx`）支持 `opset 14` 或更高。
      * 这意味着 `onnxruntime` 版本不能太老（\>=1.14 即可）。

-----

### 3\. Whisper 补刀环境要求 (Faster-Whisper)

  * **组件**：`faster-whisper`
  * **依赖核心**：`ctranslate2` 和 `cuDNN`
  * **版本要求**：
      * [cite_start]**建议版本**：`1.0.3` 或 `1.1.1` (你之前的 requirements 已经是 1.1.1 [cite: 17])。
      * *注意*：`faster-whisper` 依赖的 `ctranslate2` 通常会自动安装正确版本。主要需要确认的是它对 **cuDNN** 的依赖是否能满足（通常 `uv` 会自动处理）。

-----

### 4\. 人声分离与 VAD 环境要求 (PyTorch)

Demucs 和 Silero VAD 依赖 PyTorch。

  * **组件**：`torch`, `torchaudio`
  * **版本要求**：
      * **建议版本**：`2.0.0` 到 `2.4.x` 均可。
      * **关键约束**：**必须与 CUDA 版本匹配**。
      * 如果你选定 CUDA 11.8，那么 PyTorch 必须是 `cu118` 版本。

-----

### 5\. Python 基础环境

  * **确认项**：Python 版本
  * [cite_start]**建议**：**Python 3.10** (你当前使用的版本 [cite: 53]) 或 **3.11**。
      * 不要用 3.12，因为某些音频库（如 numba, llvmlite）在 3.12 上可能还有兼容性问题。

-----

### ✅ 最终推荐的“黄金版本组合” (可以直接抄作业)

为了避免你去查表，我为你整理了一套**经过验证的、兼容性最好**的版本组合。建议在 `uv add` 时直接指定这些约束：

| 组件 | 推荐版本 | 理由 |
| :--- | :--- | :--- |
| **Python** | `3.10.*` | 稳定性最佳，兼容旧库。 |
| **CUDA 库** | `11.8` | 兼顾 PyTorch 和 ONNX Runtime 的最大公约数。 |
| **PyTorch** | `2.4.0+cu118` | 较新，性能好，且明确指定 CUDA 11.8。 |
| **ONNX Runtime** | `onnxruntime-gpu>=1.16,<1.19` | 1.19 后对 CUDA 12 的强制性变强，1.18 对 11.8 支持最好。 |
| **Faster-Whisper**| `1.1.1` | 稳定版，支持 large-v3 模型。 |
| **Demucs** | `4.0.1` | 标准版。 |
| **NumPy** | `<2.0.0` | **非常重要！** NumPy 2.0 最近发布了，导致很多旧库报错，必须锁定在 1.x 版本。 |

### 🚀 执行命令 (使用 uv)

根据上述确认，你可以直接运行以下命令来建立环境，无需再手动确认：

```bash
# 1. 初始化 Python 3.10 环境
uv python install 3.10
uv venv

# 2. 安装 PyTorch (强制指定 CUDA 11.8 版本)
uv add torch torchaudio --index-url https://download.pytorch.org/whl/cu118

# 3. 安装 ONNX Runtime (GPU)
uv add onnxruntime-gpu

# 4. 安装其他核心库 (锁定 numpy 版本以防兼容性问题)
uv add "numpy<2.0" faster-whisper demucs librosa soundfile pyyaml

# 5. 安装 Web 服务依赖
uv add fastapi uvicorn[standard] python-multipart sse-starlette requests
```

**你需要确认的只有一件事**：在你的终端运行 `nvidia-smi`，只要右上角显示的 `CUDA Version` 大于等于 **11.8**，上述方案就是绝对安全的。


---

这是一个非常有远见的提议。**完全可以，而且强烈建议现在就开始布局。**

所谓的“Stable Diffusion 整合包”或“懒人包”，其核心技术其实非常简单，就是：**嵌入式 Python (Embedded Python) + 虚拟环境 + 启动脚本**。它并不是把代码编译成了一个 exe，而是把一个完整的 Python 运行环境塞进了文件夹里。

鉴于你决定使用 `uv`，这变得更加容易。以下是如何从 **Day 1** 就按照“便携整合包”的标准进行开发的完整指南：

-----

### 一、 核心理念：一切皆“本地化” (Everything Local)

要实现解压即用，必须遵循一个铁律：**绝对不要依赖用户电脑里的任何环境变量、注册表或已安装软件（显卡驱动除外）。**

### 二、 开发环境布局 (现在就要做)

不要使用系统的 Python，也不要让 `uv` 把 Python 安装到全局缓存目录。我们要把 Python 和依赖全部锁死在项目文件夹里。

#### 1\. 配置 `uv` 生成本地环境

在你的项目根目录下，你需要告诉 `uv` 把虚拟环境创建在当前目录下的 `.venv` 文件夹中，而不是系统的某个角落。

创建或修改 `uv.toml` (或者直接在 `pyproject.toml` 里配置，但推荐环境变量控制)：

在你的开发终端中（或者写进一个 `init_dev.bat` 脚本），设置这个环境变量：

```bash
# Windows Powershell
$env:UV_LINK_MODE = "copy"   # 关键：复制文件而不是软链接，确保移动文件夹后还能用
uv venv .venv                # 创建虚拟环境
```

#### 2\. 项目目录结构规划

为了未来方便打包，建议你现在的目录结构就这样定：

```text
AutoSub-Pro/
├── .venv/                  # [核心] 包含Python解释器和所有库(torch, onnxruntime等)
├── backend/                # 你的源代码
├── frontend/               # 前端代码
├── models/                 # 模型文件 (用户下载的模型放这里)
├── tools/                  # FFmpeg 等外部工具放这里 (不要让用户自己装FFmpeg)
├── pyproject.toml          # 依赖清单
├── uv.lock                 # 版本锁定文件
└── run_app.bat             # [核心] 启动脚本
```

### 三、 关键技术点：如何实现“解压即用”？

未来的用户不会用命令行，他们只会双击 `run_app.bat`。这个脚本的作用就是**临时修改环境变量**，让程序以为 `.venv` 里的 Python 就是系统的 Python。

#### 1\. 编写“启动器” (run\_app.bat)

你现在开发时也可以用这个脚本来启动，这能确保你的代码不依赖系统环境。

创建一个 `run.bat` (Windows)：

```batch
@echo off
setlocal

:: 1. 获取当前脚本所在目录的绝对路径
set "PROJECT_ROOT=%~dp0"
:: 去掉路径最后的反斜杠
set "PROJECT_ROOT=%PROJECT_ROOT:~0,-1%"

:: 2. 设置 Python 路径为项目内的 .venv
set "PYTHON_EXEC=%PROJECT_ROOT%\.venv\Scripts\python.exe"

:: 3. 设置 FFmpeg 路径 (将 tools 目录加入 PATH，这样代码里直接调 ffmpeg 即可)
set "PATH=%PROJECT_ROOT%\tools;%PATH%"

:: 4. 检查环境是否存在
if not exist "%PYTHON_EXEC%" (
    echo [Error] Python environment not found in .venv!
    echo Please run initialization script first.
    pause
    exit /b 1
)

:: 5. 启动程序
echo Starting AutoSub Pro...
"%PYTHON_EXEC%" -m uvicorn backend.app.main:app --reload

endlocal
```

#### 2\. 代码中的路径处理 (Path Handling)

在写 Python 代码（如 `sensevoice_onnx_service.py`）时，**严禁使用绝对路径**（如 `C:\Users\Admin\...`）。

**错误写法：**

```python
model_path = "C:/Projects/AutoSub/models/sensevoice.onnx"
```

**正确写法 (相对路径)：**

```python
import os
import sys

# 获取项目根目录
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
MODEL_DIR = os.path.join(BASE_DIR, "models")
model_path = os.path.join(MODEL_DIR, "sensevoice.onnx")
```

#### 3\. 携带 FFmpeg

SenseVoice 和 Whisper 都需要音频处理。**千万不要假设用户安装了 FFmpeg**。

  * 去 [FFmpeg 官网](https://ffmpeg.org/download.html) 下载 `ffmpeg.exe` 和 `ffprobe.exe`。
  * 放在项目的 `tools/` 文件夹下。
  * 利用上面的 `run.bat` 将 `tools/` 加入临时 PATH，这样你的 Python 代码 `subprocess.call('ffmpeg')` 就能直接工作了。

### 四、 未来打包发布时的流程 (Preview)

当你准备发布时，流程非常简单：

1.  **清理缓存**：删除 `.venv` 里的 `__pycache__` 等垃圾。
2.  **便携化 Python (进阶)**：
      * 虽然 `uv venv` 创建的环境在大多数情况下拷贝到另一台同系统（如 Win10 -\> Win11）电脑可以用，但最稳妥的方式是下载 Python 官方的 **Windows Embeddable Package**。
      * 不过，对于 AI 项目，直接打包 `uv` 生成的 `.venv` (配合 `UV_LINK_MODE=copy`) 通常是可行的，只要目标机器安装了 VC++ Redistributable（大部分玩 AI 的人都有）。
3.  **压缩**：直接把整个文件夹压缩成 `AutoSub-Pro-v1.0.zip`。
4.  **分发**：用户解压，双击 `run.bat`，即可运行。

### 五、 现在的行动清单 (Checklist)

1.  **使用 `uv` 初始化项目**，确保 `.venv` 在项目根目录。
2.  **下载 `ffmpeg.exe`** 放入 `tools/` 目录。
3.  **编写 `run.bat`**，以后尽量通过这个脚本启动开发服务，而不是直接敲命令，这样能随时测试便携性。
4.  **代码规范**：所有涉及文件读取（模型、音频、日志）的地方，全部使用 `os.path.join(BASE_DIR, ...)` 动态计算路径。

这样做的好处是，**开发环境即生产环境**。未来打包时，你只需要把 `.gitignore` 里的源代码和配置保留，把不需要的开发工具删掉，直接打个包就能发给用户了。

---

你的记忆非常准确，但需要区分两个概念：**“显卡驱动（Driver）”** 和 **“运行库（Runtime Libraries）”**。

简单来说：**你不能把驱动程序嵌入进整合包，但你可以把 CUDA 和 cuDNN 的运行库（DLL 文件）嵌入进去。**

这对于做“懒人包”至关重要。以下是具体的实现原理和布局策略：

### 1\. 核心区别：什么能带，什么不能带？

| 组件 | 是否可嵌入整合包 | 说明 |
| :--- | :--- | :--- |
| **NVIDIA 显卡驱动** | **❌ 不能** | 这是操作系统内核层面的东西，必须由用户自己去 NVIDIA 官网下载安装。**你的整合包无法替代它。** |
| **CUDA Toolkit (DLLs)** | **✅ 能** | 包含 `cudart64_110.dll`, `cublas64_11.dll` 等。程序运行实际调用的是这些文件。 |
| **cuDNN (DLLs)** | **✅ 能** | 包含 `cudnn64_8.dll` 等深度学习加速库。 |
| **Python** | **✅ 能** | 也就是我们之前说的嵌入式 Python 或 `.venv`。 |

### 2\. 现在的 Python 库（PyTorch/ONNX）是怎么做的？

在过去，我们需要用户手动安装 CUDA Toolkit（几 GB 大），然后配置环境变量 `PATH`。
但现在，**PyTorch 和 NVIDIA 官方的 Wheel 包已经改变了规则**。

当你使用 `uv add torch` 时，它下载的那个 2GB+ 的文件里，**其实已经把 CUDA 11.8/12.1 的所有核心 DLL 都打包在里面了**。

  * **PyTorch**: 它的 `.whl` 包里自带了 `lib/` 文件夹，里面全是 `cublas`, `cudart`, `cudnn` 的 DLL。
  * **ONNX Runtime**: 稍微麻烦一点，有时它依赖系统环境，但在 Windows 上，只要特定的 DLL 在目录下，它就能读取。
  * **Faster-Whisper**: 它依赖 `ctranslate2`，这个库通常也会自带或静态链接关键的 CUDA 库。

### 3\. 如何实现“零依赖”整合包？

为了确保用户哪怕没装 CUDA Toolkit（只装了显卡驱动）也能运行，你需要利用 **“DLL 搜索路径劫持”** 的技巧。

#### 步骤 A：利用 `uv` 下载所有库

当你完成 `uv sync` 后，查看 `.venv/Lib/site-packages/`，你会发现：

  * `torch/lib/` 下面有一堆 `*.dll`。
  * `nvidia/` (如果安装了 nvidia-\* 包) 下面也有一堆 `*.dll`。

这些就是“嵌入式 CUDA/cuDNN”。

#### 步骤 B：修改启动脚本 (run\_app.bat) - **关键一步**

Windows 默认可能不会去 Python 的 `site-packages` 里找 DLL，导致 ONNX Runtime 报错“找不到 DLL”。你需要把这些路径强行加到 `PATH` 里。

更新你的 `run_app.bat`：

```batch
@echo off
setlocal

set "PROJECT_ROOT=%~dp0"
set "PROJECT_ROOT=%PROJECT_ROOT:~0,-1%"
set "VENV_ROOT=%PROJECT_ROOT%\.venv"

:: 1. 定位关键 DLL 的路径 (这是 PyTorch 携带 CUDA 库的地方)
set "TORCH_LIB=%VENV_ROOT%\Lib\site-packages\torch\lib"

:: 2. 有些库可能在 nvidia 目录下 (如果你单独安装了 nvidia 包)
set "NVIDIA_LIB=%VENV_ROOT%\Lib\site-packages\nvidia\cudnn\bin"

:: 3. 暴力修改 PATH，把这些 DLL 目录全加进去
:: 这样 ONNX Runtime 就算本身没带 CUDA，也会去 PyTorch 的目录里“借” CUDA DLL 用
set "PATH=%TORCH_LIB%;%NVIDIA_LIB%;%PROJECT_ROOT%\tools;%PATH%"

set "PYTHON_EXEC=%VENV_ROOT%\Scripts\python.exe"

echo [Info] Environment configured.
echo [Info] Using bundled CUDA libraries from: %TORCH_LIB%

"%PYTHON_EXEC%" -m uvicorn backend.app.main:app --reload

endlocal
```

### 4\. 关于 ONNX Runtime 的特别注意

`onnxruntime-gpu` 是出了名的“挑食”。它不带 CUDA 库，它假设系统里有。

**解决方案（二选一）：**

1.  **“借用”法（推荐）**：
    上面的 `run_app.bat` 已经做了。因为你安装了 PyTorch，PyTorch 带了 CUDA 11.8 的 DLL。通过把 PyTorch 的 `lib` 目录加到 `PATH`，ONNX Runtime 启动时会在 PATH 里搜索，正好搜到 PyTorch 带的那个 DLL，于是就能跑了。这就是为什么我建议 PyTorch 和 ONNX Runtime 都统一用 CUDA 11.8 的原因——**共用 DLL**。

2.  **“手动塞入”法（保险）**：
    如果“借用”失败，你可以去 NVIDIA 官网下载 CUDA 11.8 和 cuDNN 8.x 的 DLL 文件（不用安装包，只要 zip 包里的 bin 文件夹内容），直接扔到你的项目 `tools/` 文件夹，或者直接扔到 Python 的根目录。这样程序必能读到。

### 总结

你的理解是对的，现在的 AI 整合包不需要用户安装 CUDA Toolkit。

**你的开发路线图：**

1.  **开发机**：只装显卡驱动。
2.  **环境**：用 `uv` 安装 `torch` (带 CUDA) 和 `onnxruntime-gpu`。
3.  **打包**：把整个 `.venv` 文件夹打包。
4.  **运行**：通过 `.bat` 脚本把 `.venv/.../torch/lib` 加入临时环境变量 `PATH`。

这样，用户只要有 N 卡和驱动，解压就能跑。

