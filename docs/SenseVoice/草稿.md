### 🏛️ 宏观架构：三层漏斗模型

1.  **物理层 (Physical Layer)**：音频清洗与切分（FFmpeg / Demucs / VAD）。
2.  **声学层 (Acoustic Layer)**：极速转录与置信度筛查（SenseVoice + Whisper 补刀）。
3.  **语义层 (Semantic Layer)**：上下文理解、纠错与翻译（LLM）。

-----

### 🛠️ 详细工作流步骤 (Step-by-Step)

#### 第一阶段：物理层 - 预处理与分段

*目标：把视频变成干净的、机器爱听的短音频片段。*

**1. 音频提取与格式化 (FFmpeg)**

  * **输入**：用户上传的视频文件 (mp4, mkv, mov 等)。
  * **操作**：调用 FFmpeg 提取音频，统一转换为 **16kHz 单声道 WAV**（这是所有 ASR 模型的最佳输入标准）。
  * **命令**：
    ```bash
    ffmpeg -i input_video.mp4 -vn -acodec pcm_s16le -ac 1 -ar 16000 -map 0:a:0 temp_audio.wav
    ```

**2. 人声分离 (Vocal Separation)**

  * **引擎**：**Demucs (htdemucs)**
  * **策略**：
      * 如果用户选“快速模式”：跳过此步。
      * **默认**：使用 `htdemucs`。相比 `mdx_extra`，它在 GPU 上快 3-4 倍，且对语音识别精度无损。
  * **操作**：
      * 输入 `temp_audio.wav`。
      * 保留 `vocals.wav`，丢弃 `drums`, `bass`, `other`。
  * **输出**：纯净人声文件 `vocals.wav`。

**3. 语音活动检测 (VAD)**

  * **引擎**：**Silero VAD v4/v5**
  * **输入**：`vocals.wav`
  * **关键参数**：
      * `threshold`: 0.5 (标准)
      * `min_speech_duration_ms`: 250ms (避免极短噪音)
      * `min_silence_duration_ms`: 500ms (保证句子间有足够停顿)
      * `speech_pad_ms`: **200ms** (前后各加 200ms 缓冲，防止切掉首尾音节)。
  * **输出**：一个列表 `VAD_Segments = [{start: 0.5, end: 4.2}, {start: 5.1, end: 8.8}, ...]`。

-----

#### 第二阶段：声学层 - 转录与过滤

*目标：以最快速度生成初稿，并找出“听不清”的地方。*

**4. 极速转录 (Primary Transcription)**

  * **引擎**：**SenseVoice-Small** (INT8 量化版)
  * **操作**：
      * 遍历 `VAD_Segments`。
      * **批量推理 (Batch Inference)**：为了速度，不要一段一段送，建议把多个短段拼成一个 Batch 送入 GPU。
  * **获取数据**：
      * `text`: 文本内容。
      * `timestamp`: 字级/词级时间戳（SenseVoice 原生支持）。
      * **`confidence_score`**: 平均置信度分数 (0.0 - 1.0)。
  * **输出**：`Draft_Subtitles_List`。

**5. 质量门控 (Quality Gate) —— *逻辑分支点***

  * **逻辑**：设定阈值 `CONFIDENCE_THRESHOLD = 0.6`。
  * **分类**：
      * **A 类 (High Confidence)**：分数 \>= 0.6。直接进入下一阶段。
      * **B 类 (Low Confidence)**：分数 \< 0.6。标记为 `needs_review`，并将该段的时间轴加入 `Retry_Queue`。

**6. 声学回捞 (Acoustic Refinement) —— *串行化处理***

  * **检查**：如果 `Retry_Queue` 为空，跳过此步。
  * **操作**：
    1.  **卸载 SenseVoice** (释放显存)。
    2.  **加载 Whisper Large-v3** (或 Medium)。
    3.  **针对性重录**：只处理 `Retry_Queue` 中的时间段。
    4.  **数据更新**：将 Whisper 识别出的文本作为 `candidate_text_2` 存入字幕对象中。
    5.  **卸载 Whisper**。

-----

#### 第三阶段：语义层 - 智能校对与翻译 (可选)

*目标：利用 LLM 的“大脑”修复“耳朵”听错的内容，并进行翻译。*

**7. LLM 批处理 (The Judge & Translator)**

  * **触发条件**：用户开启“AI 校对”或“翻译”功能。
  * **策略**：**滑动窗口 (Sliding Window)**。
      * 不要一句一句发，太慢且没上下文。
      * 建议 **5 句为一个 Batch** 发送给 LLM。
  * **Prompt 构造 (核心)**：
      * **输入**：
        ```json
        [
          {"id": 1, "text": "上一句的文本..."},
          {"id": 2, "text": "SenseVoice结果", "candidate": "Whisper结果(如果有)", "flag": "low_conf"},
          {"id": 3, "text": "下一句的文本..."}
        ]
        ```
      * **指令**：
        > "你是一个字幕专家。请根据上下文修正第 2 句的 OCR 错误或同音字错误（参考 candidate）。如果 flag 存在，请重点检查。然后将第 2 句翻译成中文。返回 JSON。"
  * **执行**：LLM 返回修正后的原文和译文。

-----

#### 第四阶段：封装与导出

**8. 时间轴对齐与封装**

  * **时间轴来源**：
      * 始终优先使用 **SenseVoice** 的时间戳（因为它是原生对齐的，且不受 Whisper 幻觉影响）。
      * 除非 LLM 判定 SenseVoice 整个句子都丢了，才考虑使用 Whisper 的时间戳。
  * **格式化**：生成 SRT / ASS 文件。
  * **UI 渲染**：
      * 正常行：显示白色。
      * LLM 修正过的行：显示绿色高亮。
      * 依然存疑的行：显示黄色警告。

-----

### 📊 数据流图 (Data Flow)

```mermaid
graph TD
    Input(视频文件) --> FFmpeg[FFmpeg: 转16k WAV]
    FFmpeg --> Separation{是否分离人声?}
    
    Separation -- 是 (默认) --> Demucs[Htdemucs: 提取 Vocals]
    Separation -- 否 (极速) --> Vocals[WAV音频]
    Demucs --> Vocals
    
    Vocals --> VAD[Silero VAD: 切分片段]
    VAD --> SenseVoice[SenseVoice: 转录 + 时间戳 + 置信度]
    
    SenseVoice --> Gate{置信度检测}
    Gate -- High (>0.6) --> Draft[初稿池]
    Gate -- Low (<0.6) --> Queue[重审队列]
    
    Queue -- 队列非空 --> LoadWhisper[加载 Whisper (串行)]
    LoadWhisper --> WhisperRec[Whisper: 二次识别]
    WhisperRec --> Draft
    
    Draft --> Feature{用户需求?}
    
    Feature -- 仅转录 --> Export[生成 SRT]
    Feature -- 校对/翻译 --> BatchLLM[构造 Context Batch]
    
    BatchLLM --> LLM[LLM: 语义修正 + 翻译]
    LLM --> Merge[合并结果]
    Merge --> Export
```

### ⏱️ 性能预估 (以 10分钟视频为例)

| 步骤 | 引擎 | 耗时 (RTX 3060) | 显存占用 |
| :--- | :--- | :--- | :--- |
| **1. 预处理** | FFmpeg | \< 5秒 | CPU |
| **2. 分离** | Htdemucs | \~30秒 | \~1.5 GB |
| **3. VAD** | Silero | \< 2秒 | \< 0.5 GB |
| **4. 转录** | SenseVoice | **\< 10秒** | \< 1.0 GB |
| **5. 重审** | Whisper | \~20秒 (仅针对 5% 片段) | \~4.0 GB (串行) |
| **6. LLM** | Qwen-7B (Int4) | \~60秒 (取决于语速) | \~5.5 GB (串行) |
| **总计** | | **约 2 分钟** | **峰值 6GB (安全)** |

### 🚀 总结

这个工作流的精髓在于：

1.  **Htdemucs 替换 mdx\_extra**：解决了前处理的瓶颈。
2.  **SenseVoice 替换 WhisperX**：解决了对齐不准和转录慢的瓶颈。
3.  **串行化 Whisper**：解决了显存爆炸的问题，只在必要时“召回”重武器。
4.  **LLM 融合**：一次性解决纠错和翻译，避免重复调用。

这就是目前 2025 年能做到的**最强本地字幕架构**。