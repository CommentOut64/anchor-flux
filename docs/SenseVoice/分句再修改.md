#### 2\. 硬编码的语言规则

  * **风险**：在 `SemanticGrouper` 中使用了硬编码的中文规则（如 `continuation_words = ['但', '可'...]`）。
  * **问题**：这会导致系统难以扩展到英语、日语或其他语言。
  * **建议**：将语言相关的规则抽象为 `LanguageStrategy` 接口，通过配置文件或策略模式注入。

#### 3\. 动态阈值的“振荡”

  * **风险**：如果一段音频中语速忽快忽慢（例如两人对话，一人急一人缓），滑动窗口可能会导致阈值计算滞后或剧烈波动。
  * **建议**：使用**中位数**代替**平均值**来计算 `avg_interval`，以排除异常长停顿的干扰。

-----

### 三、 深度优化建议 (Expert Optimization)

#### 1\. 算法层面的优化

**A. 优化动态阈值计算 (Robust Dynamic Threshold)**
建议在 `sentence_splitter.py` 中引入统计学方法，防止异常值干扰。

```python
import statistics

def _calculate_dynamic_pause_threshold(self, words, current_index):
    # ... 获取 intervals ...
    if not intervals:
        return self.config.pause_threshold
    
    # 优化建议：使用中位数而非平均值，抗噪性更强
    # 避免因为一个极长的停顿拉高了整体阈值
    median_interval = statistics.median(intervals)
    
    # 引入基准值混合，避免极端情况（例如一直很慢或一直很快）
    # 动态权重 0.7 + 静态基准 0.3
    weighted_threshold = (median_interval * self.config.pause_multiplier * 0.7) + \
                         (self.config.pause_threshold * 0.3)
                         
    return max(min(weighted_threshold, 2.0), 0.2) # 设置上下限
```

**B. 语义分组引入“重叠检测”**
在 `SemanticGrouper` 中，除了看时间间隔和连接词，还可以检查**时间重叠**。

  * *场景*：有时候 VAD 切分不准，导致两个 segment 时间上有微小重叠（或者完全接壤）。这种情况 99% 属于同一句话。

#### 2\. LLM 交互层面的优化

**A. 结构化输出 (Structured Output)**
不要让 LLM 只返回文本，建议返回 JSON，包含修改原因，以便前端高亮显示。

```json
// LLM Response Format
{
  "original": "我觉得...这个方案",
  "corrected": "我觉得这个方案",
  "type": "merge", // merge, correction, translation
  "confidence": 0.9
}
```

**B. 长度熔断保护**
在 `LLMProofreader` 中增加安全检查。如果 LLM 修改后的文本长度与原文本长度差异超过 30%（且不是翻译模式），应触发警告或自动丢弃修改，防止 LLM “写作文”。

#### 3\. 前端交互体验优化


**B. 语义组的合并/拆分操作**
在 UI 上允许用户通过快捷键（如 `Ctrl + Click`）手动合并两个组，或者在一个组中间打断。这需要 `SemanticGrouper` 提供 API 来重新计算组内的 `position`。

### 四、 代码审查指引 (Specific Code Review)

针对你提供的 `sentence_splitter.py` 代码，有几处小细节可以改进：

1.  **SplitConfig dataclass**:

    ```python
    # 建议将 defaults 抽离，避免可变参数陷阱（虽然 dataclass 处理了，但为了序列化方便）
    @dataclass
    class SplitConfig:
        # ...
        # 建议增加 language 字段，便于后续策略选择
        language: str = "auto" 
    ```

2.  **Performance**:

      * `split` 方法中的 `current_text = "".join(w.word for w in current_words)` 是在循环内部执行的。对于超长句子，这会产生 $O(N^2)$ 的字符串拼接开销。
      * **优化**：只在通过了其他轻量级检查（如时间间隔）需要判断长度时，再计算 `current_text`，或者维护一个 `running_length` 计数器。
