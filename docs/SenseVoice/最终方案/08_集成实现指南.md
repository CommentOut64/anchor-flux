# 集成实现指南

> 本文档说明如何将 Phase2 的各个组件集成到现有的 ---

## 三、新流程设计

```
音频提取 → VAD切分 → [可选:快速预判] → 频谱分诊(Chunk级) → 按需分离 → 逐Chunk转录(含熔断) → 后处理
```

### 3.1 可选：快速全局预判

对于长视频（>30分钟），可以先做一次快速预判，决定是否完全跳过分诊：

```python
from services.audio_spectrum_classifier import get_spectrum_classifier

classifier = get_spectrum_classifier()
level, avg_score = classifier.quick_global_diagnosis(audio_array, sr)

if level == "none":
    # 纯净音频，跳过所有 Chunk 的频谱分诊和分离
    skip_all_separation = True
elif level == "heavy":
    # 重度 BGM，可以考虑直接对所有 Chunk 进行分离（跳过逐个分诊）
    separate_all_chunks = True
else:
    # light 或 unknown，正常走 Chunk 级分诊
    pass
```

### 3.2 Chunk 数据结构ption_service.py` 中

---

## 一、架构变更说明

### 1.1 废弃全局 BGM 检测

旧的 `detect_background_music_level()` 方法已废弃，原因：
- 需要运行 3 次 Demucs 分离（慢、耗显存）
- 只能得到全局判断，无法精细控制每个 Chunk

新方案使用 **频谱分诊**（`AudioSpectrumClassifier`），优势：
- 纯 CPU 频谱计算，不需要 GPU
- Chunk 级别精细控制
- 速度快 10 倍以上

### 1.2 新旧流程对比

```
旧流程: 音频提取 → 全局BGM检测(Demucs) → 全局分离 → VAD → 转录
新流程: 音频提取 → VAD → 频谱分诊(Chunk级) → 按需分离 → 转录
```

---

## 二、组件状态汇总

| 组件 | 文件路径 | 状态 | 说明 |
|------|----------|------|------|
| VAD 服务 | `services/transcription_service.py` | ✅ 已实现 | `_vad_silero()`, `_memory_vad_split()` |
| 频谱分诊器 | `services/audio_spectrum_classifier.py` | ✅ 已实现 | `AudioSpectrumClassifier` |
| 快速全局预判 | `services/audio_spectrum_classifier.py` | ✅ 已实现 | `quick_global_diagnosis()` |
| 熔断决策器 | `services/fuse_breaker.py` | ✅ 已实现 | `FuseBreaker`, `execute_fuse_upgrade()` |
| 熔断数据模型 | `models/circuit_breaker_models.py` | ✅ 已实现 | `ChunkProcessState`, `FuseDecision` |
| 分诊阈值 | `core/spectrum_thresholds.py` | ✅ 已实现 | `SpectrumThresholds` |
| Demucs 服务 | `services/demucs_service.py` | ✅ 已修复 | 新增 `separate_chunk()` 方法 |
| ~~全局BGM检测~~ | ~~`services/demucs_service.py`~~ | ❌ 已废弃 | 使用频谱分诊替代 |
| **集成逻辑** | `services/transcription_service.py` | ❌ 待实现 | 需要重构转录主流程 |

---

## 二、新流程设计

```
音频提取 → VAD切分 → 频谱分诊(Chunk级) → 按需分离 → 逐Chunk转录(含熔断) → 后处理
```

### 2.1 Chunk 数据结构

VAD 输出的 Chunk 元数据：
```python
# 从 _memory_vad_split() 返回
vad_segments = [
    {"index": 0, "start": 0.0, "end": 15.5, "mode": "memory"},
    {"index": 1, "start": 15.5, "end": 32.1, "mode": "memory"},
    ...
]
```

增强后的 ChunkProcessState（用于转录流程）：
```python
from models.circuit_breaker_models import ChunkProcessState, SeparationLevel

chunk_state = ChunkProcessState(
    chunk_index=0,
    start_time=0.0,
    end_time=15.5,
    original_audio=audio_array[0:15.5*16000],  # 保留原始音频
    current_audio=audio_array[0:15.5*16000],   # 当前使用（可能被分离）
    sample_rate=16000,
    separation_level=SeparationLevel.NONE
)
```

---

## 三、集成代码示例

### 3.1 主流程集成点（在 transcription_service.py 中）

```python
from services.audio_spectrum_classifier import get_spectrum_classifier
from services.fuse_breaker import get_fuse_breaker, execute_fuse_upgrade
from services.demucs_service import get_demucs_service
from models.circuit_breaker_models import ChunkProcessState, SeparationLevel

class TranscriptionService:
    
    async def _process_with_new_architecture(
        self,
        audio_array: np.ndarray,
        sr: int = 16000,
        job_id: str = None,
        settings = None
    ):
        """
        新架构转录流程（VAD优先 + 频谱分诊 + 熔断）
        """
        # 1. VAD 物理切分
        vad_segments = self._memory_vad_split(audio_array, sr)
        
        # 2. 频谱分诊（Chunk 级别）
        spectrum_classifier = get_spectrum_classifier()
        chunk_states = []
        
        for seg in vad_segments:
            # 截取 Chunk 音频
            start_sample = int(seg['start'] * sr)
            end_sample = int(seg['end'] * sr)
            chunk_audio = audio_array[start_sample:end_sample]
            
            # 频谱分诊
            diagnosis = spectrum_classifier.diagnose_chunk(
                audio=chunk_audio,
                chunk_index=seg['index'],
                sr=sr
            )
            
            # 创建 ChunkProcessState
            state = ChunkProcessState(
                chunk_index=seg['index'],
                start_time=seg['start'],
                end_time=seg['end'],
                original_audio=chunk_audio.copy(),  # 保留原始
                current_audio=chunk_audio,
                sample_rate=sr,
                separation_level=SeparationLevel.NONE
            )
            
            # 3. 按需分离
            if diagnosis.need_separation:
                demucs = get_demucs_service()
                separated = demucs.separate_chunk(
                    audio=chunk_audio,
                    model=diagnosis.recommended_model,
                    sr=sr
                )
                state.current_audio = separated
                state.separation_level = SeparationLevel[diagnosis.recommended_model.upper()]
                state.separation_model_used = diagnosis.recommended_model
            
            chunk_states.append(state)
        
        # 4. 逐 Chunk 转录（含熔断）
        fuse_breaker = get_fuse_breaker()
        results = []
        
        for state in chunk_states:
            result = await self._transcribe_chunk_with_fuse(
                state, fuse_breaker, settings
            )
            results.append(result)
        
        return results
    
    async def _transcribe_chunk_with_fuse(
        self,
        chunk_state: ChunkProcessState,
        fuse_breaker,
        settings
    ):
        """
        单个 Chunk 转录（含熔断回溯）
        """
        max_retry = chunk_state.max_fuse_retry
        
        while True:
            # 使用 SenseVoice 转录
            sv_result = await self._sensevoice_transcribe(
                chunk_state.current_audio,
                chunk_state.sample_rate
            )
            
            # 获取置信度和事件标签
            confidence = sv_result.confidence
            event_tag = sv_result.event  # BGM/Noise/Speech 等
            
            # 熔断决策
            decision = fuse_breaker.should_fuse(
                chunk_state=chunk_state,
                confidence=confidence,
                event_tag=event_tag
            )
            
            if decision.action == FuseAction.ACCEPT:
                # 接受结果
                chunk_state.transcription_confidence = confidence
                chunk_state.event_tag = event_tag
                return sv_result
            
            elif decision.action == FuseAction.UPGRADE_SEPARATION:
                # 熔断升级
                if not chunk_state.can_upgrade_separation():
                    # 已达止损点，接受低质量结果
                    return sv_result
                
                # 执行升级
                next_level = chunk_state.get_next_separation_level()
                demucs = get_demucs_service()
                chunk_state = execute_fuse_upgrade(
                    chunk_state, next_level, demucs
                )
                # 循环回到转录
```

---

## 四、全局 BGM 检测 vs Chunk 级频谱分诊

### 4.1 关系说明

两者是**补充关系**，建议保留全局检测作为"快速通道"：

```python
# 可选：全局快速预判
if settings.enable_global_bgm_check:
    demucs = get_demucs_service()
    bgm_level, _ = demucs.detect_background_music_level(audio_path)
    
    if bgm_level == BGMLevel.NONE:
        # 纯净音频，跳过所有 Chunk 的频谱分诊
        skip_spectrum_diagnosis = True
    elif bgm_level == BGMLevel.HEAVY:
        # 重度 BGM，直接全局分离
        perform_global_separation = True
```

### 4.2 推荐策略

| 场景 | 策略 |
|------|------|
| 短视频 (<5分钟) | 跳过全局检测，直接 Chunk 级分诊 |
| 长视频 (>30分钟) | 先全局检测，再按需 Chunk 级分诊 |
| 用户选择"强制分离" | 跳过检测，直接全局分离 |

---

## 五、接口变更清单

### 5.1 demucs_service.py 新增

```python
def separate_chunk(
    self,
    audio: np.ndarray,
    model: str = None,
    sr: int = 16000
) -> np.ndarray:
    """熔断决策器专用接口"""
```

### 5.2 circuit_breaker_models.py 修改

```python
@dataclass
class ChunkProcessState:
    # 新增字段
    sample_rate: int = 16000
```

---

## 六、测试清单

- [ ] VAD 切分返回正确的 segment 元数据
- [ ] `AudioSpectrumClassifier` 可正确分诊 Chunk
- [ ] `separate_chunk()` 可正确分离音频
- [ ] 熔断决策器在低置信度+BGM标签时触发升级
- [ ] 熔断升级后使用 `original_audio` 重新分离
- [ ] 止损点（max_retry=1）生效

