基于你的分析和“分段对齐”的需求，**“一次加载 + 内存切片 + 批次对齐”** 是目前性价比最高、改造风险最小的方案。

以下是针对你问题的具体解答和实施方案。

### 一、 关键决策解答

#### 1\. 是否需要整合内存映射 (mmap)？

**结论：对于桌面端应用（处理一般时长视频），暂时不需要，直接加载进内存即可。**

  * **算笔账**：Whisper 需要 16kHz 单声道 Float32 音频。
      * **1 小时音频** = $16000 \times 3600 \times 4 \text{ bytes} \approx 230 \text{ MB}$。
      * 即便是一部 **3 小时的电影**，占用内存也就 **\~700 MB**。
  * **现状**：现代电脑（8GB+ 内存）完全吃得消。`whisperx.load_audio` 底层调用 ffmpeg 管道读取，不支持直接 mmap。如果要用 mmap，你得先由 ffmpeg 转码成巨大的 `.raw` 或 `.wav` 文件到硬盘，这又增加了磁盘 IO。
  * **策略**：除非你的用户经常处理 **5 小时以上** 的录音，否则直接读取到 Numpy Array 是最快且最简单的。

#### 2\. 是否使用 `pydub`/`ffmpeg` 流式处理？

**结论：不要流式处理，太慢。**

  * 流式处理意味着每次模型需要音频时，都要 seek 文件指针并解码。深度学习模型（Whisper）喜欢一口气吞掉数据。频繁的 IO Seek 会让 GPU 等待 CPU，导致显卡利用率低下。

#### 3\. 是否“用完即释放”？

**结论：部分释放。**

  * **保留**：整轨的 `audio_array`（Numpy 数组）一直保留到任务结束。因为它在“转录”和“对齐”阶段都要用，反复加载是大忌。
  * **释放**：转录过程中产生的临时变量、模型推理产生的中间 Tensor，用完立即释放。

-----

### 二、 修改后的架构设计 (V2.1)

我们将彻底移除 `split` 阶段的**文件写入**操作，将其变为纯粹的**元数据计算**。

#### 1\. 新流程图解

```
[Start]
   ↓
[Phase 1: Load]
   → ffmpeg 解码整轨 -> 内存 (audio_array: np.array, ~230MB/hr)
   ↓
[Phase 2: VAD/Split] (纯内存操作)
   → 在 audio_array 上跑 VAD
   → 产出 segments_metadata: List[{start, end}] (不写文件!)
   ↓
[Phase 3: Transcribe] (循环)
   → 从 audio_array 切片 (audio_array[s:e])
   → 送入 Whisper 推理
   → 得到 unaligned_text
   → 存入内存 results 列表
   ↓
[Phase 4: Align] (分批)
   → 传入整轨 audio_array (复用，不重读)
   → 传入 batches of results
   → 循环对齐 -> 更新进度
   ↓
[End]
```

-----

### 三、 代码实施方案

你需要重点修改 `backend/app/services/transcription_service.py`。

#### 1\. 移除文件分段，改为 VAD 内存分段

**修改 `_split_audio`**：不再导出 wav 文件，只返回时间戳。

```python
def _split_audio_in_memory(self, audio_array: np.ndarray, sr=16000) -> List[Dict]:
    """
    在内存中进行 VAD 分段，不产生磁盘 IO
    """
    # 也可以复用 whisperx 自带的 VAD pipeline，或者简单的基于能量的切分
    # 这里假设你使用 whisperx 的 VAD
    from whisperx.vad import load_vad_model, merge_chunks
    
    vad_model = load_vad_model(torch.device("cpu"), use_auth_token=None)
    vad_segments = vad_model({"waveform": torch.from_numpy(audio_array).unsqueeze(0), "sample_rate": sr})
    
    # 整理格式
    segments = merge_chunks(vad_segments, chunk_size=30, onset=0.5) # 示例参数
    
    # 转换为我们的通用格式
    # 注意：不再需要 'file' 路径字段，只需要 start_sample, end_sample
    processed_segments = []
    for idx, seg in enumerate(segments):
        processed_segments.append({
            "index": idx,
            "start": seg["start"], # 秒
            "end": seg["end"],     # 秒
            # "file": ... ❌ 删掉这个字段，不需要了
        })
        
    return processed_segments
```

#### 2\. 改造转录循环：直接切片

**修改 `_transcribe_segment_unaligned`**：

```python
def _transcribe_segment_memory(
    self, 
    audio_slice: np.ndarray, # 接收 numpy 数组切片
    model, 
    job: JobState
) -> Dict:
    """
    直接对内存中的音频切片进行转录
    """
    # WhisperX/Whisper 的 transcribe 接口通常支持直接传入 numpy array
    # 从而省去了 load_audio 的开销
    
    result = model.transcribe(
        audio_slice, 
        batch_size=job.settings.batch_size,
        language=job.language
    )
    
    # ... 处理结果逻辑不变 ...
    return result
```

#### 3\. 整合后的 Pipeline (核心逻辑)

```python
def _run_pipeline(self, job: JobState):
    # 1. 一次性加载音频 (解决重复加载瓶颈)
    self._update_progress(job, 'extract', 0, '加载音频...')
    audio_path = job.dir / job.filename
    # audio_array 是 float32, 16kHz
    audio_array = whisperx.load_audio(str(audio_path)) 
    
    # 2. 内存分段 (解决 IO 瓶颈)
    self._update_progress(job, 'split', 0, '分析静音片段...')
    segments_meta = self._split_audio_in_memory(audio_array)
    job.total_segments = len(segments_meta)
    
    # 3. 转录循环
    unaligned_results = []
    model = self._get_model(...)
    
    for idx, seg_meta in enumerate(segments_meta):
        # 计算切片索引 (秒 -> 采样点)
        start_sample = int(seg_meta['start'] * 16000)
        end_sample = int(seg_meta['end'] * 16000)
        
        # 【核心优化】内存切片 (Zero-copy view if consistent, or fast copy)
        audio_slice = audio_array[start_sample:end_sample]
        
        # 转录
        res = self._transcribe_segment_memory(audio_slice, model, job)
        
        # ... 保存结果、SSE 推送、Checkpoint 逻辑 ...
        # 注意 Checkpoint 现在不需要保存 segment 文件路径了，只存 time range
    
    # 4. 分批对齐 (Segmented Alignment - 解决进度条卡顿)
    self._run_segmented_alignment(unaligned_results, audio_array, job)

```

#### 4\. 分批对齐的具体实现

不需要重新加载音频，直接传入第 1 步加载的 `audio_array`。

```python
def _run_segmented_alignment(self, unaligned_results, audio_array, job):
    # 加载对齐模型
    align_model, metadata = self._get_align_model(...)
    
    all_segments = [s for res in unaligned_results for s in res['segments']]
    total = len(all_segments)
    BATCH_SIZE = 50 
    
    final_results = []
    
    for i in range(0, total, BATCH_SIZE):
        batch = all_segments[i : i + BATCH_SIZE]
        
        # WhisperX align 支持传入预加载的 audio_array
        # 它内部会根据 batch 里 segments 的时间戳去 audio_array 里找对应的声音
        # 即使我们传入整轨音频，计算量也只与 batch 的长度相关
        aligned_batch = whisperx.align(
            batch, 
            align_model, 
            metadata, 
            audio=audio_array, # 【关键】复用内存对象
            device=job.settings.device
        )
        
        final_results.extend(aligned_batch['segments'])
        
        # 更新进度
        progress = (i + BATCH_SIZE) / total
        self._update_progress(job, 'align', progress, ...)
        
    return final_results
```

-----

### 四、 Checkpoint 策略的调整

由于不再生成 `segment_0.wav` 这种文件，`checkpoint.json` 变得更加重要。

1.  **Segments 字段**：
    以前存：`{ "index": 0, "file": "/tmp/seg_0.wav" }`
    现在存：`{ "index": 0, "start": 0.0, "end": 30.5 }`

2.  **恢复逻辑**：
    当任务恢复时，程序必须**重新执行步骤 1 (加载整轨音频)**。
    因为内存里的 `audio_array` 丢了，必须重读。但因为不需要重新做 VAD 分段（直接读 checkpoint 里的 start/end），也不需要重新写小文件，所以恢复速度依然很快。

### 五、 总结

按照这个方案修改，你的应用性能将获得质的飞跃：

1.  **磁盘 IO 骤降**：消除了成百上千次的小文件写入和读取。
2.  **内存管理**：对于 \< 4小时的视频，内存完全可控（\< 1GB）。
3.  **对齐速度**：消除了重复加载音频的开销，且支持了进度条。

**下一步行动**：先实现 `_split_audio_in_memory` 和 `_transcribe_segment_memory`，替换掉原有的基于文件的函数。