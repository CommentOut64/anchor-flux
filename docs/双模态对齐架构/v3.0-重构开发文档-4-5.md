## 4. 核心模块详细设计

### 4.1 JobManager (任务管理器)

**职责**: 任务 CRUD 操作和生命周期管理

**文件**: `backend/app/services/job/job_manager.py`

**核心方法**:

```python
from typing import Optional, List
from app.models.job_models import JobState, JobConfig
from app.services.job.checkpoint_manager import CheckpointManager

class JobManager:
    """任务管理器，负责任务的创建、查询、更新和删除"""

    def __init__(self, checkpoint_manager: CheckpointManager):
        self.jobs: Dict[str, JobState] = {}
        self.checkpoint_manager = checkpoint_manager
        self.logger = logging.getLogger(__name__)

    async def create_job(
        self,
        job_id: str,
        file_path: str,
        config: JobConfig
    ) -> JobState:
        """创建新任务"""
        job = JobState(
            job_id=job_id,
            file_path=file_path,
            config=config,
            status="pending",
            created_at=datetime.now()
        )
        self.jobs[job_id] = job
        self.logger.info(f"任务创建成功: {job_id}")
        return job

    async def get_job(self, job_id: str) -> Optional[JobState]:
        """获取任务状态"""
        return self.jobs.get(job_id)

    async def update_job_status(
        self,
        job_id: str,
        status: str,
        progress: float = None
    ):
        """更新任务状态"""
        if job_id in self.jobs:
            self.jobs[job_id].status = status
            if progress is not None:
                self.jobs[job_id].progress = progress
            self.logger.debug(f"任务状态更新: {job_id} -> {status}")

    async def pause_job(self, job_id: str):
        """暂停任务"""
        if job_id in self.jobs:
            self.jobs[job_id].status = "paused"
            await self.checkpoint_manager.save_checkpoint(self.jobs[job_id])
            self.logger.info(f"任务已暂停: {job_id}")

    async def cancel_job(self, job_id: str):
        """取消任务"""
        if job_id in self.jobs:
            self.jobs[job_id].status = "cancelled"
            self.logger.info(f"任务已取消: {job_id}")

    async def scan_incomplete_jobs(self) -> List[JobState]:
        """扫描未完成的任务（用于断点续传）"""
        incomplete = []
        for job in self.jobs.values():
            if job.status in ["paused", "processing"]:
                checkpoint = await self.checkpoint_manager.load_checkpoint(job.job_id)
                if checkpoint:
                    incomplete.append(job)
        return incomplete
```

**设计要点**:
- 纯粹的任务状态管理，不涉及业务逻辑
- 与 CheckpointManager 协作实现断点续传
- 使用字典存储任务状态，支持快速查询

---

### 4.2 CheckpointManager (断点续传管理)

**职责**: 检查点的保存、加载和验证

**文件**: `backend/app/services/job/checkpoint_manager.py`

**核心方法**:

```python
import json
from pathlib import Path
from typing import Optional, Dict, Any

class CheckpointManager:
    """断点续传管理器"""

    def __init__(self, checkpoint_dir: str = "checkpoints"):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        self.logger = logging.getLogger(__name__)

    async def save_checkpoint(self, job: JobState) -> bool:
        """保存检查点"""
        try:
            checkpoint_file = self.checkpoint_dir / f"{job.job_id}.json"
            checkpoint_data = {
                "job_id": job.job_id,
                "status": job.status,
                "progress": job.progress,
                "current_chunk": job.current_chunk,
                "processed_chunks": job.processed_chunks,
                "timestamp": datetime.now().isoformat()
            }

            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)

            self.logger.info(f"检查点已保存: {job.job_id}")
            return True
        except Exception as e:
            self.logger.error(f"保存检查点失败: {e}")
            return False

    async def load_checkpoint(self, job_id: str) -> Optional[Dict[str, Any]]:
        """加载检查点"""
        try:
            checkpoint_file = self.checkpoint_dir / f"{job_id}.json"
            if not checkpoint_file.exists():
                return None

            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)

            self.logger.info(f"检查点已加载: {job_id}")
            return checkpoint_data
        except Exception as e:
            self.logger.error(f"加载检查点失败: {e}")
            return None

    async def delete_checkpoint(self, job_id: str):
        """删除检查点"""
        checkpoint_file = self.checkpoint_dir / f"{job_id}.json"
        if checkpoint_file.exists():
            checkpoint_file.unlink()
            self.logger.info(f"检查点已删除: {job_id}")

    async def verify_checkpoint(self, job_id: str) -> bool:
        """验证检查点完整性"""
        checkpoint = await self.load_checkpoint(job_id)
        if not checkpoint:
            return False

        required_fields = ["job_id", "status", "progress", "timestamp"]
        return all(field in checkpoint for field in required_fields)
```

**设计要点**:
- 使用 JSON 格式存储检查点，便于调试
- 支持检查点验证，防止损坏数据
- 独立的文件管理，不依赖数据库

---

### 4.3 AudioProcessingPipeline (音频前处理流水线)

**职责**: 音频提取、人声分离、VAD 切分

**文件**: `backend/app/pipelines/audio_processing_pipeline.py`

**核心方法**:

```python
from app.services.audio.chunk_engine import ChunkEngine
from app.services.demucs_service import DemucsService
from app.core.resource_manager import ResourceManager

class AudioProcessingPipeline:
    """音频前处理流水线"""

    def __init__(
        self,
        chunk_engine: ChunkEngine,
        demucs_service: DemucsService,
        resource_manager: ResourceManager
    ):
        self.chunk_engine = chunk_engine
        self.demucs_service = demucs_service
        self.resource_manager = resource_manager
        self.logger = logging.getLogger(__name__)

    async def process(
        self,
        video_path: str,
        enable_demucs: bool = True
    ) -> List[AudioChunk]:
        """
        执行完整的音频前处理流程

        Returns:
            List[AudioChunk]: 切分后的音频块列表，每个块包含：
                - audio: numpy array
                - start: 起始时间（秒）
                - end: 结束时间（秒）
                - vad_confidence: VAD 置信度
        """
        # 1. 提取音频并降采样到 16kHz
        self.logger.info("开始提取音频...")
        audio_path = await self._extract_audio(video_path)

        # 2. 【v3.5 修正】Demucs 策略统一为 5 分钟分块
        # 原因：整轨分离在长视频（>1小时）会导致内存溢出，且 Python GC 可能内存泄漏
        # 解决方案：统一使用 5 分钟分块策略，既利用上下文又避免 OOM

        # 3. 人声分离（Pre-VAD Separation）
        clean_audio_path = audio_path
        if enable_demucs:
            self.logger.info("开始人声分离（5分钟分块策略）...")
            # 统一策略：按 5 分钟大块切分处理
            # 优势：
            # 1. 减少 Demucs 模型加载次数（从 120 次降至 12 次，1小时视频）
            # 2. 保留足够上下文，提升分离质量
            # 3. 避免长视频 OOM 风险
            clean_audio_path = await self.demucs_service.separate_large_chunks(
                audio_path,
                chunk_size=300  # 5分钟 = 300秒
            )

        # 4. VAD 切分（基于纯人声，精度提升 30%）
        self.logger.info("开始 VAD 切分...")
        chunks = await self.chunk_engine.split_by_vad(
            clean_audio_path,
            min_silence_duration=0.5,
            speech_pad_ms=300
        )

        self.logger.info(f"音频前处理完成，共 {len(chunks)} 个片段")
        return chunks

    async def _extract_audio(self, video_path: str) -> str:
        """提取音频并降采样"""
        # 使用 ffmpeg 提取音频
        output_path = video_path.replace('.mp4', '_audio.wav')
        cmd = [
            'ffmpeg', '-i', video_path,
            '-ar', '16000',  # 降采样到 16kHz
            '-ac', '1',      # 单声道
            '-y', output_path
        ]
        # 执行命令...
        return output_path
```

**设计要点**:
- **Pre-VAD Separation**: 先整轨分离人声，再进行 VAD 切分，避免上下文丢失
- **显存自适应**: 根据可用显存动态选择分离策略
- **返回标准化数据**: 统一的 AudioChunk 数据结构

---

### 4.4 DualAlignmentPipeline (双流对齐流水线)

**职责**: 双模态推理和锚点对齐

**文件**: `backend/app/pipelines/dual_alignment_pipeline.py`

**核心方法**:

```python
from app.services.inference.sensevoice_executor import SenseVoiceExecutor
from app.services.inference.whisper_executor import WhisperExecutor
from app.services.alignment.alignment_service import AlignmentService
from app.services.streaming.sse_publisher import SSEPublisher

class DualAlignmentPipeline:
    """双流对齐流水线 - v3.0 核心"""

    def __init__(
        self,
        sv_executor: SenseVoiceExecutor,
        whisper_executor: WhisperExecutor,
        aligner: AlignmentService,
        keyword_extractor: KeywordExtractor,  # 新增: 关键词提取器
        sentence_splitter: SentenceSplitter,  # 新增: 分句器
        sse_publisher: SSEPublisher
    ):
        self.sv_executor = sv_executor
        self.whisper_executor = whisper_executor
        self.aligner = aligner
        self.keyword_extractor = keyword_extractor
        self.sentence_splitter = sentence_splitter
        self.sse_publisher = sse_publisher
        self.logger = logging.getLogger(__name__)

        # Prompt 缓存: 存储上一句 Whisper 精修文本
        self.previous_text_buffer: str = ""

    async def run(
        self,
        job_id: str,
        chunks: List[AudioChunk],
        config: JobConfig
    ):
        """
        执行双流推理和对齐

        流程:
        1. SenseVoice 极速推理 (CPU) -> 关键词提取
        2. 【第一次分句】基于时间间隔分句 -> 推送草稿（锁定状态）
        3. 构建智能 Prompt (上一句Whisper + Glossary + SenseVoice关键词)
        4. Whisper 深度推理 (GPU) -> 获取文本
        5. Alignment Service 对齐 -> 生成带时间戳的词列表
        6. 【第二次分句】基于标点和语义分句 -> 推送定稿（Chunk级替换）

        【重要纠正】Prompt 构建策略:
        - 不要把 SenseVoice 整句草稿放进 Prompt（会导致 Whisper 产生"已完成"错觉）
        - 只利用 SenseVoice 做实体抽取（NER），提取人名、生僻词等关键词
        - Prompt = 上一句Whisper文本 + Glossary (用户词表 + SenseVoice提取的关键词)

        【前端优化】双阶段分句 + Chunk级替换:
        - 快流分句：依赖时间间隔，保证草稿可读性
        - 慢流分句：依赖标点和语义，生成最终字幕行
        - 使用 chunk_id 作为锚点，一次性替换该 chunk 的所有草稿为定稿
        - 草稿阶段锁定编辑（isFinal=false），定稿后解锁（isFinal=true）
        """
        total_chunks = len(chunks)

        # 重置 Prompt 缓存
        self.previous_text_buffer = ""

        for i, chunk in enumerate(chunks):
            chunk_id = f"{job_id}_chunk_{i}"
            self.logger.info(f"处理 Chunk {i+1}/{total_chunks}")

            # ========== 快流 (Fast Stream) ==========
            # 强制使用 CPU 运行 SenseVoice，避免抢占 GPU
            sv_result = await self.sv_executor.transcribe(
                audio=chunk.audio,
                device="cpu",
                language=config.language
            )

            # ========== 【第一次分句】基于时间间隔 ==========
            # 目的：让草稿看起来不像一坨长文本，提升阅读体验
            # 依据：主要靠 max_gap (停顿) 来切分，因为 SenseVoice 可能没标点
            draft_sentences = await self.sentence_splitter.split(
                words=sv_result.word_timestamps,
                mode="time_based",  # 基于时间间隔
                chunk_offset=chunk.start
            )

            # 推送草稿（灰色斜体，锁定状态）
            await self.sse_publisher.publish_draft_batch(
                job_id=job_id,
                chunk_id=chunk_id,
                sentences=draft_sentences,
                is_final=False  # 标记为草稿，前端锁定编辑
            )

            # ========== 关键词提取 (NER) ==========
            # 从 SenseVoice 草稿中提取人名、生僻词（不是整句！）
            sv_keywords = await self.keyword_extractor.extract(
                text=sv_result.text,
                language=config.language
            )
            self.logger.debug(f"SenseVoice 提取关键词: {sv_keywords}")

            # ========== 构建智能 Prompt ==========
            # 格式: "Previous text here. Glossary: term1, term2, term3."
            whisper_prompt = self._build_smart_prompt(
                previous_text=self.previous_text_buffer,
                user_glossary=config.glossary or [],
                sv_keywords=sv_keywords,
                language=config.language
            )

            # ========== 慢流 (Slow Stream) ==========
            # 使用 GPU 运行 Whisper
            whisper_result = await self.whisper_executor.transcribe(
                audio=chunk.audio,
                language=config.language,
                prompt=whisper_prompt
            )

            # 更新 Prompt 缓存（用 Whisper 精修文本，不是 SenseVoice 草稿）
            self.previous_text_buffer = whisper_result.text

            # ========== 对齐 (Alignment) ==========
            # 将 Whisper 的文本映射到 SenseVoice 的时间戳
            # 传入 VAD 静音区间用于硬约束
            aligned_subtitle = await self.aligner.align(
                whisper_text=whisper_result.text,
                sv_tokens=sv_result.word_timestamps,  # 字级时间戳
                vad_range=(chunk.start, chunk.end),   # VAD 边界
                silence_ranges=chunk.silence_ranges,  # 静音区间列表
                chunk_offset=chunk.start
            )

            # ========== 【第二次分句】基于标点和语义 ==========
            # 目的：生成最终的、符合语法规范的字幕行
            # 依据：主要靠 Whisper 生成的标点符号 (。？！)，辅以时间间隔
            final_sentences = await self.sentence_splitter.split(
                words=aligned_subtitle.words,
                mode="semantic_based",  # 基于标点和语义
                chunk_offset=chunk.start
            )

            # 推送定稿（Chunk级批量替换，黑色正体，解锁编辑）
            await self.sse_publisher.publish_replace_chunk(
                job_id=job_id,
                chunk_id=chunk_id,
                sentences=final_sentences,
                is_final=True  # 标记为定稿，前端解锁编辑
            )

            # 更新进度
            progress = (i + 1) / total_chunks * 100
            await self.sse_publisher.publish_progress(
                job_id=job_id,
                phase="dual_inference",
                percent=progress
            )

    def _build_smart_prompt(
        self,
        previous_text: str,
        user_glossary: List[str],
        sv_keywords: List[str],
        language: str
    ) -> str:
        """
        构建智能 Prompt

        格式遵循 OpenAI 官方建议:
        "Previous text here. Glossary: term1, term2, term3."

        Args:
            previous_text: 上一句 Whisper 精修文本（保持语境连贯）
            user_glossary: 用户配置的术语表
            sv_keywords: SenseVoice 提取的关键词（人名、生僻词）
            language: 语言代码

        Returns:
            构建好的 Prompt 字符串
        """
        parts = []

        # 1. 上一句 Whisper 文本（提供上下文连贯性）
        if previous_text:
            parts.append(previous_text.strip())

        # 2. 合并 Glossary（用户词表 + SenseVoice 关键词）
        all_terms = list(set(user_glossary + sv_keywords))
        if all_terms:
            glossary_str = ", ".join(all_terms[:20])  # 限制最多20个词
            parts.append(f"Glossary: {glossary_str}.")

        return " ".join(parts) if parts else self._get_base_prompt(language)

    def _get_base_prompt(self, language: str) -> str:
        """获取基础 Whisper 提示词（无上下文时使用）"""
        prompts = {
            "zh": "简体中文字幕，包含标点符号。",
            "en": "English subtitles with punctuation.",
            "ja": "日本語字幕、句読点を含む。"
        }
        return prompts.get(language, "")
```

**设计要点**:
- **串行执行**: SenseVoice 和 Whisper 串行执行，避免显存溢出
- **快慢双流**: 用户秒级看到草稿，几秒后自动精修
- **VAD 边界传递**: 将 VAD 的硬边界传递给对齐服务，用于校准

---

### 4.5 AlignmentService (锚点对齐服务)

**职责**: 将 Whisper 文本映射到 SenseVoice 时间戳

**文件**: `backend/app/services/alignment/alignment_service.py`

**算法升级**: 从 difflib 升级到 **Needleman-Wunsch 算法**（生物信息学序列比对算法）

**核心算法**:

```python
import numpy as np
from typing import List, Tuple, Optional
from dataclasses import dataclass
from app.models.confidence_models import WordTimestamp, AlignedSubtitle

@dataclass
class AlignmentConfig:
    """对齐算法配置"""
    match_score: int = 2       # 匹配得分
    mismatch_penalty: int = -1 # 错配惩罚
    gap_penalty: int = -2      # 缺失惩罚
    silence_threshold: float = 0.3  # 静音区阈值（秒）
    energy_anchor_threshold: float = 0.5  # 能量锚点偏移阈值（秒）


class CharacterNormalizer:
    """
    字符级归一化器 - v3.5 核心创新

    解决问题：中英混合场景下，Whisper输出"Hello世界"，SenseVoice输出"hello 世界"，
    词级对齐会失败。

    解决方案：统一归一化到字符级别进行对齐，对齐完成后使用Whisper原始文本。
    """

    def normalize_for_alignment(self, text: str) -> str:
        """
        归一化文本用于对齐

        规则：
        1. 转小写
        2. 移除所有空格和标点
        3. 统一Unicode规范化（NFC）
        """
        import re
        import unicodedata

        # 1. 转小写
        text = text.lower()

        # 2. 移除所有空格和标点（保留中英文字符和数字）
        text = re.sub(r'[\s\p{P}]+', '', text, flags=re.UNICODE)

        # 3. 统一Unicode规范化（NFC）
        text = unicodedata.normalize('NFC', text)

        return text

    def text_to_char_list(self, text: str) -> Tuple[List[str], List[int]]:
        """
        将文本转换为字符列表，同时保留字符到原词的映射

        Returns:
            chars: 归一化后的字符列表
            char_to_word_idx: 每个字符对应的原词索引
        """
        normalized = self.normalize_for_alignment(text)
        chars = list(normalized)

        # 简化版：假设每个字符都映射到自己（实际实现需要更复杂的映射逻辑）
        char_to_word_idx = list(range(len(chars)))

        return chars, char_to_word_idx


class AlignmentService:
    """
    锚点对齐服务 - 时空解耦的核心

    【算法架构】v3.5 最终版本
    采用"字符级归一化预处理 + Needleman-Wunsch 核心算法"的三层架构:

    Layer 1: 字符级归一化预处理（解决中英混合对齐问题）
    - 将 Whisper 和 SenseVoice 文本统一归一化到字符级别
    - 归一化规则：转小写、移除空格标点、Unicode NFC 规范化
    - 保留字符到原词的映射关系（用于对齐后还原）

    Layer 2: Needleman-Wunsch 核心算法（计算最优对齐路径）
    - 输入：归一化后的字符序列
    - 输出：对齐路径（match/mismatch/insert/delete）
    - 优势：字符级对齐避免了分词差异导致的对齐失败

    Layer 3: 时间戳映射与校准（生成最终字幕）
    - 根据对齐路径将 SenseVoice 时间戳映射到 Whisper 文本
    - 使用 Whisper 原始文本（保留大小写、标点）
    - 应用静音区硬约束和能量锚点校准

    【核心创新】
    1. 字符级归一化：解决"Hello世界" vs "hello 世界"等中英混合对齐失败问题
    2. 静音区硬约束：VAD 静音段视为禁区，时间戳不得落入
    3. 能量锚点校准：利用音频能量峰值重新定位边界
    """

    def __init__(self, config: AlignmentConfig = None):
        self.config = config or AlignmentConfig()
        self.logger = logging.getLogger(__name__)
        self.normalizer = CharacterNormalizer()  # 字符级归一化器

    async def align(
        self,
        whisper_text: str,
        sv_tokens: List[WordTimestamp],
        vad_range: Tuple[float, float],
        silence_ranges: List[Tuple[float, float]] = None,  # 新增: 静音区间列表
        energy_anchors: List[Tuple[float, float]] = None,  # 新增: 能量锚点
        chunk_offset: float = 0.0
    ) -> AlignedSubtitle:
        """
        Needleman-Wunsch 锚点对齐算法（v3.5 字符级归一化版本）

        Args:
            whisper_text: Whisper 输出的文本（高质量，主链）
            sv_tokens: SenseVoice 的字级时间戳（精准时间，辅链）
            vad_range: VAD 检测的语音边界 (start, end)
            silence_ranges: VAD 检测的静音区间列表 [(start, end), ...]
            energy_anchors: 音频能量锚点 [(time, energy), ...]
            chunk_offset: 当前 chunk 的时间偏移

        Returns:
            AlignedSubtitle: 对齐后的字幕，包含字级时间戳
        """
        silence_ranges = silence_ranges or []
        energy_anchors = energy_anchors or []

        # 1. 【v3.5 新增】字符级归一化预处理
        # 将 Whisper 和 SenseVoice 文本归一化到字符级别
        whisper_normalized = self.normalizer.normalize_for_alignment(whisper_text)
        sv_text = "".join([t.word for t in sv_tokens])
        sv_normalized = self.normalizer.normalize_for_alignment(sv_text)

        whisper_chars = list(whisper_normalized)
        sv_chars = list(sv_normalized)

        self.logger.debug(f"Whisper 归一化字符: {whisper_chars[:20]}...")
        self.logger.debug(f"SenseVoice 归一化字符: {sv_chars[:20]}...")

        # 2. 【新增】静音区硬约束预处理
        # 先将 SenseVoice 时间戳吸附到静音边界
        sv_tokens = self._apply_silence_constraints(sv_tokens, silence_ranges, chunk_offset)

        # 3. Needleman-Wunsch 字符级序列对齐
        alignment_path = self._needleman_wunsch(whisper_chars, sv_chars)

        # 4. 根据对齐路径生成带时间戳的词列表
        aligned_words = self._build_aligned_words(
            whisper_words, sv_tokens, alignment_path, chunk_offset
        )

        # 5. 【新增】能量锚点差分校准
        if energy_anchors:
            aligned_words = self._calibrate_with_energy_anchors(
                aligned_words, energy_anchors, chunk_offset
            )

        # 6. VAD 边界校准（最终保护）
        aligned_words = self._calibrate_with_vad(aligned_words, vad_range)

        # 7. 填补 Gap（确保时间连续）
        aligned_words = self._fill_gaps(aligned_words)

        return AlignedSubtitle(
            text=whisper_text,
            words=aligned_words,
            start=aligned_words[0].start if aligned_words else vad_range[0],
            end=aligned_words[-1].end if aligned_words else vad_range[1]
        )

    # ==================== Needleman-Wunsch 算法 ====================

    def _needleman_wunsch(
        self,
        seq1: List[str],  # Whisper 主链
        seq2: List[str]   # SenseVoice 辅链
    ) -> List[Tuple[int, int, str]]:
        """
        Needleman-Wunsch 全局序列对齐算法

        Returns:
            对齐路径: [(i, j, op), ...] 其中 op 为 'match'/'mismatch'/'insert'/'delete'
        """
        m, n = len(seq1), len(seq2)
        match = self.config.match_score
        mismatch = self.config.mismatch_penalty
        gap = self.config.gap_penalty

        # 初始化得分矩阵
        score = np.zeros((m + 1, n + 1), dtype=int)
        for i in range(m + 1):
            score[i][0] = i * gap
        for j in range(n + 1):
            score[0][j] = j * gap

        # 填充得分矩阵
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                # 计算相似度（支持模糊匹配）
                similarity = self._word_similarity(seq1[i-1], seq2[j-1])
                diag_score = match if similarity > 0.8 else mismatch

                score[i][j] = max(
                    score[i-1][j-1] + diag_score,  # 匹配/错配
                    score[i-1][j] + gap,            # 删除（Whisper 多了）
                    score[i][j-1] + gap             # 插入（SenseVoice 多了）
                )

        # 回溯获取对齐路径
        alignment = []
        i, j = m, n
        while i > 0 or j > 0:
            if i > 0 and j > 0:
                similarity = self._word_similarity(seq1[i-1], seq2[j-1])
                diag_score = match if similarity > 0.8 else mismatch
                if score[i][j] == score[i-1][j-1] + diag_score:
                    op = 'match' if similarity > 0.8 else 'mismatch'
                    alignment.append((i-1, j-1, op))
                    i -= 1
                    j -= 1
                    continue

            if i > 0 and score[i][j] == score[i-1][j] + gap:
                alignment.append((i-1, -1, 'delete'))  # Whisper 词无对应
                i -= 1
            elif j > 0:
                alignment.append((-1, j-1, 'insert'))  # SenseVoice 词无对应
                j -= 1

        alignment.reverse()
        return alignment

    def _word_similarity(self, word1: str, word2: str) -> float:
        """计算两个词的相似度（支持中文模糊匹配）"""
        w1, w2 = word1.lower(), word2.lower()
        if w1 == w2:
            return 1.0

        # 编辑距离相似度
        max_len = max(len(w1), len(w2))
        if max_len == 0:
            return 1.0

        # 简化的编辑距离计算
        distance = self._levenshtein_distance(w1, w2)
        return 1.0 - (distance / max_len)

    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """Levenshtein 编辑距离"""
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    def _build_aligned_words(
        self,
        whisper_words: List[str],
        sv_tokens: List[WordTimestamp],
        alignment: List[Tuple[int, int, str]],
        offset: float
    ) -> List[WordTimestamp]:
        """根据对齐路径构建带时间戳的词列表"""
        aligned_words = []
        last_end_time = 0.0

        for w_idx, sv_idx, op in alignment:
            if op in ('match', 'mismatch'):
                # 使用 Whisper 的词 + SenseVoice 的时间
                sv_token = sv_tokens[sv_idx]
                aligned_words.append(WordTimestamp(
                    word=whisper_words[w_idx],
                    start=sv_token.start + offset,
                    end=sv_token.end + offset,
                    confidence=sv_token.confidence if op == 'match' else 0.7,
                    is_pseudo=(op == 'mismatch')
                ))
                last_end_time = sv_token.end + offset

            elif op == 'delete':
                # Whisper 词无对应 SenseVoice 时间，需要插值
                # 使用前一个词的结束时间作为起点
                word = whisper_words[w_idx]
                estimated_duration = 0.3  # 默认 0.3 秒
                aligned_words.append(WordTimestamp(
                    word=word,
                    start=last_end_time,
                    end=last_end_time + estimated_duration,
                    confidence=0.5,
                    is_pseudo=True
                ))
                last_end_time += estimated_duration

            # 'insert' 操作跳过（SenseVoice 多识别的词，信任 Whisper）

        return aligned_words

    # ==================== 静音区硬约束 ====================

    def _apply_silence_constraints(
        self,
        sv_tokens: List[WordTimestamp],
        silence_ranges: List[Tuple[float, float]],
        offset: float
    ) -> List[WordTimestamp]:
        """
        静音区硬约束 - VAD 静音段视为禁区

        规则:
        1. 任何字幕的 start/end 不得落在静音内部
        2. 若 SenseVoice 时间戳越界，强制吸附到最近的静音边界
        """
        if not silence_ranges:
            return sv_tokens

        for token in sv_tokens:
            token_start = token.start + offset
            token_end = token.end + offset

            for silence_start, silence_end in silence_ranges:
                # 检查 start 是否落入静音区
                if silence_start < token_start < silence_end:
                    # 吸附到最近的边界
                    if token_start - silence_start < silence_end - token_start:
                        token.start = silence_start - offset
                    else:
                        token.start = silence_end - offset
                    self.logger.debug(f"词 '{token.word}' start 吸附到静音边界")

                # 检查 end 是否落入静音区
                if silence_start < token_end < silence_end:
                    if token_end - silence_start < silence_end - token_end:
                        token.end = silence_start - offset
                    else:
                        token.end = silence_end - offset
                    self.logger.debug(f"词 '{token.word}' end 吸附到静音边界")

        return sv_tokens

    # ==================== 能量锚点校准 ====================

    def _calibrate_with_energy_anchors(
        self,
        words: List[WordTimestamp],
        energy_anchors: List[Tuple[float, float]],
        offset: float
    ) -> List[WordTimestamp]:
        """
        能量锚点差分校准

        当 SenseVoice 与 Whisper 对齐结果在某段偏移 > 阈值时，
        利用能量锚点（音频能量峰值）重新定位边界
        """
        if not energy_anchors or not words:
            return words

        threshold = self.config.energy_anchor_threshold

        # 找到能量峰值点
        peak_times = [t for t, e in energy_anchors if e > 0.7]  # 高能量点

        for word in words:
            word_mid = (word.start + word.end) / 2

            # 查找最近的能量峰值
            nearest_peak = None
            min_distance = float('inf')
            for peak_time in peak_times:
                distance = abs(peak_time - word_mid)
                if distance < min_distance:
                    min_distance = distance
                    nearest_peak = peak_time

            # 如果偏移超过阈值，进行校准
            if nearest_peak and min_distance > threshold:
                shift = nearest_peak - word_mid
                # 限制最大偏移量
                shift = max(-0.5, min(0.5, shift))
                word.start += shift
                word.end += shift
                self.logger.debug(f"词 '{word.word}' 能量锚点校准，偏移 {shift:.3f}s")

        return words

    # ==================== VAD 边界校准 ====================

    def _calibrate_with_vad(
        self,
        words: List[WordTimestamp],
        vad_range: Tuple[float, float]
    ) -> List[WordTimestamp]:
        """VAD 边界校准 - 最终保护，防止时间戳溢出"""
        if not words:
            return words

        vad_start, vad_end = vad_range
        first_word_start = words[0].start
        last_word_end = words[-1].end

        # 检查是否溢出 VAD 边界
        if last_word_end > vad_end + 0.5:  # 溢出超过 0.5 秒
            # 线性压缩所有时间戳
            scale_factor = (vad_end - vad_start) / (last_word_end - first_word_start)
            for word in words:
                word.start = vad_start + (word.start - first_word_start) * scale_factor
                word.end = vad_start + (word.end - first_word_start) * scale_factor
            self.logger.warning(f"时间戳已校准，压缩比例: {scale_factor:.2f}")

        return words

    def _fill_gaps(self, words: List[WordTimestamp]) -> List[WordTimestamp]:
        """填补时间间隙，确保连续性"""
        if len(words) < 2:
            return words

        for i in range(1, len(words)):
            gap = words[i].start - words[i-1].end
            if gap > 0.1:  # 间隙超过 100ms
                # 平分间隙
                mid = (words[i-1].end + words[i].start) / 2
                words[i-1].end = mid
                words[i].start = mid

        return words

    def _tokenize(self, text: str) -> List[str]:
        """文本分词（支持中英文）"""
        import re
        # 中文按字分，英文按词分
        tokens = []
        # 使用正则分离中英文
        pattern = r'[\u4e00-\u9fff]|[a-zA-Z]+|[0-9]+'
        matches = re.findall(pattern, text)
        for match in matches:
            tokens.append(match.lower())
        return tokens
```

**设计要点**:

1. **Needleman-Wunsch 算法**: 替代 difflib，提供更精确的序列对齐
   - Whisper 词序列为主链（不可删改）
   - SenseVoice 词序列为辅链（提供时间）
   - 支持模糊匹配（编辑距离相似度）

2. **静音区硬约束**: VAD 静音段视为禁区
   - 任何字幕的 start/end 不得落在静音内部
   - 越界时强制吸附到最近的静音边界
   - **先应用静音约束再执行对齐**

3. **能量锚点校准**: 利用音频能量峰值重新定位边界
   - 当偏移 > 阈值时，利用能量锚点重新定位
   - 适合多人对话/交叠场景

4. **置信度标记**: 区分真实对齐和伪对齐
   - match: 置信度保持原值
   - mismatch: 置信度降为 0.7
   - delete (插值): 置信度降为 0.5

---

### 4.6 SentenceSplitter (分句服务)

**职责**: 双阶段分句 - 快流基于时间间隔，慢流基于标点和语义

**文件**: `backend/app/services/nlp/sentence_splitter.py`

**设计说明**:

这是前端优化的核心组件。在快流和慢流分别执行分句，前者依赖时间间隔保证草稿可读，后者结合标点/语义输出最终行。

**核心算法**:

```python
from typing import List, Tuple
from dataclasses import dataclass
from app.models.confidence_models import WordTimestamp

@dataclass
class Sentence:
    """分句结果"""
    id: str
    text: str
    start: float
    end: float
    words: List[WordTimestamp]
    confidence: float = 1.0


class SentenceSplitter:
    """
    分句服务 - 双阶段分句

    【设计原则】
    必须在两个地方调用分句算法，因为快慢流产生的数据结构虽然相似，但质量不同。

    【第一次分句（快流）】
    - 位置：SenseVoice 返回结果后，SSE 推送前
    - 依据：主要靠 max_gap (停顿) 来切分，因为 SenseVoice 可能没标点
    - 目的：为了让草稿看起来不像一坨长文本，提升阅读体验

    【第二次分句（慢流）】
    - 位置：Alignment 完成后，SSE 推送前
    - 依据：主要靠 Whisper 生成的标点符号 (。？！)，辅以时间间隔
    - 目的：生成最终的、符合语法规范的字幕行
    """

    def __init__(
        self,
        max_gap_time: float = 1.5,  # 时间间隔阈值（秒）
        max_sentence_duration: float = 10.0,  # 单句最大时长（秒）
        max_words_per_sentence: int = 20  # 单句最大词数
    ):
        self.max_gap_time = max_gap_time
        self.max_sentence_duration = max_sentence_duration
        self.max_words_per_sentence = max_words_per_sentence
        self.logger = logging.getLogger(__name__)

    async def split(
        self,
        words: List[WordTimestamp],
        mode: str = "time_based",  # "time_based" 或 "semantic_based"
        chunk_offset: float = 0.0
    ) -> List[Sentence]:
        """
        分句主函数

        Args:
            words: 词级时间戳列表
            mode: 分句模式
                - "time_based": 基于时间间隔（快流使用）
                - "semantic_based": 基于标点和语义（慢流使用）
            chunk_offset: 当前 chunk 的时间偏移

        Returns:
            分句结果列表
        """
        if not words:
            return []

        if mode == "time_based":
            return await self._split_by_time(words, chunk_offset)
        elif mode == "semantic_based":
            return await self._split_by_semantic(words, chunk_offset)
        else:
            raise ValueError(f"Unknown split mode: {mode}")

    async def _split_by_time(
        self,
        words: List[WordTimestamp],
        chunk_offset: float
    ) -> List[Sentence]:
        """
        基于时间间隔分句（快流使用）

        策略：
        1. 检测词与词之间的时间间隔
        2. 如果间隔 > max_gap_time，则切分
        3. 如果单句时长 > max_sentence_duration，强制切分
        4. 如果单句词数 > max_words_per_sentence，强制切分
        """
        sentences = []
        current_words = []
        sentence_start = words[0].start if words else 0.0

        for i, word in enumerate(words):
            current_words.append(word)

            # 检查是否需要切分
            should_split = False

            # 条件1: 时间间隔过大
            if i < len(words) - 1:
                gap = words[i + 1].start - word.end
                if gap > self.max_gap_time:
                    should_split = True
                    self.logger.debug(f"时间间隔切分: gap={gap:.2f}s")

            # 条件2: 单句时长过长
            sentence_duration = word.end - sentence_start
            if sentence_duration > self.max_sentence_duration:
                should_split = True
                self.logger.debug(f"时长切分: duration={sentence_duration:.2f}s")

            # 条件3: 单句词数过多
            if len(current_words) >= self.max_words_per_sentence:
                should_split = True
                self.logger.debug(f"词数切分: words={len(current_words)}")

            # 条件4: 最后一个词
            if i == len(words) - 1:
                should_split = True

            # 执行切分
            if should_split and current_words:
                sentence = self._build_sentence(current_words, chunk_offset)
                sentences.append(sentence)
                current_words = []
                if i < len(words) - 1:
                    sentence_start = words[i + 1].start

        return sentences

    async def _split_by_semantic(
        self,
        words: List[WordTimestamp],
        chunk_offset: float
    ) -> List[Sentence]:
        """
        基于标点和语义分句（慢流使用）

        策略：
        1. 优先按标点符号切分（。？！）
        2. 如果没有标点，回退到时间间隔切分
        3. 如果单句过长，强制切分
        """
        sentences = []
        current_words = []
        sentence_start = words[0].start if words else 0.0

        # 中文和英文的句末标点
        sentence_end_punctuation = {'。', '？', '！', '.', '?', '!', '…'}

        for i, word in enumerate(words):
            current_words.append(word)

            # 检查是否需要切分
            should_split = False

            # 条件1: 句末标点（优先级最高）
            if word.word.strip() in sentence_end_punctuation:
                should_split = True
                self.logger.debug(f"标点切分: word='{word.word}'")

            # 条件2: 时间间隔过大（回退策略）
            elif i < len(words) - 1:
                gap = words[i + 1].start - word.end
                if gap > self.max_gap_time:
                    should_split = True
                    self.logger.debug(f"时间间隔切分: gap={gap:.2f}s")

            # 条件3: 单句时长过长（强制切分）
            sentence_duration = word.end - sentence_start
            if sentence_duration > self.max_sentence_duration:
                should_split = True
                self.logger.debug(f"时长切分: duration={sentence_duration:.2f}s")

            # 条件4: 单句词数过多（强制切分）
            if len(current_words) >= self.max_words_per_sentence:
                should_split = True
                self.logger.debug(f"词数切分: words={len(current_words)}")

            # 条件5: 最后一个词
            if i == len(words) - 1:
                should_split = True

            # 执行切分
            if should_split and current_words:
                sentence = self._build_sentence(current_words, chunk_offset)
                sentences.append(sentence)
                current_words = []
                if i < len(words) - 1:
                    sentence_start = words[i + 1].start

        return sentences

    def _build_sentence(
        self,
        words: List[WordTimestamp],
        chunk_offset: float
    ) -> Sentence:
        """构建句子对象"""
        if not words:
            raise ValueError("Cannot build sentence from empty words list")

        # 拼接文本
        text = "".join(w.word for w in words)

        # 计算时间范围
        start = words[0].start
        end = words[-1].end

        # 计算平均置信度
        avg_confidence = sum(w.confidence for w in words) / len(words)

        # 生成唯一ID
        sentence_id = f"sent_{int(start * 1000)}"

        return Sentence(
            id=sentence_id,
            text=text,
            start=start,
            end=end,
            words=words,
            confidence=avg_confidence
        )
```

**设计要点**:

1. **双模式分句**:
   - `time_based`: 基于时间间隔，用于快流（SenseVoice）
   - `semantic_based`: 基于标点和语义，用于慢流（Whisper）

2. **多重切分条件**:
   - 标点符号（。？！）- 优先级最高
   - 时间间隔 > 1.5秒
   - 单句时长 > 10秒
   - 单句词数 > 20个

3. **与双流流水线集成**:
   ```python
   # 快流分句（基于时间）
   draft_sentences = await self.sentence_splitter.split(
       words=sv_result.word_timestamps,
       mode="time_based",
       chunk_offset=chunk.start
   )

   # 慢流分句（基于语义）
   final_sentences = await self.sentence_splitter.split(
       words=aligned_subtitle.words,
       mode="semantic_based",
       chunk_offset=chunk.start
   )
   ```

4. **前端友好**:
   - 每个句子包含唯一ID
   - 包含完整的词级时间戳
   - 包含平均置信度

**未来优化方向**:

1. **智能断句**: 使用 NLP 模型识别语义边界
2. **多语言支持**: 针对不同语言的标点规则
3. **用户偏好**: 允许用户自定义单句最大时长和词数

---

### 4.7 KeywordExtractor (关键词提取服务)

**职责**: 从 SenseVoice 草稿中提取人名、生僻词等关键词（NER）

**文件**: `backend/app/services/nlp/keyword_extractor.py`

**设计说明**:

这是 Prompt 自适应系统的核心组件。不要把 SenseVoice 的整句草稿放进 Whisper Prompt（会导致 Whisper 产生"已完成"错觉），只利用 SenseVoice 做实体抽取（NER），提取人名、生僻词等关键词。

**核心算法**:

```python
import re
from typing import List, Set
from collections import Counter

class KeywordExtractor:
    """
    关键词提取服务 - 从 SenseVoice 草稿中提取关键词

    【设计原则】
    不要把 SenseVoice 整句草稿放进 Whisper Prompt！
    只提取关键词（人名、生僻词），避免 Whisper 产生"已完成"错觉。

    【提取策略】
    1. 人名识别: 中文姓氏 + 2-3字名字
    2. 专有名词: 连续大写字母、品牌名
    3. 生僻词: 低频词、长词
    4. 数字串: 电话号码、身份证号等
    """

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # 中文常见姓氏（前100个）
        self.chinese_surnames = {
            '王', '李', '张', '刘', '陈', '杨', '黄', '赵', '周', '吴',
            '徐', '孙', '马', '朱', '胡', '郭', '何', '林', '高', '罗',
            '郑', '梁', '谢', '宋', '唐', '许', '韩', '冯', '邓', '曹',
            '彭', '曾', '肖', '田', '董', '袁', '潘', '于', '蒋', '蔡',
            '余', '杜', '叶', '程', '苏', '魏', '吕', '丁', '任', '沈'
        }

        # 常见词表（用于过滤）
        self.common_words = self._load_common_words()

    async def extract(
        self,
        text: str,
        language: str = "zh",
        max_keywords: int = 10
    ) -> List[str]:
        """
        提取关键词

        Args:
            text: SenseVoice 草稿文本
            language: 语言代码
            max_keywords: 最多返回关键词数量

        Returns:
            关键词列表
        """
        keywords = set()

        if language == "zh":
            # 中文关键词提取
            keywords.update(self._extract_chinese_names(text))
            keywords.update(self._extract_rare_words(text))
            keywords.update(self._extract_numbers(text))
        elif language == "en":
            # 英文关键词提取
            keywords.update(self._extract_proper_nouns(text))
            keywords.update(self._extract_rare_words(text))

        # 过滤常见词
        keywords = self._filter_common_words(keywords)

        # 限制数量
        result = list(keywords)[:max_keywords]

        self.logger.debug(f"提取关键词: {result}")
        return result

    def _extract_chinese_names(self, text: str) -> Set[str]:
        """提取中文人名"""
        names = set()

        # 模式1: 姓氏 + 2-3字名字
        for surname in self.chinese_surnames:
            # 匹配 "姓 + 2-3个汉字"
            pattern = f"{surname}[\\u4e00-\\u9fff]{{1,2}}"
            matches = re.findall(pattern, text)
            names.update(matches)

        return names

    def _extract_proper_nouns(self, text: str) -> Set[str]:
        """提取英文专有名词（连续大写字母开头的词）"""
        # 匹配首字母大写的词（至少2个字母）
        pattern = r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b'
        matches = re.findall(pattern, text)

        # 过滤单个大写字母开头的常见词（如 "The", "A"）
        proper_nouns = {m for m in matches if len(m) > 2}

        return proper_nouns

    def _extract_rare_words(self, text: str) -> Set[str]:
        """提取生僻词（长词、低频词）"""
        rare_words = set()

        # 中文：提取4字及以上的词
        chinese_long_words = re.findall(r'[\u4e00-\u9fff]{4,}', text)
        rare_words.update(chinese_long_words)

        # 英文：提取8字母及以上的词
        english_long_words = re.findall(r'\b[a-zA-Z]{8,}\b', text)
        rare_words.update(english_long_words)

        return rare_words

    def _extract_numbers(self, text: str) -> Set[str]:
        """提取数字串（电话号码、身份证号等）"""
        numbers = set()

        # 匹配连续数字（至少6位）
        pattern = r'\d{6,}'
        matches = re.findall(pattern, text)
        numbers.update(matches)

        return numbers

    def _filter_common_words(self, keywords: Set[str]) -> Set[str]:
        """过滤常见词"""
        return {kw for kw in keywords if kw not in self.common_words}

    def _load_common_words(self) -> Set[str]:
        """加载常见词表"""
        # 简化版，实际应从文件加载
        return {
            '的', '了', '在', '是', '我', '有', '和', '就', '不', '人',
            '都', '一', '一个', '上', '也', '很', '到', '说', '要', '去',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to',
            'for', 'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are'
        }
```

**设计要点**:

1. **NER 实体抽取**: 只提取关键词，不提取整句
   - 中文人名: 姓氏 + 2-3字名字
   - 英文专有名词: 首字母大写的词
   - 生僻词: 长词（中文4字+，英文8字母+）
   - 数字串: 电话号码、身份证号等

2. **避免 Whisper "已完成"错觉**:
   - 不要把完整句子放进 Prompt
   - 只提供发音线索（关键词）
   - Whisper 仍需自己推理完整句子

3. **轻量级实现**:
   - 基于正则表达式，无需加载大模型
   - 毫秒级响应，不影响实时性
   - 可扩展：支持加载自定义词表

4. **与 Prompt 构建集成**:
   ```python
   # 在 DualAlignmentPipeline 中使用
   sv_keywords = await self.keyword_extractor.extract(
       text=sv_result.text,
       language=config.language
   )

   # 构建 Prompt: 上一句Whisper + Glossary (用户词表 + SenseVoice关键词)
   whisper_prompt = self._build_smart_prompt(
       previous_text=self.previous_text_buffer,
       user_glossary=config.glossary or [],
       sv_keywords=sv_keywords,  # 只有关键词，不是整句
       language=config.language
   )
   ```

**未来优化方向**:

1. **集成 NER 模型**: 使用 spaCy 或 HanLP 提升人名识别准确率
2. **用户反馈学习**: 记录用户修正的关键词，动态更新词表
3. **领域自适应**: 根据视频类型（技术、娱乐、新闻）调整提取策略

---

### 4.7 ResourceManager (显存/模型生命周期管理)

**职责**: 显式管理模型加载/卸载，防止 OOM

**文件**: `backend/app/core/resource_manager.py`

**核心方法**:

```python
import torch
from typing import Dict, Any
from app.services.monitoring.hardware_monitor import HardwareMonitor

class ResourceManager:
    """资源管理器 - 显存生命周期管理（v3.5 显存分层策略）"""

    def __init__(self, hardware_monitor: HardwareMonitor):
        self.loaded_models: Dict[str, Any] = {}
        self.hardware_monitor = hardware_monitor
        self.logger = logging.getLogger(__name__)

    def get_available_vram(self) -> float:
        """获取可用显存（MB）"""
        return self.hardware_monitor.get_gpu_memory_available()

    def get_vram_tier(self) -> str:
        """
        【v3.5 新增】获取显存档位

        Returns:
            "high": >8GB - 可运行 Large-v3 + Demucs 5分钟分块
            "medium": 4-8GB - 可运行 Medium + Demucs 5分钟分块
            "low": <4GB - 仅运行 Small，Demucs跳过或CPU
        """
        vram_mb = self.hardware_monitor.get_gpu_memory_total()

        if vram_mb > 8000:
            return "high"
        elif vram_mb > 4000:
            return "medium"
        else:
            return "low"

    def get_recommended_whisper_model(self) -> str:
        """
        【v3.5 新增】根据显存档位推荐 Whisper 模型

        显存分层策略：
        - High (>8GB): Large-v3 (GPU)
        - Medium (4-8GB): Medium (GPU)
        - Low (<4GB): Small (GPU)
        """
        tier = self.get_vram_tier()

        tier_to_model = {
            "high": "large-v3",
            "medium": "medium",
            "low": "small"
        }

        return tier_to_model[tier]

    def should_enable_demucs(self) -> bool:
        """
        【v3.5 新增】判断是否应该启用 Demucs

        策略：
        - High/Medium: 启用 Demucs（5分钟分块）
        - Low: 跳过 Demucs
        """
        tier = self.get_vram_tier()
        return tier in ["high", "medium"]

    async def acquire_whisper(self, model_size: str = None):
        """
        获取 Whisper 模型（自动管理显存）

        Args:
            model_size: 指定模型大小，如果为None则根据显存档位自动选择
        """
        # 【v3.5 新增】自动选择模型大小
        if model_size is None:
            model_size = self.get_recommended_whisper_model()
            self.logger.info(f"根据显存档位自动选择 Whisper 模型: {model_size}")

        # 如果显存紧张，先卸载 Demucs
        if self.get_available_vram() < 4000:  # 4GB
            self.logger.warning("显存不足，卸载 Demucs")
            await self.unload("demucs")

        # SenseVoice 推荐常驻 CPU (Int8)，占用极小，无需卸载

        # 加载 Whisper
        if "whisper" not in self.loaded_models:
            self.logger.info(f"加载 Whisper 模型: {model_size}")
            from faster_whisper import WhisperModel
            model = WhisperModel(model_size, device="cuda", compute_type="float16")
            self.loaded_models["whisper"] = model

        return self.loaded_models["whisper"]

    async def acquire_demucs(self):
        """获取 Demucs 模型"""
        if "demucs" not in self.loaded_models:
            self.logger.info("加载 Demucs 模型")
            # 加载逻辑...
            self.loaded_models["demucs"] = demucs_model

        return self.loaded_models["demucs"]

    async def unload(self, model_name: str):
        """卸载模型并释放显存"""
        if model_name in self.loaded_models:
            del self.loaded_models[model_name]
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            self.logger.info(f"模型已卸载: {model_name}")

    async def check_oom_risk(self, required_vram_mb: float) -> bool:
        """检查 OOM 风险"""
        available = self.get_available_vram()
        if available < required_vram_mb:
            self.logger.warning(
                f"OOM 风险: 需要 {required_vram_mb}MB，可用 {available}MB"
            )
            return True
        return False
```

**设计要点**:
- **显式生命周期**: 明确模型的加载和卸载时机
- **OOM 预防**: 在加载前检查显存，必要时卸载其他模型
- **优先级策略**: SenseVoice (CPU) 常驻，Demucs 和 Whisper 按需加载

---

## 5. 预设系统重构（v3.5 矩阵式预设）

### 5.1 当前预设系统分析

**现有预设**:
- `default` (极速): SenseVoice Only
- `preset1` (补刀): SenseVoice + 智能补刀
- `preset2` (轻校): 补刀 + 按需校对
- `preset3` (精校): 补刀 + 全文精修
- `preset4` (全译): 精校 + 全文翻译
- `preset5` (重译): 精校 + 重点翻译

**问题**:
- 线性预设不够灵活（用户想要"Fast + 翻译"无法表达）
- 预设与硬件能力脱节，无智能推荐
- 缺乏基于运行时性能的动态调整

### 5.2 v3.5 矩阵式预设系统设计

**核心理念**: 基础层（Base Layer）+ 增强层（Enhancement Layer）的组件化思维

#### 基础层（Base Layer）- 必选其一

| 预设名称 | 适用场景 | Demucs | 模型配置 | 速度/质量 |
|---------|---------|--------|---------|----------|
| **Fast Mode** | 实时会议、快速查阅 | Off | SenseVoice (CPU) Only | 速度 20x / 质量 中 |
| **Pro Mode** | UP主投稿、正式出版 | On (5分钟分块) | SenseVoice (CPU) + Whisper Large-v3 (GPU) | 速度 1x / 质量 极高 |

**注意**: 原v3.0的"Standard"已废弃，因为弃用WhisperX后不存在中间态。Pro Mode即为默认推荐。

#### 增强层（Enhancement Layer）- 可选多个

| 增强功能 | 说明 | 依赖 |
|---------|------|------|
| **LLM Proofread** | 使用AI修正错别字和语法错误 | 需要LLM API Key |
| **LLM Translate** | 自动翻译字幕到目标语言 | 需要LLM API Key |

#### 预设组合示例

```python
# backend/app/pipelines/presets.py

from dataclasses import dataclass
from typing import List, Optional

@dataclass
class PresetConfig:
    """矩阵式预设配置"""
    # 基础层
    base_mode: str  # "fast" 或 "pro"

    # 增强层
    enhancements: List[str]  # ["llm_proofread", "llm_translate"]

    # 派生配置（自动计算）
    enable_demucs: bool = False
    whisper_model: Optional[str] = None
    enable_llm_proofread: bool = False
    enable_llm_translate: bool = False

    def __post_init__(self):
        """根据基础层和增强层自动计算派生配置"""
        # 基础层配置
        if self.base_mode == "fast":
            self.enable_demucs = False
            self.whisper_model = None
        elif self.base_mode == "pro":
            self.enable_demucs = True
            self.whisper_model = "large-v3"

        # 增强层配置
        self.enable_llm_proofread = "llm_proofread" in self.enhancements
        self.enable_llm_translate = "llm_translate" in self.enhancements


# 预定义预设组合
PRESET_MATRIX = {
    "fast": PresetConfig(
        base_mode="fast",
        enhancements=[]
    ),
    "pro": PresetConfig(
        base_mode="pro",
        enhancements=[]
    ),
    "pro_proof": PresetConfig(
        base_mode="pro",
        enhancements=["llm_proofread"]
    ),
    "pro_trans": PresetConfig(
        base_mode="pro",
        enhancements=["llm_translate"]
    ),
    "pro_full": PresetConfig(
        base_mode="pro",
        enhancements=["llm_proofread", "llm_translate"]
    )
}


class PresetRecommender:
    """预设推荐器 - 基于硬件能力"""

    def __init__(self, hardware_monitor: HardwareMonitor):
        self.hardware_monitor = hardware_monitor

    def recommend_preset(self) -> str:
        """根据硬件能力推荐预设"""
        vram_mb = self.hardware_monitor.get_gpu_memory_total()

        if vram_mb >= 8000:  # 8GB+
            return "pro"  # 可以运行 Large-v3
        elif vram_mb >= 4000:  # 4GB+
            return "pro"  # 仍推荐 Pro（可降级为 Medium）
        else:
            return "fast"  # 仅 SenseVoice

    def get_preset_config(self, preset_name: str) -> PresetConfig:
        """获取预设配置"""
        return PRESET_MATRIX.get(preset_name, PRESET_MATRIX["pro"])
```

### 5.3 前端预设选择器调整

**新增硬件提示**:

```vue
<!-- frontend/src/components/editor/PresetSelector.vue -->

<template>
  <div class="preset-selector">
    <div class="hardware-info">
      <span>检测到显存: {{ vramGB }}GB</span>
      <span class="recommend">推荐: {{ recommendedPreset }}</span>
    </div>

    <div class="preset-grid">
      <div
        v-for="preset in presets"
        :key="preset.id"
        :class="['preset-card', {
          'selected': selectedPreset === preset.id,
          'disabled': !preset.available
        }]"
        @click="selectPreset(preset)"
      >
        <h3>{{ preset.name }}</h3>
        <p>{{ preset.description }}</p>
        <div class="preset-tags">
          <span v-if="preset.id === recommendedPreset" class="tag-recommend">
            推荐
          </span>
          <span v-if="!preset.available" class="tag-disabled">
            显存不足
          </span>
        </div>
      </div>
    </div>
  </div>
</template>

<script setup>
import { ref, computed, onMounted } from 'vue'
import { getHardwareInfo } from '@/api/system'

const vramGB = ref(0)
const recommendedPreset = ref('standard')

const presets = computed(() => [
  {
    id: 'fast',
    name: '极速预览',
    description: 'SenseVoice 单引擎，秒级出结果',
    available: true
  },
  {
    id: 'standard',
    name: '标准对齐',
    description: '双流对齐，平衡速度与质量',
    available: vramGB.value >= 4
  },
  {
    id: 'pro',
    name: '精校全流',
    description: 'Large-v3 + LLM 校对，极致质量',
    available: vramGB.value >= 8
  }
])

onMounted(async () => {
  const hwInfo = await getHardwareInfo()
  vramGB.value = Math.floor(hwInfo.gpu.vram_total / 1024)
  recommendedPreset.value = hwInfo.recommended_preset
})
</script>
```

---
