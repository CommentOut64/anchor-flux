## 6. 统一日志管理

### 6.1 当前日志系统问题

- 缺乏日志轮转机制
- 缺乏结构化日志支持
- 第三方库日志级别硬编码

### 6.2 增强日志管理设计

**文件**: `backend/app/core/logging.py`

```python
import logging
import logging.handlers
from pathlib import Path
from typing import Optional
import json
from datetime import datetime

class StructuredFormatter(logging.Formatter):
    """结构化日志格式化器 - 支持 JSON 输出"""

    def format(self, record: logging.LogRecord) -> str:
        log_data = {
            "timestamp": datetime.fromtimestamp(record.created).isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }

        # 添加额外字段
        if hasattr(record, 'job_id'):
            log_data['job_id'] = record.job_id
        if hasattr(record, 'chunk_id'):
            log_data['chunk_id'] = record.chunk_id

        return json.dumps(log_data, ensure_ascii=False)


class EnhancedLoggingManager:
    """增强日志管理器"""

    def __init__(
        self,
        log_dir: str = "logs",
        max_bytes: int = 10 * 1024 * 1024,  # 10MB
        backup_count: int = 5,
        enable_structured: bool = False
    ):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        self.max_bytes = max_bytes
        self.backup_count = backup_count
        self.enable_structured = enable_structured

    def setup_logging(self, log_level: str = "INFO"):
        """设置日志系统"""
        root_logger = logging.getLogger()
        root_logger.setLevel(getattr(logging, log_level.upper()))

        # 清除现有处理器
        root_logger.handlers.clear()

        # 1. 控制台处理器（人类可读）
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_formatter = logging.Formatter(
            '%(asctime)s.%(msecs)03d [%(levelname)s] [%(name)s] %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        console_handler.setFormatter(console_formatter)
        root_logger.addHandler(console_handler)

        # 2. 文件处理器（带轮转）
        file_handler = logging.handlers.RotatingFileHandler(
            self.log_dir / "app.log",
            maxBytes=self.max_bytes,
            backupCount=self.backup_count,
            encoding='utf-8'
        )
        file_handler.setLevel(logging.DEBUG)

        if self.enable_structured:
            # 结构化日志（JSON）
            file_handler.setFormatter(StructuredFormatter())
        else:
            # 传统格式
            file_handler.setFormatter(console_formatter)

        root_logger.addHandler(file_handler)

        # 3. 错误日志单独文件
        error_handler = logging.handlers.RotatingFileHandler(
            self.log_dir / "error.log",
            maxBytes=self.max_bytes,
            backupCount=self.backup_count,
            encoding='utf-8'
        )
        error_handler.setLevel(logging.ERROR)
        error_handler.setFormatter(console_formatter)
        root_logger.addHandler(error_handler)

        # 4. 第三方库日志过滤
        self._configure_third_party_loggers()

        logging.info("日志系统初始化完成")

    def _configure_third_party_loggers(self):
        """配置第三方库日志级别"""
        third_party_loggers = {
            'faster_whisper': logging.WARNING,
            'torch': logging.WARNING,
            'transformers': logging.WARNING,
            'onnxruntime': logging.WARNING,
            'urllib3': logging.WARNING,
            'httpx': logging.WARNING
        }

        for logger_name, level in third_party_loggers.items():
            logging.getLogger(logger_name).setLevel(level)


# 全局日志管理器实例
_logging_manager: Optional[EnhancedLoggingManager] = None


def setup_logging(
    log_dir: str = "logs",
    log_level: str = "INFO",
    enable_structured: bool = False
):
    """设置全局日志系统"""
    global _logging_manager
    _logging_manager = EnhancedLoggingManager(
        log_dir=log_dir,
        enable_structured=enable_structured
    )
    _logging_manager.setup_logging(log_level)


def get_logger(name: str) -> logging.Logger:
    """获取日志记录器"""
    return logging.getLogger(name)


# 日志上下文管理器（用于添加 job_id 等上下文）
class LogContext:
    """日志上下文 - 自动添加 job_id 等字段"""

    def __init__(self, logger: logging.Logger, **context):
        self.logger = logger
        self.context = context
        self.old_factory = None

    def __enter__(self):
        self.old_factory = logging.getLogRecordFactory()

        def record_factory(*args, **kwargs):
            record = self.old_factory(*args, **kwargs)
            for key, value in self.context.items():
                setattr(record, key, value)
            return record

        logging.setLogRecordFactory(record_factory)
        return self.logger

    def __exit__(self, exc_type, exc_val, exc_tb):
        logging.setLogRecordFactory(self.old_factory)


# 使用示例
# with LogContext(logger, job_id="job_123", chunk_id="chunk_5"):
#     logger.info("处理中...")  # 日志会自动包含 job_id 和 chunk_id
```

**配置文件调整**:

```python
# backend/app/core/config.py

class Settings(BaseSettings):
    # 日志配置
    LOG_DIR: str = "logs"
    LOG_LEVEL: str = "INFO"
    LOG_MAX_BYTES: int = 10 * 1024 * 1024  # 10MB
    LOG_BACKUP_COUNT: int = 5
    LOG_ENABLE_STRUCTURED: bool = False  # 生产环境可开启 JSON 格式
```

---

## 7. 统一模型管理

### 7.1 当前模型管理问题

- 配置分散在多个文件
- 缺乏详细的性能监控
- 无法运行时动态调整缓存大小

### 7.2 统一模型配置

**文件**: `backend/app/core/model_config.py`

```python
from dataclasses import dataclass
from typing import Dict, Optional

@dataclass
class UnifiedModelConfig:
    """统一模型配置"""

    # 缓存配置
    cache_size: int = 3
    memory_threshold: float = 0.8
    enable_auto_cleanup: bool = True

    # 下载配置
    download_timeout: int = 600
    parallel_downloads: int = 1
    mirror_url: Optional[str] = None

    # 预加载配置
    preload_vad: bool = True
    preload_demucs: bool = False
    preload_whisper: bool = False

    # 监控配置
    enable_monitoring: bool = True
    monitoring_interval: int = 5  # 秒

    # Whisper 配置
    whisper_models: Dict[str, str] = None
    whisper_default_model: str = "medium"

    # SenseVoice 配置
    sensevoice_model_path: str = "models/sensevoice"
    sensevoice_device: str = "cpu"

    # Demucs 配置
    demucs_model: str = "htdemucs"
    demucs_device: str = "cuda"

    def __post_init__(self):
        if self.whisper_models is None:
            self.whisper_models = {
                "tiny": "tiny",
                "base": "base",
                "small": "small",
                "medium": "medium",
                "large-v2": "large-v2",
                "large-v3": "large-v3"
            }


# 全局配置实例
model_config = UnifiedModelConfig()
```

### 7.3 模型性能监控

**文件**: `backend/app/services/monitoring/model_monitor.py`

```python
from dataclasses import dataclass
from typing import Dict
import time

@dataclass
class ModelMetrics:
    """模型性能指标"""
    model_name: str
    load_count: int = 0
    inference_count: int = 0
    total_inference_time: float = 0.0
    avg_inference_time: float = 0.0
    cache_hits: int = 0
    cache_misses: int = 0


class ModelPerformanceMonitor:
    """模型性能监控器"""

    def __init__(self):
        self.metrics: Dict[str, ModelMetrics] = {}
        self.logger = logging.getLogger(__name__)

    def record_load(self, model_name: str):
        """记录模型加载"""
        if model_name not in self.metrics:
            self.metrics[model_name] = ModelMetrics(model_name=model_name)
        self.metrics[model_name].load_count += 1

    def record_inference(self, model_name: str, duration: float):
        """记录推理时间"""
        if model_name not in self.metrics:
            self.metrics[model_name] = ModelMetrics(model_name=model_name)

        metrics = self.metrics[model_name]
        metrics.inference_count += 1
        metrics.total_inference_time += duration
        metrics.avg_inference_time = (
            metrics.total_inference_time / metrics.inference_count
        )

    def record_cache_hit(self, model_name: str):
        """记录缓存命中"""
        if model_name not in self.metrics:
            self.metrics[model_name] = ModelMetrics(model_name=model_name)
        self.metrics[model_name].cache_hits += 1

    def record_cache_miss(self, model_name: str):
        """记录缓存未命中"""
        if model_name not in self.metrics:
            self.metrics[model_name] = ModelMetrics(model_name=model_name)
        self.metrics[model_name].cache_misses += 1

    def get_metrics(self, model_name: str) -> Optional[ModelMetrics]:
        """获取模型指标"""
        return self.metrics.get(model_name)

    def get_all_metrics(self) -> Dict[str, ModelMetrics]:
        """获取所有模型指标"""
        return self.metrics

    def get_cache_hit_rate(self, model_name: str) -> float:
        """获取缓存命中率"""
        metrics = self.metrics.get(model_name)
        if not metrics:
            return 0.0

        total = metrics.cache_hits + metrics.cache_misses
        if total == 0:
            return 0.0

        return metrics.cache_hits / total


# 全局监控器实例
model_monitor = ModelPerformanceMonitor()
```

---

## 8. SSE系统调整

### 8.1 当前SSE系统分析

**优点**:
- 完整的命名空间事件系统（progress.*、signal.*、subtitle.*）
- 智能重连机制（指数退避，最大重试5次）
- 线程安全的消息广播

**问题**:
- 断线重连后无历史事件补发
- 缺乏事件优先级机制
- 大量字幕更新时可能导致前端卡顿

### 8.2 增强版 SSE Publisher

**文件**: `backend/app/services/streaming/sse_publisher.py`

```python
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import asyncio

@dataclass
class SSEEvent:
    """SSE 事件数据结构"""
    id: str
    type: str
    payload: Dict
    timestamp: float
    priority: int = 0  # 0=普通, 1=重要, 2=紧急

class EnhancedSSEPublisher:
    """增强版 SSE 发布器 - 支持历史事件和优先级"""

    def __init__(self, sse_manager):
        self.sse_manager = sse_manager
        self.event_history: Dict[str, List[SSEEvent]] = {}  # job_id -> events
        self.max_history_size = 1000  # 每个任务最多保留 1000 条历史
        self.logger = logging.getLogger(__name__)

    async def publish_draft(
        self,
        job_id: str,
        chunk_id: str,
        text: str,
        start: float,
        end: float,
        confidence: float
    ):
        """发布草稿事件（快流）"""
        event = SSEEvent(
            id=f"{job_id}_{chunk_id}_draft",
            type="subtitle.draft",
            payload={
                "temp_id": chunk_id,
                "text": text,
                "start": start,
                "end": end,
                "confidence": confidence,
                "is_draft": True
            },
            timestamp=datetime.now().timestamp(),
            priority=0
        )

        await self._publish_event(job_id, event)

    async def publish_overwrite(
        self,
        job_id: str,
        chunk_id: str,
        subtitle: 'AlignedSubtitle'
    ):
        """发布覆盖事件（慢流）"""
        event = SSEEvent(
            id=f"{job_id}_{chunk_id}_final",
            type="subtitle.overwrite",
            payload={
                "target_temp_id": chunk_id,
                "text": subtitle.text,
                "words": [asdict(w) for w in subtitle.words],
                "start": subtitle.start,
                "end": subtitle.end,
                "is_draft": False
            },
            timestamp=datetime.now().timestamp(),
            priority=1  # 定稿事件优先级更高
        )

        await self._publish_event(job_id, event)

    async def publish_progress(
        self,
        job_id: str,
        phase: str,
        percent: float,
        message: str = ""
    ):
        """发布进度事件"""
        event = SSEEvent(
            id=f"{job_id}_progress_{phase}",
            type="progress.overall",
            payload={
                "phase": phase,
                "percent": percent,
                "message": message,
                "status": "processing"
            },
            timestamp=datetime.now().timestamp(),
            priority=0
        )

        await self._publish_event(job_id, event)

    async def publish_signal(
        self,
        job_id: str,
        signal_type: str,
        data: Dict = None
    ):
        """发布信号事件"""
        event = SSEEvent(
            id=f"{job_id}_signal_{signal_type}",
            type=f"signal.{signal_type}",
            payload=data or {},
            timestamp=datetime.now().timestamp(),
            priority=2  # 信号事件最高优先级
        )

        await self._publish_event(job_id, event)

    async def _publish_event(self, job_id: str, event: SSEEvent):
        """发布事件并存入历史"""
        # 存入历史
        if job_id not in self.event_history:
            self.event_history[job_id] = []

        self.event_history[job_id].append(event)

        # 限制历史大小
        if len(self.event_history[job_id]) > self.max_history_size:
            self.event_history[job_id] = self.event_history[job_id][-self.max_history_size:]

        # 实时推送
        await self.sse_manager.broadcast(
            channel=f"job:{job_id}",
            event_type=event.type,
            data=event.payload
        )

        self.logger.debug(f"SSE 事件已发布: {event.type} (job={job_id})")

    def get_history(
        self,
        job_id: str,
        since_timestamp: float = 0.0,
        event_types: Optional[List[str]] = None
    ) -> List[SSEEvent]:
        """获取历史事件（用于断线重连）"""
        if job_id not in self.event_history:
            return []

        events = self.event_history[job_id]

        # 过滤时间戳
        events = [e for e in events if e.timestamp > since_timestamp]

        # 过滤事件类型
        if event_types:
            events = [e for e in events if e.type in event_types]

        return events

    def clear_history(self, job_id: str):
        """清除历史事件"""
        if job_id in self.event_history:
            del self.event_history[job_id]
            self.logger.info(f"已清除任务历史事件: {job_id}")
```

### 8.3 新增历史事件 API

**文件**: `backend/app/api/routes/transcription_routes.py`

```python
@router.get("/jobs/{job_id}/events/history")
async def get_event_history(
    job_id: str,
    since: float = Query(0.0, description="起始时间戳"),
    types: Optional[str] = Query(None, description="事件类型，逗号分隔")
):
    """获取任务的历史事件（用于断线重连）"""
    event_types = types.split(',') if types else None

    events = sse_publisher.get_history(
        job_id=job_id,
        since_timestamp=since,
        event_types=event_types
    )

    return {
        "job_id": job_id,
        "events": [
            {
                "id": e.id,
                "type": e.type,
                "payload": e.payload,
                "timestamp": e.timestamp
            }
            for e in events
        ],
        "count": len(events)
    }
```

### 8.4 前端断线重连增强

**文件**: `frontend/src/services/sseChannelManager.js`

```javascript
class SSEChannelManager {
  // ... 现有代码 ...

  async reconnect(channel, jobId) {
    // 获取最后一条消息的时间戳
    const lastTimestamp = this.getLastEventTimestamp(jobId) || 0

    // 重新连接
    this.subscribe(channel, jobId, this.handlers[jobId])

    // 拉取丢失的事件
    try {
      const response = await fetch(
        `/api/jobs/${jobId}/events/history?since=${lastTimestamp}`
      )
      const data = await response.json()

      // 重放历史事件
      for (const event of data.events) {
        this.handleEvent(jobId, event.type, event.payload)
      }

      console.log(`已重放 ${data.count} 条历史事件`)
    } catch (error) {
      console.error('拉取历史事件失败:', error)
    }
  }

  getLastEventTimestamp(jobId) {
    // 从本地存储或内存中获取最后一条消息的时间戳
    return localStorage.getItem(`last_event_ts_${jobId}`)
  }

  handleEvent(jobId, eventType, payload) {
    // 保存时间戳
    localStorage.setItem(`last_event_ts_${jobId}`, Date.now() / 1000)

    // 调用处理器
    const handler = this.handlers[jobId]
    if (handler && handler[eventType]) {
      handler[eventType](payload)
    }
  }
}
```
