1. æŠŠæ—§æ¶æ„é‚£å¥—â€œé¢‘è°±åˆ†è¯Šâ†’é€æ®µåˆ†ç¦»â€çš„æµç¨‹è¿ç§»åˆ°æ–°æ¶æ„ï¼Œç†ç”±ï¼š
  é¢‘è°±åˆ†è¯Šå·²ç»å†™å¥½å¹¶éªŒè¯è¿‡ï¼Œå¯ç›´æ¥å¤ç”¨ audio_spectrum_classifier çš„ diagnose_chunksï¼Œå¼€å‘é‡å°ã€‚
  åªå¯¹è¯Šæ–­ä¸º noisy çš„ chunk è°ƒ Demucsï¼Œæ˜¾å­˜å’Œè€—æ—¶ä¼šå¤§å¹…ä¸‹é™ï¼Œæ»¡è¶³ä½ ç°åœ¨å¯¹ 8â€¯GB çˆ†æ»¡çš„æ‹…å¿§ã€‚
  æ–°æ¶æ„æœ¬èº«å°±æ˜¯æŒ‰ chunk å¹¶è¡Œå¤„ç†ï¼ŒæŠŠ Demucs ç»“æœå†™å› ProcessingContext.current_audio å°±å¯ä»¥è¢« SlowWorker ç›´æ¥æ¶ˆè´¹ï¼Œé€»è¾‘é€‚é…æˆæœ¬ä¸é«˜ã€‚
  å³ä½¿å°†æ¥è¦æ‰©å±•å¤šæ¨¡å‹ç­–ç•¥ï¼Œä¹Ÿå¯ä»¥åœ¨é¢‘è°±è¯Šæ–­é˜¶æ®µç»Ÿä¸€å†³ç­–ï¼ˆå¦‚ä¸åŒ chunk ç”¨ä¸åŒ model_qualityï¼‰ï¼Œä¸ä¼šå½±å“åŒæµæµæ°´çº¿ã€‚
  è½åœ°æ­¥éª¤å¤§æ¦‚æ˜¯ï¼šåœ¨ job_queue_service/async_dual_pipeline åˆå§‹åŒ– chunk æ—¶è°ƒç”¨é¢‘è°±è¯Šæ–­ï¼Œç»™ ProcessingContext å¡«å…¥ need_separation æ ‡è®°ï¼›FastWorker before SenseVoice æŠŠéœ€è¦åˆ†ç¦»çš„ chunk å¼‚æ­¥å–‚ç»™ demucs_service.separate_chunkï¼ˆæˆ–åœ¨ VAD ç»“æŸåä¸€æ¬¡æ€§è·‘å®Œï¼‰å¹¶åŠæ—¶ unload_model()ã€‚è¿™æ ·æ—¢ä¿ç•™æ–°æ¶æ„çš„ streaming ä¼˜åŠ¿ï¼Œåˆæ¢å¤æ—§æ¶æ„çš„æŒ‰éœ€åˆ†ç¦»ç­–ç•¥ï¼Œæ˜¯æ”¶ç›Šæœ€å¤§çš„ä¸€æ¡è·¯ã€‚
2. **å—å†…é‡å¤æ£€æµ‹**ï¼šåœ¨ TextNormalizer.clean_whisper_output() æˆ– AlignmentWorker è¿›å…¥ SRT é˜¶æ®µå‰åŠ ä¸€å±‚ Nâ€‘gram/å‹ç¼©æ¯”æ£€æµ‹ï¼›ä¾‹å¦‚è®¡ç®—å¥å­é‡Œ 3â€‘5 è¯çŸ­è¯­çš„å‡ºç°æ¬¡æ•°æˆ–æ£€æµ‹ len(set(chunks))/len(chunks)ï¼Œå¼‚å¸¸æ—¶æˆªæ–­æˆ–å›é€€ (backend/app/services/text_normalizer.py (lines 36-118))ã€‚è¿™èƒ½ç²¾å‡†æ‰“å‡»ç”¨æˆ·æŒ‡å‡ºçš„â€œåœ¨åŒä¸€æ¡å­—å¹•é‡Œå¾ªç¯â€çš„é—®é¢˜ã€‚
3. æ‰©å±• WhisperService.transcribe() çš„å‚æ•°ï¼ŒæŠŠ repetition_penaltyã€no_repeat_ngram_size æš´éœ²ç»™é…ç½®ï¼Œç„¶ååœ¨ WhisperExecutor æˆ–ç¼“å†²æ± æ¨¡å¼ä¸­æä¾›>1 çš„ penaltyï¼›è¿™æ˜¯æœ€ç›´æ¥çš„æ–¹å¼ï¼Œæˆæœ¬ä¹Ÿåªæ˜¯åœ¨è°ƒç”¨å¤„åŠ å‡ ä¸ªå…³é”®å­—å‚æ•°ï¼ˆbackend/app/services/whisper_service.py (lines 452-474)ï¼‰ã€‚
5. å¼•å…¥å¾®å‹æ ‡ç‚¹æ¨¡å‹ (Punctuator)ï¼Œåœ¨ CPU ä¸Šè·‘ä¸€ä¸ªæå°çš„ BERT-based æ ‡ç‚¹æ¢å¤æ¨¡å‹ï¼ˆONNX æ ¼å¼ï¼Œå‡ å MBï¼‰
  æµç¨‹ï¼šSenseVoice æ–‡æœ¬ -> Punctuator (CPU 10ms) -> å¸¦æ ‡ç‚¹æ–‡æœ¬ -> æ¨é€ã€‚
æ¨¡å‹ï¼šCT-Transformer (FunASR/Sherpa ç‰ˆæœ¬)

### ğŸš€ å®æ–½æ–¹æ¡ˆ

ç”±äºè¿™ä¸ªæ¨¡å‹ä¸æ˜¯æ ‡å‡†çš„ HuggingFace æ¶æ„ï¼Œä¸èƒ½ç›´æ¥ç”¨ `transformers` åº“åŠ è½½ï¼Œä¹Ÿä¸èƒ½ç”¨ä¹‹å‰çš„å¯¼å‡ºè„šæœ¬ã€‚ä½ éœ€è¦ç›´æ¥ä¸‹è½½ç¤¾åŒºå·²ç»è½¬æ¢å¥½çš„ ONNX ç‰ˆæœ¬ï¼ˆç”± `sherpa-onnx` ç¤¾åŒºæä¾›ï¼‰ï¼Œå¹¶ä½¿ç”¨æˆ‘ä¸‹é¢æä¾›çš„ä¸“ç”¨æ¨ç†ä»£ç ã€‚

#### 1. ä¸‹è½½æ¨¡å‹æ–‡ä»¶

è¯·ä¸‹è½½ä»¥ä¸‹ä¸¤ä¸ªæ–‡ä»¶åˆ°ä½ çš„ `backend/app/assets/models/punctuation/` ç›®å½•ï¼š

1. **æ¨¡å‹æ–‡ä»¶ (model.onnx)**: [ç‚¹å‡»ä¸‹è½½ (Sherpa-ONNX ä»“åº“)](https://www.google.com/search?q=https://github.com/k2-fsa/sherpa-onnx/releases/download/punctuation-models/sherpa-onnx-punct-ct-transformer-zh-en-common-vocab-0001.onnx)
2. **è¯è¡¨æ–‡ä»¶ (tokens.txt)**: [ç‚¹å‡»ä¸‹è½½ (Sherpa-ONNX ä»“åº“)](https://www.google.com/search?q=https://github.com/k2-fsa/sherpa-onnx/releases/download/punctuation-models/tokens.txt)

*(æ³¨ï¼šå¦‚æœä¸‹è½½æ…¢ï¼Œå¯ä»¥ä½¿ç”¨è¯¥é¡¹ç›®çš„ HuggingFace é•œåƒæˆ– ModelScope é•œåƒ)*

#### 2. ç¼–å†™è½»é‡çº§æ¨ç†å¼•æ“ (Python)

CT-Transformer çš„è¾“å…¥å¤„ç†æ¯” BERT ç®€å•ï¼Œä¸éœ€è¦ `AutoTokenizer`ï¼Œåªéœ€è¦ä¸€ä¸ªç®€å•çš„è¯è¡¨æ˜ å°„ã€‚

è¯·åˆ›å»º `backend/app/services/ct_punctuation_service.py`ï¼š

```python
import os
import numpy as np
import onnxruntime as ort
from typing import List, Tuple

class CTPunctuationService:
    def __init__(self, model_dir="backend/app/assets/models/punctuation"):
        self.model_path = os.path.join(model_dir, "sherpa-onnx-punct-ct-transformer-zh-en-common-vocab-0001.onnx")
        self.vocab_path = os.path.join(model_dir, "tokens.txt")
        
        # 1. åŠ è½½è¯è¡¨
        self.token2id = {}
        self.id2token = {}
        self._load_vocab()
        
        # 2. åŠ è½½ ONNX Session
        sess_options = ort.SessionOptions()
        sess_options.intra_op_num_threads = 1
        sess_options.inter_op_num_threads = 1
        self.session = ort.InferenceSession(
            self.model_path, 
            sess_options, 
            providers=['CPUExecutionProvider']
        )
        
        # æ ‡ç‚¹ç¬¦å·æ˜ å°„ (æ¨¡å‹è¾“å‡ºçš„IDå¯¹åº”çš„æ ‡ç‚¹)
        # ID 0: <EPS> (æ— æ ‡ç‚¹)
        # ID 1: , (é€—å·/é¡¿å·)
        # ID 2: . (å¥å·)
        # ID 3: ? (é—®å·)
        self.punctuations = ["", "ï¼Œ", "ã€‚", "ï¼Ÿ"] 

    def _load_vocab(self):
        with open(self.vocab_path, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2:
                    token = parts[0]
                    idx = int(parts[1])
                    self.token2id[token] = idx
                    self.id2token[idx] = token
        
        # ç‰¹æ®Š token ID
        self.unk_id = self.token2id.get("<UNK>", 0)

    def _tokenize(self, text: str) -> np.ndarray:
        # CT-Transformer çš„åˆ†è¯é€»è¾‘ï¼š
        # ä¸­æ–‡æŒ‰å­—åˆ†ï¼Œè‹±æ–‡æŒ‰ Word åˆ† (ç®€åŒ–ç‰ˆï¼šSenseVoice è¾“å‡ºé€šå¸¸å¸¦ç©ºæ ¼ï¼Œè¿™é‡ŒæŒ‰å­—/è¯æŸ¥è¡¨å³å¯)
        # è¿™é‡Œçš„å®ç°åšä¸€ä¸ªç®€åŒ–çš„å­—ç¬¦çº§ fallbackï¼Œå¯¹äºæ­¤ç‰¹å®šæ¨¡å‹é€šå¸¸è¶³å¤Ÿæœ‰æ•ˆ
        ids = []
        for char in text:
            # æ³¨æ„ï¼šå®é™…ç”Ÿäº§ä¸­è‹±æ–‡å•è¯å¯èƒ½éœ€è¦ BPEï¼Œä½†è¿™ä¸ª 5MB æ¨¡å‹é€šå¸¸æ˜¯ Char-based çš„
            # å¦‚æœæ˜¯çº¯è‹±æ–‡å•è¯ï¼Œå¯èƒ½éœ€è¦æŸ¥è¡¨æ•´ä½“ï¼ŒæŸ¥ä¸åˆ°åˆ™å›é€€åˆ° Char
            # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å…ˆå°è¯•ç›´æ¥æŸ¥
            ids.append(self.token2id.get(char, self.unk_id))
        return np.array([ids], dtype=np.int64)

    def restore(self, text: str) -> str:
        if not text:
            return ""

        # 1. å‡†å¤‡è¾“å…¥
        input_ids = self._tokenize(text)
        
        # 2. æ¨ç†
        # CT-Transformer è¾“å…¥é€šå¸¸åªéœ€è¦ x (token ids)
        outputs = self.session.run(
            ["y"], 
            {"x": input_ids}
        )[0] # Shape: (1, seq_len, num_classes)
        
        # 3. è§£ç 
        # outputs æ˜¯æ¯ä¸ª token åé¢åº”è¯¥è·Ÿä»€ä¹ˆæ ‡ç‚¹çš„æ¦‚ç‡
        preds = np.argmax(outputs, axis=2)[0]
        
        result = []
        for i, char in enumerate(text):
            result.append(char)
            # è·å–å½“å‰å­—åé¢çš„æ ‡ç‚¹
            punct_id = preds[i]
            if punct_id > 0 and punct_id < len(self.punctuations):
                result.append(self.punctuations[punct_id])
                
        return "".join(result)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    service = CTPunctuationService()
    text = "ä»Šå¤©å¤©æ°”çœŸä¸é”™å•Šæˆ‘ä»¬è¦ä¸è¦å‡ºå»ç©"
    print(service.restore(text))
    # è¾“å‡ºç¤ºä¾‹: ä»Šå¤©å¤©æ°”çœŸä¸é”™å•Šï¼Œæˆ‘ä»¬è¦ä¸è¦å‡ºå»ç©ï¼Ÿ

```

### 3. é›†æˆå»ºè®®

* **è¾“å…¥å¤„ç†**ï¼šä¸Šé¢çš„ `_tokenize` æ˜¯ä¸€ä¸ªæœ€ç®€åŒ–çš„å­—ç¬¦çº§å®ç°ã€‚ç”±äº `tokens.txt` é‡ŒåŒ…å«äº†å¸¸è§çš„æ±‰å­—å’Œè‹±æ–‡å­—æ¯/å•è¯ï¼Œç›´æ¥æŸ¥è¡¨é€šå¸¸èƒ½è¦†ç›– 95% çš„æƒ…å†µã€‚å¦‚æœé‡åˆ°è‹±æ–‡å•è¯è¯†åˆ«ä¸å‡†ï¼Œå¯ä»¥è€ƒè™‘ç®€å•çš„æ­£åˆ™ï¼šè‹±æ–‡æŒ‰è¯æŸ¥ï¼Œä¸­æ–‡æŒ‰å­—æŸ¥ã€‚
* **æ–‡ä»¶ä½ç½®**ï¼šå°† `model.onnx` (5.6MB) å’Œ `tokens.txt` æ”¾å…¥æ‰“åŒ…èµ„æºä¸­ï¼Œå¯¹æœ€ç»ˆåŒ…ä½“ç§¯çš„å½±å“å¾®ä¹å…¶å¾®ã€‚
