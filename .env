UV_LINK_MODE=copy
UV_PROJECT_ENVIRONMENT=.venv

# ========================================
# AnchorFlux 配置
# ========================================
# 说明: 修改此文件后需要重启服务才能生效
# ========================================

# ========== Whisper 模型配置 ==========
#
# 可选值:
#   tiny      - 最快速,准确率较低 (39MB, 1GB显存, 32x实时)
#   base      - 快速,准确率一般 (74MB, 1GB显存, 16x实时)
#   small     - 平衡选择 (244MB, 2GB显存, 6x实时)
#   medium    - 默认推荐 (769MB, 5GB显存, 2x实时)
#   large-v3  - 最高准确率 (1550MB, 10GB显存 float16 / 6-7GB int8_float16, 1x实时)
#   turbo     - 快速且准确 (809MB, 6GB显存, 8x实时)
#
# 默认: medium (推荐日常使用)
# 如需更高准确率,可改为: large-v3
#
WHISPER_MODEL=large-v3

# ========== Whisper 计算类型 ==========
#
# 可选值:
#   auto          - 自动选择 (推荐, >=8GB显存用int8_float16, <8GB用int8)
#   int8_float16  - 混合精度 (精度高, 显存优化, 适合>=8GB显存)
#   int8          - 纯整数量化 (最快, 最省显存, 适合<8GB显存)
#   float16       - 半精度浮点 (精度最高, 显存需求大)
#
# 默认: auto (根据显存自动选择最优方案)
#
# 显存对比 (large-v3模型):
#   float16:       ~10GB   精度最高
#   int8_float16:  ~6-7GB  精度接近float16, 推荐
#   int8:          ~5-6GB  精度略降, 速度最快
#
WHISPER_COMPUTE_TYPE=auto

# ========== HuggingFace 镜像源 ==========
#
# 是否使用国内镜像源 (hf-mirror.com)
# 可选值: true, false
# 默认: true (国内用户推荐)
#
# 如遇下载问题或在国外,可改为: false
#
USE_HF_MIRROR=true

# ========== PyPI 镜像源 ==========
#
# PyPI 镜像源地址,用于加速 Python 包下载
# 可选值:
#   https://pypi.tuna.tsinghua.edu.cn/simple  - 清华大学镜像源(推荐)
#   https://mirrors.aliyun.com/pypi/simple/   - 阿里云镜像源
#   https://pypi.mirrors.ustc.edu.cn/simple/  - 中国科技大学镜像源
#   (留空或注释掉则使用官方源)
#
# 默认: 清华大学镜像源
# 注意: 如果镜像源不可用(如403错误),脚本会自动回退到官方源
#
PYPI_MIRROR=https://pypi.tuna.tsinghua.edu.cn/simple

# ========== SenseVoice ONNX 模型配置 ==========
#
# 设备选择:
#   cpu   - 默认,使用 CPU 推理 (配合 INT8 量化模型速度快)
#   cuda  - 使用 GPU 推理 (需要 FP32 模型,CUDA 对 INT8 支持不完整)
#   auto  - 自动选择 (检测到 CUDA 则用 GPU,否则用 CPU)
#
# 默认: cpu (推荐,配合量化模型速度最优)
#
SENSEVOICE_DEVICE=cpu

# 模型类型选择:
#   quantized - 默认,使用 INT8 量化模型 (model_quant.onnx,体积小速度快,仅支持 CPU)
#   fp32      - 使用 FP32 完整模型 (model.onnx,体积大精度高,支持 GPU)
#
# 注意: 如果选择 cuda 设备,建议同时设置 SENSEVOICE_MODEL_TYPE=fp32
#       因为 CUDA 对 INT8 量化算子支持不完整
#
# 默认: quantized (推荐,CPU 下速度最快)
#
SENSEVOICE_MODEL_TYPE=quantized